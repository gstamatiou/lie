\documentclass{report}

% Packages for math symbols and equations
\input{packages}
\input{commands}

\usepackage{tikz-cd}


% Title page information
\title{Lie group notes}
\author{Giorgos}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter*{Conventions}
\begin{itemize}
    \item By Lie group, we mean either real or complex.
    \item A closed Lie subgroup of a Lie group is a submanifold that is also a subgroup (see \cref{thm:closed_subgroup}). 
    \item A Lie subgroup is a subgroup that is also an immersed submanifold.
\end{itemize}

\chapter{Lie groups: basic definitions}
We will have two notions of subgroups of Lie groups:
\begin{definition}
    Let $G$ be a complex or real Lie group and $H$ be a subgroup of $G$.
    \begin{enumerate}[label = (\roman*)]
        \item $H$ is a Lie subgroup if it is an immersed submanifold of $G$ with the multiplication and inverse maps being smooth (or analytic).
        \item $H$ is a closed Lie subgroup if it is an embedded submanifold of $G$.
    \end{enumerate}
\end{definition}
\begin{example}
    Any of the classical Lie groups are closed Lie subgroups of $\GL(n, \mathbb K)$.
    For a Lie subgroup that is not closed, consider the irrational winding on the torus, that is $G = \mathbb T^2 = \mathbb R^2/\mathbb Z^2$, and $H = f(\mathbb R)$ with $f(t,s) = (t \mod \mathbb Z, \alpha s \mod \mathbb Z)$, where $\alpha \in \mathbb R\backslash \mathbb Q$.
\end{example}
\section{Lie subgroups and quotients}
The following theorem allows us to reduce the study of Lie groups to the study of finite groups and connected Lie groups, since $G_0$ is a normal subgroup of $G$ and $G/G_0$ is a discrete group.
where $G^0$ is the identity component of $G$.
\begin{theorem}[Theorem 2.6, \cite{kirillov2008introduction}]
    Let $G$ be a real or complex Lie group and $G^0$ its identity component.
    Then $G^0$ is a normal subgroup of $G$ and a Lie group itself, while $G/G^0$ is a discrete group.
\end{theorem}

In fact, we can reduce the case of connected Lie groups to simply connected Lie groups:
\begin{theorem}[Theorem 2.7, \cite{kirillov2008introduction}]
    Let $G$ be a connected Lie group. Then its universal cover $\tilde G$ has a canonical structure of a Lie group such that the covering map $p: \tilde G \to G$ is a homomorphism of Lie groups whose kernel is isomorphic to the fundamental group of $G$.
    Moreover, in this case, $\ker p$ is a discrete central subgroup in $\tilde G$.
\end{theorem}
\begin{proof}
    The lifting property of the universal cover implies that $\tilde G$ is a Lie group.
    The kernel is discrete, being the fiber of a covering map.
    The fact that it is central follows from the more general fact that every discrete normal subgroup of a connected Lie group is central.
    To show the latter, one considers the map $G \to N, g \to g n g^{-1}$ where $N$ is the normal subgroup and $n \in N$ is some fixed element.
    Then the inverse image of any element $n' \in N$ is closed an open in $G$, so it is all of $G$ in the case where $n' = n$ and empty otherwise.
\end{proof}

\begin{example}
    In the case where $G = \mathbb T^2$ is the torus, the covering map $p: \mathbb R^2 \to \mathbb T^2$ is given by $p(t,s) = e^{2\pi i t, 2\pi i s }$, and $\ker p = \mathbb N^2 \simeq \pi_1(\mathbb T^2)$.
\end{example}

We have the following connection between subgroups and Lie subgroups (i.e.\ subgroups that are also submanifolds):
\begin{theorem}[Theorem 2.8, \cite{kirillov2008introduction}]\label{thm:closed_subgroup}
    \begin{enumerate}[label=(\roman*)]
        \item Any Lie subgroup of a Lie group is closed in the topology of the ambient group.
        \item Any closed subgroup of a Lie group is a real Lie subgroup.
    \end{enumerate}
\end{theorem}
\begin{proof}
    For the first part of the theorem, we note that $\overline H$ is a subgroup of $G$ as well.
    We claim that $H$ (and thus $Hx$ for every $x \in \overline H$ ) is open and dense in $\overline H$.
    To see this, note that $e \in \overline H$ implies that for every neighborhood $U \subseteq G$ of $e$ in $G$, $U \cap H$ is nonempty.
    In particular there exists some open set $U$ in $G$ containing $r$ such that $U \cap H \neq \emptyset$.
    Then $U\cap H$ will be a neighborhood of $e$ in $\overline H$ and $H$ is open because it can be written as the union of all subsets of the form $h(U\cap H)$ for $h \in H$.

    To conclude, note that for $x, y \in \overline H$, $Hx \cap Hy$ is dense in $\overline H$, so it is nonempty.
    This implies that $Hx = Hy = H$ and $H$ thus $H = \overline H$.
\end{proof}

The following lemma is useful in showing connectedness of Lie groups:
\begin{lemma}[Connectedness from a quotient to the group]
    Let $G$ be a Lie group and $H$ a closed subgroup.
    If $G/H$ and $H$ are connected, then so is $G$.
\end{lemma}
\begin{proof}
    Assume for the shake of contradiction that $G = A \sqcup B$ with $A, B$ open and disjoint and denote with $q: G \to G/H$ the quotient map.
    Since $H$ is connected, the same is true for is cosets.
    Thus $A,B$ are unions of cosets of $H$, so $q(A), q(B)$ are open and disjoint in $G/H$.
    This is a contradiction.
\end{proof}
For instance it can be used to show that $\SO(n), \SU(n), \mathrm{Sp}(n)$ are connected (see for instance \cite{knapp1996lie}[Proposition 1.136]).
\section{Homogeneous spaces}
We begin by describing coset spaces of Lie groups.
\begin{theorem}[Theorem 2.11, \cite{kirillov2008introduction}]
    Let $G$ be a Lie group of dimension $n$, $H \leq G$ a closed Lie subgroup of dimension $k$.
    \begin{enumerate}[label = (\roman*)]
        \item The coset space $G/H$ has a natural structure of a manifold of dimension $n-k$ such that the canonical map $p: G \to G/H$ is a fiber bundle, with fiber diffeomorphic to $H$.
        The tangent space at the identity is isomorphic to the quotient space $T_H G/H \simeq T_eG/T_eH$.
        \item If H is a normal closed Lie subgroup then $G/H$ has a canonical Lie group structure.
    \end{enumerate}
\end{theorem}

The following is the analog of the homomorphism theorem for Lie groups:
\begin{theorem}[Theorem 2.5, \cite{kirillov2008introduction}]
    Let $f:G_1 \to G_2$ be a Lie group morphism.
    \begin{enumerate}[label = (\roman*)]
        \item $H = \ker f$ is a normal closed Lie subgroup of $G_1$ and $f$ induces an injective homomorphism $G_1/H \to G_2$ that is an immersion
        \item If moreover $\Im f$ is an embedded submanifold, then it is a closed Lie subgroup of $G_2$ and $f$ induces an isomorphism $G_1/H \to \Im f$.
    \end{enumerate}
\end{theorem}


\begin{theorem}[Theorem 2.20, \cite{kirillov2008introduction}]
    Let $G$ be a Lie group acting on a manifold $M$, and $m \in M$.
    \begin{enumerate}[label = (\roman*)]
        \item The stabilizer $G_m$ is a closed Lie subgroup of $G$, with Lie algebra
        \[
        \mathfrak h = \{ x \in \mathfrak g \mid \rho_*(x)(m) = 0 \}.
        \]
        where $\rho: G \to \mathrm{Diff}(M)$ is the action of $G$ on $M$.
        \item The orbit map $g \mapsto g \cdot m$ induces an injective immersion $G/G_m \hookrightarrow \mathcal O_m$ whose image coincides with $\mathcal O_m$.
        \item The orbit $\mathcal O_m$ is an immersed submanifold with tangent space $T_m \mathcal O_m = \mathfrak g / \mathfrak h $.
        \item If the orbit is a submanifold, then the orbit map is a diffeomorphism.
    \end{enumerate}
\end{theorem}
\begin{proof}
See \cite[Theorem 2.20, Theorem 3.29]{kirillov2008introduction}.
\end{proof}

The case of one orbit gives rise to $G$-homogeneous spaces:
\begin{theorem}[Theorem 2.22 \cite{kirillov2008introduction}]
    Let $M$ be a $G$-homogeneous space and $m \in M$.
    Then the orbit map $G \to M$ is a fiber bundle over $M$ with fiber $G_m$.
\end{theorem}

The following proposition gives us a way to check whether a Lie group morphism is a covering map:
\begin{proposition}[Lie morphism covering map criterion]\label{prop:covering_map_criterion}
    Let $f:G_1 \to G_2$ be a Lie group morphism such that $f_*: \mathfrak g_1 \to \mathfrak g_2$ is an isomorphism.
    Then $f$ is a covering map and $\ker f$ is a discrete central group.
\end{proposition}
\begin{proof}
    The inverse mapping theorem tells us that there exist neighborhoods $U_1, U_2$ of the identities of $G_1$ and $G_2$ respectively such that $f:U_1 \to U_2$ is a diffeomorphism.
    In particular, $f$ is surjective since its image contains $U_2$ which generates $G_2$, being a neighborhood of the identity.
    Moreover, for every $g \in G_1$ and $z_1, z_2 \in \ker f$:
    \[
        gU_1 z_1 \cap gU_1 z_2 = \{e_1\} \text{ unless } z_1 = z_2.
    \]
    Indeed, if $g u z_1 = g v z_2$ for some $u, v \in U_1$, then $u = v z_2 z_1^{-1}$, so $f(u) = f(v)$.
    But $f$ being injective on $U_1$, this implies $u = v$ and $z_1 = z_2$.

    Thus $f(g)U_1$ is an evenly covered neighborhood for every $f(g) \in G_2$.
    \[
    f^{-1}(f(g)U_2) = g U_1 \ker f = \bigsqcup_{z \in \ker f} g U_1 z.
    \]

    To see that $\ker f$ is discrete, note that the injectivity of $f$ implies $U_1 \cap \ker f = \{ e_1 \}$.
    Being a discrete normal subgroup, it must also be central.
\end{proof}

\section{Classical Lie groups}

\begin{definition}
    We define
    \[
    Sp(n, \mathbb K) = 
    \left\{
        A \in GL(2n, \mathbb K) \mid \omega(Ax, Ay) = \omega(x, y)
    \right\}.
    \]
    where $\omega$ is the standard symplectic form on $\mathbb K^{2n}$, which is given by
    $\omega(x, y) = x^* J y = \sum_{i=1}^n x_i y_{i+n} - x_{i+n}y_i$, 
    and 
    \[
    J = 
    \begin{pmatrix}
    0 & -I_n \\
    I_n & 0
    \end{pmatrix}.
    \]
    We also define the gorup of unitary quaternionic transformations by
    \[
    \Sp(n) = \Sp(n, \mathbb C) \cap \SU(2n).
    \]
\end{definition}

The following theorem tells us that the logarithmic map behaves well when restricted to a neighborhood of the identity in each classical group.

\begin{table}[h!]
    \centering
    \begin{tabular}{c c c c c c}
        $G$ & $O(n, \mathbb{R})$ & $SO(n, \mathbb{R})$ & $U(n)$ & $SU(n)$ & $Sp(n)$ \\
        \hline \hline
        $\mathfrak{g}$ & $x^t = -x$ & $x^t = -x$ & $x^* = -x$ & $x^* = -x, \ \text{tr}, x = 0$ & $J^{-1}x^*J = -x, x^* = -x$ \\
        $\dim G$ & $\frac{n(n-1)}{2}$ & $\frac{n(n-1)}{2}$ & $n^2$ & $n^2 - 1$ & $n(2n+1)$ \\
        $\pi_0(G)$ & $\mathbb{Z}_2$ & $\{1\}$ & $\{1\}$ & $\{1\}$ & $\{1\}$ \\
        $\pi_1(G)$ & $\mathbb{Z}_2 \ (n \ge 3)$ & $\mathbb{Z}_2 \ (n \ge 3)$ & $\mathbb{Z}$ & $\{1\}$ & $\{1\}$ \\
        $\mathfrak{g}_{\mathbb C}$ &  &  & $\mathfrak{gl}(n, \mathbb C)$ & $\mathfrak{sl}(n, \mathbb C)$ & \\
        Semisimple & Yes & Yes & No & Yes & Yes \\ 
        Reductive & Yes & Yes & Yes & Yes & Yes \\
        $\mathfrak{z}(\mathfrak g)$ & \{0\} & $\{0\}$ & $i \mathbb R \id$ & $\{0\}$ & $\{0\}$ \\
    \end{tabular}
    \caption{Compact classical groups}
    \label{table:classical_groups}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c c c c}
        $G$ & $GL(n, \mathbb{R})$ & $SL(n, \mathbb{R})$ & $Sp(n, \mathbb{R})$ \\
        \hline \hline
        $\mathfrak{g}$ & $\mathfrak{gl}(n, \mathbb{R})$ & $\text{tr} \, x = 0$ & $x + J^{-1}x'J = 0$ \\
        $\dim G$ & $n^2$ & $n^2 - 1$ & $n(2n + 1)$ \\
        $\pi_0(G)$ & $\mathbb{Z}_2$ & $\{1\}$ & $\{1\}$ \\
        $\pi_1(G)$ & $\mathbb{Z}_2 \ (n \ge 3)$ & $\mathbb{Z}_2 \ (n \ge 3)$ & $\mathbb{Z}$ \\
        $\mathfrak{g}_{\mathbb C}$ &  & $\mathfrak{sl(n, \mathbb C)}$ &  \\
        Semisimple & Yes & When $n = 2$ & Yes \\
        Reductive & Yes & Yes & Yes \\
        $\mathfrak{z}(\mathfrak g)$ & $\{0\}$ & $\{0\}$ & $\{0\}$ \\
    \end{tabular}
    \caption{Noncompact real classical groups.}
    \label{table:noncompact_real_classical_groups}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c c c c c}
        $G$ & $GL(n, \mathbb{C})$ & $SL(n, \mathbb{C})$ & $O(n, \mathbb{C})$ & $SO(n, \mathbb{C})$ \\
        \hline \hline
        $\pi_0(G)$ & $\{1\}$ & $\{1\}$ & $\mathbb{Z}_2$ & $\{1\}$ \\
        $\pi_1(G)$ & $\mathbb{Z}$ & $\{1\}$ & $\mathbb{Z}_2$ & $\mathbb{Z}_2$ \\
    \end{tabular}
    \caption{Complex classical groups.}
    \label{table:complex_classical_groups}
\end{table}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{lie_algebras_venn.jpg}
    \caption{Venn Diagram}
    \label{fig:venn}
\end{figure}

\chapter{Lie groups and Lie algebras}
\section{Exponential map}
\begin{definition}[Proposition 3.1, \cite{kirillov2008introduction}]
    Let $G$ be a Lie group and $x \in \mathfrak g$.
    The one-parameter subgroup $\gamma_x: \mathbb K \to G$ is the unique Lie group morphism such that $\gamma_x'(0) = x$.
    We define the exponential map of $G$ as
    \[
    \exp(x) = \gamma_x(1)
    \]
\end{definition}
\begin{remark}
    By looking at the proof of the statements in the above definition, one can see that for $x\in \mathfrak g$, the curve
    \[
    \exp(tx) = \gamma_x(t) = \gamma_{tx}(1).
    \]
    integral curve of the left-invariant vector field $X \in \mathcal X(G)$ that satisfies
    \[
    X_e = x.
    \]
\end{remark}

The following are some properties of the exponential map:
\begin{theorem}[Theorems 3.7 and 3.36, \cite{kirillov2008introduction}]
    Let $G$ be a Lie group.
    \begin{enumerate}[label = (\roman*)]
        \item $d_{e} \exp = \id_{\mathfrak g}$
        \item The exponential map is a local diffeomorphism at $0$.
        \item For any Lie group morphism $\phi:G_1 \to G_2$, we have $d_e \phi (exp(x)) = \exp d_e (\phi(x))$ for all $x\in \mathfrak g$.
        \item For any $g \in G, x \in \mathfrak g$
        \[
        g \exp(x) g^{-1} = \exp(\Ad_g x).
        \]
        \item For $x, y \in \mathfrak g$, we have:
        \[
        \text{If } [x,y] = 0 \text{ then } e^x e^y = e^y e^x = e^{x+y}.
        \]
    \end{enumerate}
\end{theorem}

\begin{remark}
    The exponential map is not surjective in general. It is however for compact Lie groups.
\end{remark}

\begin{lemma}
    The exponential map of a connected Lie group $G$ sends generators of $\mathcal g$ to generators of $G$.
    That is, if $\{ x_1, \ldots, x_n \}$ is a basis of $\mathfrak g$, then $\left\{ \exp(t x_i) : t \in \mathbb R, i \in \llbracket 1, n \rrbracket \right\}$ is a basis of $G$.
\end{lemma}
\begin{proof}
    Consider the map $f: \mathbb R^n \to \mathbb G$ given by
    \[
    f(t_1, \ldots, t_n) = \exp(t_1 x_1) \cdots \exp(t_n x_n).
    \]
    Then $d_0 f = I_n$ in the basis with respect to the standard basis for $\mathbb R^n$ and $\{ x_1, \ldots, x_n \}$ for $\mathfrak g$.
    In particular it is surjective and by the constant rank theorem, $f(\mathbb R^n)$ contatins a neighborhood of the identity of $G$. 
\end{proof}
\begin{example}
    Let $G = SO(3, \mathbb R)$.
    Then $\mathfrak{so}(3, \mathbb R)$ consists of skew-symmetric mattrices, with basis:
    \begin{align*}
        J_1 = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \end{pmatrix}, \quad
        J_2 = \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0 \end{pmatrix}, \quad
        J_3 = \begin{pmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}.
    \end{align*}
    The exponential matrix is given by
    \[
    e^{tJ_1} = 
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & \cos(t) & -\sin(t) \\
        0 & \sin(t) & \cos(t)
    \end{pmatrix},
    \]
    i.e.\ rotation around x-axis by angle t; similarly, $J_y, J_z$ generate rotations around
    $y, z$ axes.
    Elements of the form $exp(tJx), exp(tJ_y ), exp(tJ_z )$ generate
    a neighborhood of identity in $\SO(3, \mathbb R)$.
    Since $SO(3, \mathbb R)$ is connected, these elements generate the whole group. 
    For this reason, it is common to refer to $J_x , J_y , J_z$ as “inﬁnitesimal generators” of $SO(3, \mathbb R)$. Thus,
    in a certain sense $SO(3, \mathbb R)$ is generated by three elements.
\end{example}

\begin{remark}
    To motivate the term ``inﬁnitesimal generators'', one can think of them as directions in which one can move from the identity (using the exponential map) in order to generate the whole group.
\end{remark}

\section{The commutator}
In literature, one has at least three (and four in the case of vector fields) ways to define the commutator of some Lie group, Namely
\begin{enumerate}[label = (\roman*)]
    \item Using left-invariant vector fields (see \cite{lee2018introduction}).
    \item As the lowest order term in the logarithm of the multiplication of exponentials (see \cref{lem:commutator_definition}).
    \item As the differential of the adjoint representation.
    \item In the case of vector fields, as the differentiation of the second vector field along the first.
\end{enumerate}

For completeness, we recall that in \cite{lee2018introduction}, the commutator of a Lie group is defined as
\[
[x,y] = [X,Y]_e \text{ for } x,y \in \mathfrak g
\]
where $X,Y$ are the left-invariant vector fields for which $X_e = x, Y_e = y$, and the bracket of two vector fields is defined as $[X,Y] = XY - YX$.

In \cite{kirillov2008introduction}, the commutator is defined as the lowest order term in the logarithm of the multiplication of exponentials (see \cref{lem:commutator_definition}).
We say that a map $Q:\mathfrak g \to \mathfrak g$ is of order $k$ if $Q(t x) = t^k Q(X)$ for $t \in \mathbb R, x \in \mathfrak g$.
\begin{lemma}[Lemma 3.11, \cite{kirillov2008introduction}]\label{lem:commutator_definition}
    Let $G$ be a Lie group.
    Then there exists a neighborhood of $0 \in \mathfrak g$ and smooth (or analytic in the complex case) functions $[\cdot, \cdot], \mu: \mathfrak g \times \mathfrak g \to \mathfrak g$ such that in that neighborhood:
    \begin{align*}
        e^x e^y &= e^{\mu(x,y)}\\
        \mu(x,y) &= x + y + \frac{1}{2}[x.y] + \cdots
    \end{align*}
    with $[\cdot, \cdot]$ being a bilinear skew-symmetric form and the dots denoting terms of order higher than $3$.
\end{lemma}

The commutator has the following properties:
\begin{proposition}[Proposition 3.12, \cite{kirillov2008introduction}]
    Let $G, H$ be Lie groups and $\phi: G \to H$ a Lie group morphism.
    Then for $x,y \in \mathfrak g$ we have
    \begin{enumerate}[label = (\roman*)]
        \item $\phi_{*} [x,y] = [\phi_{*} x, \phi_{*} y]$.
        \item $\Ad_g[x,y] = [\Ad_g x, \Ad_g y]$.
        \item $e^x e^y e^{-x} e^{-y} = e^{[x,y]+\cdots}$, where dots stand for degrees higher than $2$.
        \item $\ad_x y = [x,y]$, where $\ad = d_e \Ad: \mathfrak g \to \mathfrak{gl}(\mathfrak g)$.
        \item $Ad_{e^x} = e^{\ad_x} \in \mathfrak{gl}(\mathfrak g)$
    \end{enumerate}
\end{proposition}
For instance when $G$ is commutative, the commutator is zero (and the exponential map is a homomorphism).
\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item For $\mathfrak g = \mathfrak{gl}(d, \mathbb K)$, the commutator is given by
        \[
        [x,y] = xy - yx.
        \]
        \item For a general associative algebra $A$, the commutator over $\mathcal K$, is given by the same formula.
        \item Any vector space can be made into a commutative Lie algebra by defining the commutator to be zero.
    \end{enumerate}
\end{example}

\begin{theorem}[Jacobi identity]
    Let $G$ be a real or complex Lie group and $x,y,z \in \mathfrak g$.
    Then
    \[
    [x,[y,z]] + [y,[z,x]] + [z,[x,y]] = 0.
    \]
\end{theorem}
\begin{proof}
    See \cite[Theorem 3.16]{kirillov2008introduction}.
\end{proof}
\begin{proposition}
    Differentiation at the identity induces a map
    \[
    \hom(G_1, G_2) \to \hom(\mathfrak g_1, \mathfrak g_2)
    \]
    which is injective when $G_1$ is connected.
\end{proposition}


\section{Subalgebras, ideals and center}
\begin{definition}
    A subalgebra of a Lie algebra $\mathfrak g$ is a subspace $\mathfrak h \subseteq \mathfrak g$ that is closed under the Lie bracket: $[x,y] \in \mathfrak h$ for all $x,y \in \mathfrak h$.
    An ideal is a subalgebra $\mathfrak h$ such that $[x,y] \in \mathfrak h$ for all $x \in \mathfrak h, y \in \mathfrak g$.
\end{definition}
Given some ideals, we can perform operations on them to get new ideals:
\begin{proposition}[Proposition 3.19, \cite{kirillov2008introduction}]
    Let $I_1, I_2$ be ideals of a Lie algebra $\mathfrak g$.
    Then $I_1 + I_2, I_1 \cap I_2, [I_1, I_2]$ are ideals as well.
\end{proposition}
It is easy to see a quotient of a Lie algebra by an ideal carries a canonical structure of a Lie algebra.
We moreover have the following lemma, reminiscent of the homomorphism theorem for groups:
\begin{lemma}
    Let $f: \mathfrak g_1 \to \mathfrak g_2$ be a Lie algebra morphism.
    Then $\ker f$ is an ideal of $\mathfrak g_1, \im f$ is a subalgebra of $\mathfrak g_2$ and $f$ gives rise to an isomorphism of Lie algebras: $\mathfrak g_1/\ker f \simeq \im f$.
\end{lemma}

The next theorem tells us that Lie subgroups correspond to subalgebras, and normal closed Lie subgroups correspond to ideals.
In the case of the latter, a converse statement gives us a way to check if a subgroup is normal by checking whether the corresponding subalgebra is an ideal.
In this way we obtain a link between an algebraic condition (being a normal subgroup) and a linear one (being an ideal).
\begin{theorem}[Theorem 3.22, \cite{kirillov2008introduction}]
    Let $G$ be a real or complex Lie group with Lie algebra $\mathfrak g$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $H$ is a Lie subgroup (not necessarily closed) of $G$, then $\mathfrak h$ is a subalgebra of $\mathfrak g$.
        \item If $H$ is a normal closed Lie subgroup of $G$, then $\mathfrak h$ is an ideal of $\mathfrak g$.
        \item If $H$ is a closed Lie subgroup of $G$, both $G$ and  $H$ are connected, then $\mathfrak h$ is an ideal of $\mathfrak g$ if and only if $H$ is a normal subgroup of $G$.
    \end{enumerate}
\end{theorem}

One of the first ways to study a Lie algebra is by analyzing how close it is to a commutative Lie algebra.
There are numerous ways to do this, some of which will be elaborated below.
\begin{enumerate}[label = (\roman*)]
    \item By looking at the center $\mathfrak z(\mathfrak g) = \ker \ad$ of $\mathfrak g$. The bigger the center, the more commutative the Lie algebra.
    \item By looking at the commutator $[\mathfrak g, \mathfrak g]$. The smaller the commutator, the more commutative the Lie algebra.
    \item By looking at the degree of solvability of $\mathfrak g$. The smaller the degree of solvability, the more commutative the Lie algebra, since derived series are merely succesive extensions of commutative subalgebras.
\end{enumerate}
To support this, take for instance the commutant.
It is the smallest ideal of the Lie algebra, such that the quotient is abelian.
\begin{lemma}
    Let $\mathfrak g$ be a Lie algebra.
    Then for every ideal $I$ of $\mathfrak g$ such that the quotient $\mathfrak g/I$ is abelian, $I \supseteq [\mathfrak g, \mathfrak g]$.
\end{lemma}
Going on to solvable algebras, we give the following definition.
\begin{definition}
    Let $\mathfrak g$ be a Lie algebra. We define the derived series of $\mathfrak g$ as the decreasing sequence of ideals
    \[
    D^i \mathfrak g = [D^i \mathfrak g, D^i \mathfrak g],
    \]
    starting with $D^0 \mathfrak g = \mathfrak g$.
    The Lie algebra $\mathfrak g$ is said to be solvable if one of the following equivalent conditions holds:
    \begin{enumerate}[label = (\roman*)]
        \item The derived series terminates, i.e.\ there exists $n \in \mathbb N$ such that $D^n \mathfrak g = 0$.
        \item There exists a chain of subalgebras $\mathfrak a^0 = \mathfrak g \supseteq \mathfrak a^1 \supseteq \cdots \supseteq \mathfrak a^k  = 0$ such that $\mathfrak a^i$ is an ideal in $\mathfrak a^i$ and the quotient $\mathfrak a^i/ \mathfrak a^{i+1}$ is abelian.
        \item For large enough $n$, every commutator of the form $[x_1, [x_2, \ldots [x_{n-1}, x_n]\ldots]]$ ($2^n$ terms arranged in a binary tree of length $n$) is zero (see \cref{fig:derived_series}).
    \end{enumerate}
\end{definition}
\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.55\linewidth}
        \centering
        \includegraphics[width=\textwidth]{derived_series_tree.jpg}
        \caption{Illustration of derived series. Each node is the bracket of its children.}
        \label{fig:derived_series}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\linewidth}
        \centering
        \includegraphics[width=\textwidth]{lower_central_series_graph.jpg}
        \caption{Illustration of lower central series. Each node is the bracket of its children.}
        \label{fig:lower_central_series}
    \end{minipage}
\end{figure}

Similar to the solvable algebras, we also have the notion of nilpotent algebras.
\begin{definition}
    Let $\mathfrak g$ be a Lie algebra. We define the lower central series of $\mathfrak g$ as the decreasing sequence of ideals, begining with $D_0 \mathfrak g = \mathfrak g$ and
    \[
    D_{i+1} \mathfrak g = [\mathfrak g, D_i \mathfrak g].
    \]
    The algebra is called nilpotent if one of the following conditions holds:
    \begin{enumerate}[label = (\roman*)]
        \item The lower central series terminates, i.e.\ there exists $n \in \mathbb N$ such that $D_n \mathfrak g = 0$.
        \item There exists a chain of ideals $\mathfrak a_0 = \mathfrak g \supseteq \mathfrak a_1 \supseteq \cdots \supseteq \mathfrak a_k = 0$ such that $[\mathfrak g, \mathfrak a_i] \subseteq \mathfrak a_{i+1}$.
        \item For large enough $n$, every commutator of the form $[x_1, [x_2, \ldots [x_{n-1}, x_n]\ldots]]$ ($n$ terms) is zero (see \cref{fig:lower_central_series}).
    \end{enumerate}
\end{definition}




An important example is $\mathfrak{gl}(n , \mathbb K)$.
Before we describe it, we make some preliminary remarks that will facilitate the computations.
We shall call $k$-diagonal of a matrix $a_{ij}$ the entries whose indices satisfy $i-j=k$, and denote with $A_k \subseteq \gl(n, \mathbb K)$ the additive group of matrices whose entries are zero below the $k$-diagonal (i.e.\ $a_{ij} \neq 0$ implies $i-j \leq k$).
Note then $A_k A_l \subseteq A_{k+l}$.

On the other hand, let $\mathcal F$ be a flag in a finite-dimensional vector space $V$, that is a chain of subspaces (without repetitions and without restriction on the dimensions of the subspaces):
\[
\mathcal F = \left\{ 0 \subsetneq V_1 \subsetneq V_2 \subsetneq \cdots \subsetneq V_n \right\}.
\]
Then we define:
\begin{align*}
    \mathfrak b(\mathcal F) &= \left\{ x \in \gl(V) : x V_i \subseteq V_i \text{ for all } i \right\}\\
    \mathfrak n(\mathcal F) &= \left\{ x \in \gl(V) : x V_i \subseteq V_{i-1} \text{ for all } i \right\}\\
    \mathfrak a_k(\mathcal F) &= \left\{ x \in \gl(V) : x V_i \subseteq V_{i - k} \right\}.
\end{align*}
Note that $\mathfrak b(\mathcal F) = \mathfrak a_0(\mathcal F)$ and $\mathfrak n(\mathcal F) = \mathfrak a_1(\mathcal F)$, while $\mathfrak a_{k} \mathfrak a_l, [\mathfrak a_k, \mathfrak a_l] \subseteq \mathfrak a_{k+l}$.
Using induction, this means that $D_i \mathfrak n(\mathcal F) \subseteq \mathfrak a_{i+1}$, 
so $\mathfrak n(\mathcal F)$ is nilpotent.
In particular, letting $\mathcal F = \mathcal F_0$ be the standard flag in $\mathbb K^n$, the algebras $\mathfrak a_k$ are the matrices that are zero below the $k$-th diagonal:
\[
\left\{ A : a_{ij} = 0 \text{ for } i - j > k
\right\}
\]
\begin{example}
    The commutator of $\mathfrak{gl}(n, \mathbb K)$ is given by
    \[
    [\gl(n, \mathbb K), \gl(n, \mathbb K)] = \ssl(n, \mathbb K).
    \]
    Let now $\mathfrak b \subseteq \gl(n, \mathbb K)$ be the subalgebra of upper triangular matrices and $\mathfrak n$ be the subalgebra of strictly upper triangular matrices.
    Then $\mathfrak b$ is solvable and $\mathfrak n$ is nilpotent.

    Noting that $\mathfrak b = \mathfrak b (\mathcal F_0), \mathfrak n = \mathfrak n(\mathcal F_0)$ for $\mathcal F_0$ being the standard flag in $\mathbb K^n$, we have seen above that $\mathfrak n$ is nilpotent.
    
    To see that $\mathfrak b$ is solvable, note that $D^1 \mathfrak b = [\mathfrak b, \mathfrak b] \subseteq \mathfrak n = \mathfrak a_i$ since the diagonal entries of $xy$ and $yx$ coincide.
    Inductively, this implies that $D^{i+1} \mathfrak b \subseteq \mathfrak a_{2^i}$.

    Note finally that $\mathfrak b $ is not nilpotent because $D_i \mathfrak b = \mathfrak n$.
    Indeed, we have seen that $D_1 \mathfrak b = [\mathfrak b, \mathfrak b] \subseteq \mathfrak n$.
    On the other hand, the relation 
    \[
    E_{ij} E_{kl} = \delta_{jk} E_{il}
    \]
    tells us that $E_{ij} = [E_{ii}, E_{ij}] \in D_2 \mathfrak b$, so $D_2 \mathfrak b = D_1 \mathfrak b = \mathfrak n$, so it follows by induction.
\end{example}
\begin{example}[Nonisomorphic solvable and non-nilpotent algebras]
    There exists an infinite family of pairwise non-isomorphic Lie algebras that are all solvable but not nilpotent.
    These are given by
    \[
    \mathfrak g_\alpha = \spa\left\{
    X = \begin{pmatrix}
        0 & 0 & 1\\
        0 & 0 & 0\\
        0 & 0 & 0
    \end{pmatrix},
    Y = \begin{pmatrix}
        0 & 0 & 0\\
        0 & 0 & 1\\
        0 & 0 & 0
    \end{pmatrix},
    X = \begin{pmatrix}
        \alpha & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 0
    \end{pmatrix}
    \right\}.
    \]
    See \cref{ex:solvable_not_nilpotent} for a proof.
\end{example}
Finally, we have the following properties for solvable and nilpotent algebras.
\begin{proposition}
    \begin{enumerate}[label = (\roman*)]
        \item A real Lie algebra is solvable (resp.\ nilpotent) if and only if its complexification is.
        \item If $\mathfrak g$ is solvable (resp.\ nilpotent), then every subalgebra and quotient of $\mathfrak g$ is solvable (and resp.\ nilpotent).
        \item If $\mathfrak g$ is nilpotent, it is solvable.
        \item If $I \subseteq \mathfrak g$ is an ideal such that both $\mathfrak g, \mathfrak g/I$ are solvable, then $\mathfrak g$ is solvable.
    \end{enumerate}
\end{proposition}
\begin{proof}
For certain points, it is useful to note that if $I\subseteq \mathfrak g$ is an ideal, then $D^i \mathfrak (g/I) = (D^i \mathfrak g) + I$.
\end{proof}

\section{Lie algebra of vector fields}
For a manifold $M$, the space $\mathrm{Vect}(M)$ can be made into a Lie algebra by directly defining the bracket of two vector fields and showing that it is skew-symmetric and bilinear.
However, one can think of $\mathrm{Diff}(M)$ as similar to a Lie group (but not quite, since it is infinite-dimensional), whose Lie algebra is $\mathrm{Vect}(M)$.
\begin{definition}
    Let $M$ be a smooth manifold and $G$ be a Lie group.
    \begin{enumerate}[label = (\roman*)]
        \item A smooth map $\rho: G \to \mathrm{Diff}(M)$ is a map that arises from a smooth action of $G$ on $M$.
        \item The Lie algebra of $\mathrm{Diff}(M)$ is the space of vector fields $\mathrm{Vect}(M)$.
        \item The exponential map of $\mathrm{Diff}(M)$ is the flow (whenever well-defined) of a vector field: $\left(e^X\right)_m = \Phi^1_X(m)$.
        \item The differential of $\rho: G \to \mathrm{Diff}(M)$ is the map $\rho_*: \mathfrak g \to \mathrm{Vect}(M)$ given by
        \[
        \left(\rho_*(x)\right)_m = \left.\frac{\d}{\d t}\right|_{t=0} \rho\left(e^{tx}\right)(m).
        \]
        \item The commutator $[\xi, \eta]$ of two vector fields $\xi, \eta \in \mathrm{Vect}(M)$ is the unique vector field that satisfies
        \[
        \Phi^t_\xi \Phi^s_\eta \Phi^{-t}_\xi \Phi^{-s}_\eta = \Phi^{ts}_{[\xi, \eta]} + \cdots
        \]
        where the dots stand for terms of order 3 and higher in $s,t$.
    \end{enumerate}
\end{definition}

\begin{remark}
    To motivate why $\mathrm{Vect}(M) = \mathfrak{diff}(M)$, we note that $\mathfrak{diff}(M)$ should be given by the derivatives of the one-parameter subgroups of $\mathrm{Diff}(M)$.
    If $\phi^t \in \mathrm{Diff}(M)$ is a one-parameter subgroup of $M$, then its derivative defines a vector field $\partial_{t=0} \phi^t \in \mathrm{Vect}(M)$:
    \[
    \frac{\d}{\d t}\Big|_{t=0} \phi^t(m) \in T_m(M), \text{ for } m \in M.
    \]
    Similarly, to motivate the differential of $\rho: G \to \mathrm{Diff}(M)$, we note that $\rho$ maps one-parameter subgroups to one-parameter subgroups.
\end{remark}

\begin{proposition}[Proposition 3.23, \cite{kirillov2008introduction}]
    Let $M$ be a smooth manifold and $G$ be a Lie group acting smoothly on $M$.
    \begin{enumerate}[label = (\roman*)]
        \item The commutator of vector fields makes $\mathrm{Vect}(M)$ into a Lie algebra.
        \item The commutator can be also defined by the following formulas:
        \begin{align*}
            [\xi, \eta] &= \frac{\d}{\d t} (\Phi^t_\xi)_* \eta \\
            \partial_{[\xi, \eta]}f &= \partial_\eta \partial_\xi f - \partial_\xi \partial_\eta f\\
            \left[ f^i \partial_i, g^j \partial_j \right] &= \left( g^i \partial_i f^j - f^i \partial_i g^j \right) \partial_j
        \end{align*}
    \end{enumerate}
\end{proposition}
Note that the first expression from the second point of the above theorem should be read as
\[
[\xi, \eta]_p = \frac{\d}{\d t}\Big|_{t=0} d_{\Phi^{-t}_\xi(p)} \Phi^t_\xi (\eta_{\Phi^{-t}_\xi(p)})
\]
which allows us to interpret the Lie bracket of two vector fields as differentiation of the one along (the integral curves of) the other.

\begin{theorem}[Theorem 3.25, \cite{kirillov2008introduction}]
    The transformation $\rho_*: \mathfrak g \to \mathrm{Vect}(M)$ is a Lie algebra morphism.
\end{theorem}

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item Considering the action of $G$ on itself by left-multiplication $L: G \to \mathrm{Diff}(G)$, the Lie algebra isomorphism
        \[
        L_*: \mathfrak g \leftrightarrow \{\text{right invariant vector fields on } \} G
        \]
        associates to $x \in \mathfrak g$ the right-invariant vector field $X$ such that $X_e = x$.
    \end{enumerate}
\end{example}

\section{Stabilizers and the center}
\begin{corollary}
    Let $f: G_1 \to G_2$ be a morphism of real or complex Lie groups.
    Then 
    \begin{enumerate}[label = (\roman*)]
        \item $\ker f $ is a closed Lie subgroup with Lie algebra $\ker f_*$.
        \item The map $G_1/\ker f \to \mathrm{Im} f$ is an immersion.
        \item When $\mathrm{Im}f$ is a submanifold, the map is a $G_1/\ker f \to \mathrm{Im}f$ diffeomorphism.
    \end{enumerate}
\end{corollary}

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item 
        Let $V$ be a vector space over $\mathbb K$ with a bilinear form $B$.
        Then
        \[
        \mathrm{O}(V, B) = \left\{ g \in \GL(V) : B(gu, gv) = B(u,v) \text{ for all } u, v \in V \right\}
        \]
        is a closed Lie group with Lie algebra:
        \[
        \mathfrak{o}(V,B) = \left\{ g \in \mathfrak{gl}(V): B(xu, xv) = B(u,v) \text{ for all } u,v \in V\right\}.
        \]
    
        Indeed, define an action of $\GL(V)$ on the space $\mathcal F$ of bilinear forms on $V$ by $gF(u,v) = F(g^{-1}u, g^{-1}v)$.
        This defines a representation $\rho: \GL(V) \to \GL(\mathcal F)$ and $\mathrm{O}(V,B)$ is the stabilizer of $B$ under this action.
        The space $\mathcal F$ is a vector space, so $\mathrm{Vect}(\mathcal F)$ can be identifies with the space smooth maps from $\mathcal F$ to itself.
        Then for $x \in \mathfrak{gl}(V)$
        \begin{align*}
            \left(\rho_*(x)\right)_B(u,v) &= \left( \frac{\d}{\d t}\Big|_{t=0} \rho(e^{tx}) B\right)(u, v) = \left( \frac{\d}{\d t}\Big|_{t=0} B(e^{-tx}u, e^{-tx}v)\right)\\
            &= -B(u, xv) - B(xu, v).
        \end{align*}
    
        \item 
        Let $A$ be a finite-dimensional associative algebra over $\mathbb{K}$. Then the group of all automorphisms of $A$
        \[
        \text{Aut}(A) = \{g \in \text{GL}(A) \mid (g a) \cdot (g b) = g(a \cdot b) \text{ for all } a,b \in A\}
        \]
        is a Lie group with Lie algebra
        \[
        \text{Der}(A) = \{x \in \mathfrak{gl}(A) \mid (x.a)b + a(x.b) = x(ab) \text{ for all } a, b \in A\}
        \]
        (this Lie algebra is called the \textit{algebra of derivations of} $A$).
        
        Indeed, if we consider the space $W$ of all linear maps $A \otimes A \to A$ and define the action of $G$ by $(g f)(a \otimes b) = g f(g^{-1}a \otimes g^{-1}b)$ then $\text{Aut} A = G_\mu$, where $\mu : A \otimes A \to A$ is the multiplication. So $\text{Aut}(A)$ is a Lie group with Lie algebra $\text{Der}(A)$.
        \item 
        The same argument also shows that for a finite-dimensional Lie algebra $\mathfrak{g}$, the group
        \[
        \text{Aut}(\mathfrak{g}) = \{ g \in \text{GL}(\mathfrak{g}) \mid [ga, gb] = g[a, b] \text{ for all } a, b \in \mathfrak{g} \}
        \]
        is a Lie group with Lie algebra
        \[
        \text{Der}(\mathfrak{g}) = \{ x \in \mathfrak{gl}(\mathfrak{g}) \mid [x.a, b] + [a, x.b] = x.[a, b] \text{ for all } a, b \in \mathfrak{g} \}
        \]
        called the Lie algebra of derivations of $\mathfrak{g}$.
    \end{enumerate}
\end{example}


Finally, we can show that the center of $G$ is a closed Lie subgroup.


\begin{theorem}
Let $G$ be a connected Lie group. Then its center $Z(G)$ is a closed Lie subgroup with Lie algebra the ideal 
\[
\mathfrak{z}(\mathfrak{g}) = \{ x \in \mathfrak{g} \mid [x, y] = 0 \ \forall y \in \mathfrak{g} \}.
\]
\end{theorem}

The quotient group $G / Z(G)$ is usually called the \textit{adjoint group} associated with $G$ and denoted $\text{Ad}\, G$:
\[
\text{Ad}\, G = G / Z(G) = \text{Im}(\text{Ad}: G \rightarrow \text{GL}(\mathfrak{g})) \quad \text{(for connected } G).
\]
The corresponding Lie algebra is
\[
\text{ad}\, \mathfrak{g} = \mathfrak{g} / \mathfrak{z}(\mathfrak{g}) = \text{Im}(\text{ad}: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g})).
\]


\section{Campbell--Hausdorff formula}
So far, we have shown that the multiplication in a Lie group $G$ deﬁnes the
commutator in $g = T_1 G$. 
However, the deﬁnition of commutator only used the lowest non-trivial term of the group law in logarithmic coordinates. Thus, it might be expected that higher terms give more operations on g. However, it turns out that it is not so: the whole group law is completely determined by the lowest term, i.e.\ by the commutator, as follows from the Campbell--Hausdorff formula:
\begin{theorem}[Campbell--Hausdorff Formula]
    For small enough $x, y \in \mathfrak{g}$ one has
    \[
    \exp(x) \exp(y) = \exp(\mu(x, y))
    \]
    for some $\mathfrak{g}$-valued function $\mu(x, y)$ which is given by the following series convergent in some neighborhood of $(0, 0)$:
    \[
    \mu(x, y) = x + y + \sum_{n \geq 2} \mu_n(x, y), 
    \]
    where $\mu_n(x, y)$ is a Lie polynomial in $x, y$ of degree $n$, i.e. an expression consisting of commutators of $x, y$, their commutators, etc., of total degree $n$ in $x, y$. This expression is universal: it does not depend on the Lie algebra $\mathfrak{g}$ or on the choice of $x, y$.
    In particular, the group operation in a connected Lie group $G$ can be
    recovered from the commutator in $\mathfrak g$.
    \end{theorem}
    
    It is possible to write the expression for $\mu$ explicitly. 
    However, this is rarely useful, so we will only write the first few terms:
    \[
    \mu(x, y) = x + y + \frac{1}{2}[x, y] + \frac{1}{12} \left( [x, [x, y]] + [y, [y, x]] \right) + \cdots 
    \]
    

Although it follows from the Campbell--Hausdorff formula, the commutation criterion of the exponential can be also proven directly:
\begin{theorem}[Theorem 3.36, \cite{kirillov2008introduction}]
    Let $x, y \in \mathfrak g$.
    Then $e^x e^y = e^y e^x$ if and only if $[x,y]=0$.
    In that case $e^x e^y = e^y e^x = e^{x+y}$.
\end{theorem}
\begin{proof}
    The if direction follows from the definition of the commutator via Taylor series.
    For the other direction we follow the method suggested in Exercise 3.11 of \cite{kirillov2008introduction}:
    \[
    \Ad(e^x)y = e^{\ad x} y = \sum_{n\geq 0} \frac{(\ad x)^n}{n!} y = y
    \]
    But $C_g e^z = e^{\Ad_g z}$, so the above implies that $e^x e^y e^{-x} = C_{e^x}e^y = e^{\Ad(e^x)y} = e^y$.
\end{proof}

\section{Fundamental theorems of Lie theory}
Let us summarize the results we have so far about the relation between Lie
groups and Lie algebras.

\begin{enumerate}
    \item Every real or complex Lie group $G$ defines a Lie algebra $\mathfrak{g} = T_1 G$ (respectively, real or complex), with commutator defined by (3.2); we will write $\mathfrak{g} = \text{Lie}(G)$. Every morphism of Lie groups $\varphi : G_1 \to G_2$ defines a morphism of Lie algebras $\varphi_* : \mathfrak{g}_1 \to \mathfrak{g}_2$. For connected $G_1$, the map
    \[
    \text{Hom}(G_1, G_2) \to \text{Hom}(\mathfrak{g}_1, \mathfrak{g}_2)
    \]
    \[
    \varphi \mapsto \varphi_*
    \]
    is injective. (Here $\text{Hom}(\mathfrak{g}_1, \mathfrak{g}_2)$ is the set of Lie algebra morphisms.)
    \item As a special case of the previous, every Lie subgroup $H \subset G$ defines a Lie subalgebra $\mathfrak{h} \subset \mathfrak{g}$.
    \item The group law in a connected Lie group $G$ can be recovered from the commutator in $\mathfrak{g}$; however, we do not yet know whether we can also recover the topology of $G$ from $\mathfrak{g}$.
\end{enumerate}

However, this still leaves a number of questions.
\begin{question}
    Given a morphism of Lie algebras $\mathfrak{g}_1 \to \mathfrak{g}_2$, where $\mathfrak{g}_1 = \text{Lie}(G_1)$, $\mathfrak{g}_2 = \text{Lie}(G_2)$, can this morphism always be lifted to a morphism of the Lie groups?
\end{question}
\begin{question}
    Given a Lie subalgebra $\mathfrak{h} \subset \mathfrak{g} = \text{Lie}(G)$, does there always exist a corresponding Lie subgroup $H \subset G$?
\end{question}
\begin{question}
    Can every Lie algebra be obtained as a Lie algebra of a Lie group?
\end{question}

As the following example shows, in this form the answer to question 1 is negative.

\begin{example}
Let $G_1 = S^1 = \mathbb{R}/\mathbb{Z}, G_2 = \mathbb{R}$. Then the Lie algebras are $\mathfrak{g}_1 = \mathfrak{g}_2 = \mathbb{R}$ with zero commutator. Consider the identity map $\mathfrak{g}_1 \to \mathfrak{g}_2 : a \mapsto a$. Then the lift $\tilde f: \mathbb R \to \mathbb R$ of the corresponding morphism to the universal covering group (provided that the underlying morphism $f: \mathbb S^1 \to \mathbb R$ exists), should be given by $\tilde(\theta) = \theta + c$; on the other hand, it must also satisfy $\tilde f(\mathbb{Z}) = \{0\}$. Thus, this morphism of Lie algebras can not be lifted to a morphism of Lie groups.
\end{example}

In this example the difficulty arose because $G_1$ was not simply-connected. It turns out that this is the only difficulty: after taking care of this, the answers to all the questions posed above are positive. The following theorems give precise statements.

\begin{theorem}[First fundemental theorem of Lie theory]
For any real or complex Lie group $G$, there is a bijection between connected Lie subgroups $H \subset G$ and Lie subalgebras $\mathfrak{h} \subset \mathfrak{g}$, given by $H \mapsto \mathfrak{h} = \text{Lie}(H) = T_1 H$.
\end{theorem}

\begin{theorem}[Second fundemental theorem of Lie theory]
If $G_1, G_2$ are Lie groups (real or complex) and $G_1$ is connected and simply connected, then $\text{Hom}(G_1, G_2) = \text{Hom}(\mathfrak{g}_1, \mathfrak{g}_2)$, where $\mathfrak{g}_1, \mathfrak{g}_2$ are Lie algebras of $G_1, G_2$ respectively.
\end{theorem}

\begin{theorem}[Lie's third theorem]
Any finite-dimensional real or complex Lie algebra is isomorphic to a Lie algebra of a Lie group (respectively, real or complex).
\end{theorem}

These are the fundamental theorems of Lie theory and combining these with the previous results, we get the following important corollary.

\begin{corollary}[Le group and algebra equivalence]
    The categories of finite-dimensional Lie algebras and connected, simply-connected Lie groups are equivalent.
    In other words, for any real or complex finite-dimensional Lie algebra $\mathfrak{g}$, there is a unique (up to isomorphism) connected simply-connected Lie group $G$ (respectively, real or complex) with $\text{Lie}(G) = \mathfrak{g}$. Any other connected Lie group $G'$ with Lie algebra $\mathfrak{g}$ must be of the form $G/Z$ for some discrete central subgroup $Z \subset G$.
\end{corollary}

\section{Complex and real forms}
The operation of complexiﬁcation, which is trivial at the level of Lie algebras,
is highly non-trivial at the level of Lie groups.
Real forms and their complexifications can be topologically quite different: for example, $\SU(n)$ is compact while $\SL(n, C)$ is not. On the other hand, it is natural to expect that their algebras share many algebraic properties, such as semisimplicity.
Thus we may use the properties of the complex Lie algebra to study the real Lie algebra, and vice versa, or use the properties of one real form to study another real form.
Thus, we may use, for example, use the compact group $\SU(n)$ to prove some results
about the non-compact group $\SL(n, C)$. Moreover, since $\mathfrak{sl}(n, \mathbb R)_{\mathbb C} = \mathfrak{sl}(n, \mathbb C)$,
this will also give us results about the non-compact real group $\SL(n, \mathbb R)$.

\begin{definition}
    \begin{enumerate}[label = (\roman*)]
        \item 
        The complexification $\mathfrak g_{\mathbb C}$ of a real Lie algebra $\mathfrak g$ is the complex Lie algebra
    \[
    \mathfrak g_{\mathbb C} \equaldef \mathfrak g \otimes_{\mathbb R} \mathbb C = \mathfrak g \oplus i \mathfrak g.
    \]
    We say that $\mathfrak g$ is a real form of $\mathfrak g_{\mathbb C}$.
    \item
    Let $G$ be a connected complex Lie group with Lie algebra $\mathfrak g$. 
    A closed real Lie subgroup $K \leq G$ is a real form of $G$ if its Lie algebra $\mathfrak k$ is a real form of $\mathfrak g$.
    \end{enumerate}
\end{definition}

In some cases, complexiﬁcation is obvious, as in $\mathfrak g = sl(n, R)$.
Others however, are less obvious, as $\mathfrak g = \mathfrak u(n)$.
Finding a real form for a complex group is can be often direct:
\begin{proposition}
    Let $G$ be a connected simply-connected complex Lie group with Lie algebra $\mathfrak g$.
    Then every real form $\mathfrak k$ of $\mathfrak g$ is realised as the Lie algebra of a real form $K$ of $G$.
\end{proposition}
\begin{proof}
    Considering $\mathfrak g = \mathfrak k \oplus i \mathfrak k$, we define the real Lie algebra automorphism
    \[
    \theta: \mathfrak g \to \mathfrak g, \quad x + iy \mapsto x - iy.
    \]
    Since $G$ is connected and simply connected, the second fundamental theorem of Lie groups tells us that $\theta$ can be lifted to a Lie group automorphism $\Theta: G \to G$.
    Then $K = G^\Theta$ is a real form of $G$ with Lie algebra $\ker \theta = \mathfrak k$.
\end{proof}
Going in the opposite direction, from a real Lie group to a complex one, is
more subtle: there are real Lie groups that can not be obtained as real forms
of a complex Lie group (for example, it is known that the universal cover
of $\SL(2, \mathbb R)$ is not a real form of any complex Lie group). It is still possible
to deﬁne a complexiﬁcation for any real Lie group G; however, in general the former does not admit the latter as a subgroup.


\chapter{Representations of Lie groups and Lie algebras}
In this chapter, we will discuss the representation theory of Lie groups and Lie
algebras. 
Unless speciﬁed otherwise, all Lie groups, algebras,
and representations are ﬁnite-dimensional, and all representations are complex.
Lie groups and Lie algebras can be either real or complex; unless speciﬁed
otherwise, all results are valid both for the real and complex case.

\section{Basic definitions}

\begin{definition}
    A representation of a Lie group \( G \) is a vector space \( V \) together with a morphism \( \rho : G \to \mathrm{GL}(V) \).
    
    A representation of a Lie algebra \( \mathfrak{g} \) is a vector space \( V \) together with a morphism \( \rho : \mathfrak{g} \to \mathfrak{gl}(V) \).
    
    A morphism between two representations \( V, W \) of the same group \( G \) is a linear map \( f : V \to W \) which commutes with the action of \( G \): \( f \rho(g) = \rho(g)f \). In a similar way, one defines a morphism of representations of a Lie algebra. The space of all \( G \)-morphisms (respectively, \( \mathfrak{g} \)-morphisms) between \( V \) and \( W \) will be denoted by \( \mathrm{Hom}_G(V, W) \) (respectively, \( \mathrm{Hom}_{\mathfrak{g}}(V, W) \)).
\end{definition}
    
\begin{remark}
Morphisms between representations are also frequently called \textit{intertwining operators} because they ``intertwine'' action of \( G \) in \( V \) and \( W \).
\end{remark}

The notion of a representation is completely parallel to the notion of module over an associative ring or algebra; the difference of terminology is due to historical reasons. In fact, it is also usual to use the word ``module'' rather than ``representation'' for Lie algebras: a module over Lie algebra \( \mathfrak{g} \) is the same as a representation of \( \mathfrak{g} \). We will use both terms interchangeably.

Note that in this definition we did not specify whether \( V \) and \( G \) are real or complex. Usually if \( G \) (respectively, \( \mathfrak{g} \)) is complex, then \( V \) should also be taken a complex vector space. However, it also makes sense to take complex \( V \) even if \( G \) is real: in this case we require that the morphism \( G \to \mathrm{GL}(V) \) be smooth, considering \( \mathrm{GL}(V) \) as \( 2n^2 \)-dimensional real manifold. Similarly, for real Lie algebras we can consider complex representations requiring that \( \rho : \mathfrak{g} \to \mathfrak{gl}(V) \) be \( \mathbb{R} \)-linear.
    
Of course, we could also restrict ourselves to consideration of real representations of real groups. However, it turns out that the introduction of complex representations significantly simplifies the theory even for real groups and algebras. Thus, from now on, all representations will be complex unless specified otherwise.
    
\begin{example}
    For a Lie group $G$ or a Lie algebra $\mathfrak g$ we always have the following two representations:
    \begin{enumerate}[label = (\roman*)]
        \item Trivial representation: $V = \mathbb C, \rho(g) = \id$ for all $g \in G$ (respectively $\rho(x) = 0$ for all $x \in \mathfrak g$). 
        \item Adjoint representation: $V = \mathfrak g, \rho(g) = \Ad_g$ for all $g \in G$ (respectively $\rho(x) = \ad_x$ for all $x \in \mathfrak g$). 
        \item Coadjoint representation: $V = \mathfrak g^*, \rho(x) = \ad_x^* $ for all $x \in \mathfrak g$.
    \end{enumerate}
\end{example}

The first important result about representations of Lie groups and Lie algebras is the following theorem.
It tells us that from a representation (or a morphism of representations) of a Lie group we can obtain a representation (or a morphism of representations) of its Lie algebra, and that when $G$ is connected, simply connected, then representing $G$ and representing its Lie algebra is equivalent.
\begin{theorem}
Let \( G \) be a Lie group (real or complex) with Lie algebra \( \mathfrak{g} \).
\begin{enumerate}
    \item Every representation \( \rho : G \to \mathrm{GL}(V) \) defines a representation \( \rho_* : \mathfrak{g} \to \mathfrak{gl}(V) \), and every morphism of representations of \( G \) is automatically a morphism of representations of \( \mathfrak{g} \).
    \item If \( G \) is connected, simply-connected, then \( \rho \mapsto \rho_* \) gives an equivalence of categories of representations of \( G \) and representations of \( \mathfrak{g} \). In particular, every representation of \( \mathfrak{g} \) can be uniquely lifted to a representation of \( G \), and \( \mathrm{Hom}_G(V, W) = \mathrm{Hom}_{\mathfrak{g}}(V, W) \).
\end{enumerate}
\end{theorem}
This is an important result, as Lie algebras are, after all, finite dimensional vector spaces, so they are easier to deal with.
\begin{example}
    A representation of \( \mathrm{SU}(2) \) is the same as a representation of \( \mathfrak{su}(2) \), i.e. a vector space with three endomorphisms \( X, Y, Z \), satisfying commutation relations \( XY - YX = Z, YZ - ZY = X, ZX - XZ = Y \).
\end{example}
\begin{remark}
    This theorem can also be used to describe representations of a group which is connected but not simply-connected, in the sense that representations of $G$ are the same as representations of its universal cover that are trivial over the fundamentla group of $G$.
    More concretely, any such group $G$ can be written as \( G = \tilde{G}/Z \) for some simply-connected group \( \tilde{G} \) and a discrete central subgroup \( Z \subset G \). Thus, representations of \( G \) are the same as representations of \( \tilde{G} \) satisfying \( \rho(Z) = \mathrm{id} \). An important example of this is when \( G = \mathrm{SO}(3, \mathbb{R}) \), \( \tilde{G} = \mathrm{SU}(2) \).
\end{remark}

\begin{lemma}
    Let $\mathfrak g $ be a real Lie algebra and $\mathfrak g_{\mathbb C}$ its complexification.
    Then the categories of complex representations of $\mathfrak g$ and $\mathfrak g_{\mathbb C}$ are equivalent.
\end{lemma}
The next example shows that we can reduce the problem of study of representations of a non-compact Lie group $\SL(2, \mathbb C)$ to 
\begin{example}
    The categories of ﬁnite-dimensional representations of $\SL(2, \mathbb C)$,
$\SU(2)$, $\mathfrak{sl}(2, \mathbb C)$ and $\su(2)$ are all equivalent. Indeed,
$\mathfrak{sl}(2, C) = (\su(2))_{\mathbb C}$ , so categories of their ﬁnite-dimensional representations are equivalent; since the groups $\SU(2), \SL(2, C)$ are simply-connected, they
have the same representations as the corresponding Lie algebras.
\end{example}

\section{Operations on representations}
\subsection{Subrepresentations and quotients}

\begin{definition}
    Let $V$ be a representation of $G$ (respectively $\mathfrak g$). 
    A subrepresentation is a vector subspace $W \subseteq V$ stable under the action:
    $\rho(g)W \subseteq W$ for all $g \in G$ (respectively, $\rho(x)W \subseteq W$ for all $x \in \mathfrak g$).
\end{definition}

When $W \subseteq V$ is a subrepresentation, then one can obtain the factor representation or quotient representation $V/W$ given by
\[
\rho_{V/W}(g) = \rho_V(g) + W.
\]


\subsection{Direct sum and tensor product}
\begin{lemma}\label{lem:direct_sum_tensor_product}
    Let $V , W$ be representations of $G$ (respectively, $\mathfrak g$). 
    Then there is a canonical structure of a representation on $V^*, V \oplus W, V \otimes W$.
\end{lemma}
\begin{proof}
    It is given by
    \begin{align*}
        \rho(g)(v + w) &= \rho(g)(v) + \rho(g)(w) \text{ on } V \oplus W \text{ for } G,\\
        \rho(x)(v + w) &= \rho(x)(v) + \rho(x)(w) \text{ on } V \oplus W \text{ for } \mathfrak g,\\
        \rho(g)(v \otimes w) &= \rho(g)(v) \otimes \rho(g)(w) \text{ on } V \otimes W \text{ for } G,\\
        \rho(x)(v \otimes w) &= \rho(x)(v) \otimes w + v \otimes \rho(x)(w) \text{ on } V \otimes W \text{ for } \mathfrak g,\\
        \rho_{V^*}(g) &= \rho(g^{-1})^t \text{ on } V^* \text{ for } G,\\
        \rho_{V^*}(x) &= -\rho(x)^t \text{ on } V^* \text{ for } \mathfrak g,
    \end{align*}
    where we denote with $A^t$ the adjoint opearor $A^t:V^* \to V^*$ for $A: V \to V$.
\end{proof}
For example, the coadjoint representation is obtained by dualizing the adjoint representation of $\mathfrak g$.

Before giving the next series of examples, recall that for any finite dimensional vector spaces $V, W$, we have the following cannonical isomorphism:
\begin{align*}
    V^* \otimes W &\simeq \hom(V, W)\\
    f \otimes w &\mapsto \left( u \mapsto f(u)w \right)\\
    e^i \otimes (Ae_i) &\mapsfrom A
\end{align*}
where $e_1, \ldots, e_n$ are any basis of $V$ and $e_1, \cdots, e^n$ are its dual basis for $V^*$.

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item Coadjoint representation on $\mathfrak g^*$: Dualizing the adjoint representation of $\mathfrak g$ we obtain the coadjoint representation given by $\rho(x) = ad_x^*$
        \item $\hom(V,W)$: Given two representations $V, W$ of $G$, the space of linear maps $\hom(V,W)$ is a representation by:
        \begin{align*}
            \rho(g)_{\hom(V,W)}(A) &= \rho_V(g)A\rho_W(g^{-1}) \text{ for } g \in G,\\
            \rho(x)_{\hom(V,W)}(A) &= \rho_V(x)A - A\rho_W(x) \text{ for } x \in \mathfrak g.
        \end{align*}
        This is obtained by dualizing and tensoring as in the lemma above, i.e.\ the diagram below is commutative\\
        \begin{center}
            \begin{tikzcd}
                V \otimes W^* \arrow[r, "\simeq"] \arrow[d, "\rho(g)"] & \hom(V,W) \arrow[d, "\rho(g)"] \\
                V \otimes W^* \arrow[r, "\simeq"] & \hom(V,W)
            \end{tikzcd}    
        \end{center}
        In particular, we have that the isomorphism $\psi: V \otimes W^* \to \hom_K(V,W)$ restricts to an isomorphism $ (V^* \otimes W)^G \simeq \hom_{\mathbb K}(V, W)$.
        \item Bilinear forms: Given a representation $V$, the space of bilinear forms on $V$ becomes a representation when we consider it as a subspace of $V^* \otimes V^*$. It is given by:
        \begin{align*}
            g B(u, v) &= B(g^{-1}u, g^{-1}v), \text{ for } g \in G\\
            x B(u, v) &= -B(x u, v) - B(u, x v), \text{ for } x \in \mathfrak g.    
        \end{align*}
    \end{enumerate}
\end{example}

\subsection{Invariants}
\begin{definition}
    Let $V$ be a representation of a Lie group $G$. A vector $v \in V$ is
    called invariant if $\rho(g)v = v$ for all $g \in G$. The subspace of invariant vectors
    in $V$ is denoted by $V^G$.
    Similarly, let $V$ be a representation of a Lie algebra $\mathfrak g$. A vector $v \in V$ is
    called invariant if $\rho(x)v = 0$ for all $x \in g$. The subspace of invariant vectors
    in $V$ is denoted by $V^{\mathfrak g}$ .
\end{definition}

\begin{remark}
    When $G$ is connected, and $V$ is a representation of $G$, then
    \[
    V^G = V^{\mathfrak g}
    \]
\end{remark}

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item $\hom(V, W)$: Considering $\hom(V,W)$ as the representation arising from two representations $V, W$, we have $\hom(V, W)^G = \hom_G(V, W)$.
        \item Bilinear forms: We have $B \in V^G$ if and only if the linear map $V \to V^*$, given by $v \to B(v, -)$, is a representations morphism.
    \end{enumerate}
\end{example}

\section{Irreducible representations}
One of the main problems of the representation theory is the problem of classi-
ﬁcation of all representations of a Lie group or a Lie algebra. In this generality,
it is an extremely difﬁcult problem and for a general Lie group, no satisfactory
answer is known. We will later show that for some special classes of Lie groups
(namely compact Lie groups and semisimple Lie groups, to be deﬁned later)
this problem does have a good answer.
The most natural approach to this problem is to start by studying the sim-
plest possible representations, which would serve as building blocks for more
complicated representations.

\begin{definition}
    \begin{enumerate}[label = (\roman*)]
        \item A non-zero representation V of $G$ or $\mathfrak g$ is called irreducible
        or simple if it has no subrepresentations other than $0$ and $V$ . 
        Otherwise $V$ is called reducible.
        \item $V$ is a completely completely reducible or semisimple representation if it is isomorphic to a direct sum of irreducible representations:
        $V = \oplus_i V_i$, with $V_i$ irreducible.
        \item If $V$ is completely reducible, one usually groups together isomorphic summands writing $V = \oplus_i n_i V_i$, where $V_i$ are pair-wise non-isomorphic irreducible
        representations and the numbers $n_i$ are called multiplicities.
    \end{enumerate}
\end{definition}
\begin{remark}
    Sometimes, it will be useful to consider the isomorphic expressions:
    \[
    n V \simeq \oplus_1^n V \simeq \mathbb C^n \otimes V
    \]
    where the latter isomorphism is given by the map $(v_1, \cdots, v_n) \mapsto e_1 \otimes v_1 + \cdots  e_n \otimes v_n $ and in the other direction by the map $z \otimes v \mapsfrom (z_1v, \cdots, z_n v)$.
\end{remark}

Thus, more modest goals of the representation theory would be answering
the following questions:
\begin{enumerate}[label = (\roman*)]
    \item For a given Lie group $G$, classify all irreducible representations of G.
    \item For a given representation $V$ of a Lie group G, given that it is completely reducible, ﬁnd explicitly the decomposition of $V$ into direct sum of irreducibles.
    \item For which Lie groups $G$ all representations are completely reducible?
\end{enumerate}

\begin{example}[Irreducible representations]
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb C^n$: It is irreducible as a representation of $\SL(n, \mathbb C), \GL(n, \mathbb C)$.
    \end{enumerate}
\end{example}

\begin{example}[Reducible representations]
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb R$: It is reducible but not completely reducible as a representation of $\mathbb R$ (either seen as a group, or a Lie algebra).
        \item $\mathbb C^n \otimes \mathbb C^n$: It is completely reducible as a representation of $\GL(n, \mathbb C)$, since $\mathbb C^n \otimes \mathbb C^n = S^2 \mathbb C^n \oplus \Lambda^2 \mathbb C^n$, where $S^2 \mathbb C^n$ is the symmetric tensor product and $\Lambda^2 \mathbb C^n$ is the antisymmetric tensor product.
        \item Unitary representations are completely irreducible (see \cref{thm:unitary_implies_completeley_reducible}).
        This includes in particular representations of compact groups (see \cref{thm:compact_implies_unitary}).
        \item $\mathfrak{sl}(2, \mathbb C)$: Putting aside that we can classify all its finite-dimensional representations: any representation of $V$ is completely reducible, and any irreducible representation is isomorphic to a highest weight representation (see \cref{thm:irreducible_representations_of_sl2}).
    \end{enumerate}
\end{example}
\begin{proof}
    \begin{enumerate}[label = (\roman*)]
        \item\label{ex:R_not_completely_irreducible} 
        Looking at the Lie algebra $\mathfrak g = \mathbb R$, representations on $V$ are linear maps $\rho_*: \mathbb R \to \mathfrak{gl}(V)$ who will have the form $t \mapsto t A$, where $A = \rho_*(1)$. 
        Hence representations of $G = \mathbb R$ are the Lie group morphisms $\mathbb R \to \GL(d, \mathbb C)$ that have the form $t \mapsto e^{tA}$, where $A = \rho_*(1) \in \mathfrak{gl}(\mathbb C, d)$.
        If $v \in V$ is an eigenvector of $A$, then $\mathbb Cv$ is a subrepresentation in $V$, so the only irreducible representations of $\mathbb R$ are one-dimensional.
        In fact, $t \mapsto e^{tA}$ is completely irreducible if and only if $A$ is diagonalizable.
        Hence, the irreducible ones are given as the linear maps $\rho_*: \mathbb R \to \mathfrak{gl}(V) \simeq \mathbb C, t \mapsto t \lambda$, for some $\lambda = \lambda_{\rho_*} \in \mathbb C$, and the corresponding representation of $G$ is given by $\rho: \mathbb R \to \GL(V) \simeq \mathbb C^{\times}, t \mapsto e^{t \lambda}$.
    \end{enumerate}
\end{proof}

One tool which can be used in decomposing representations into direct sum
is the use of central elements.
\begin{lemma}
    Let $V$ be a representation of a Lie group $G$ (or a Lie algebra $\mathfrak g$ respectively).
    \begin{enumerate}[label = (\roman*)]
        \item Let $A \in \hom_G(V, V)$ be a diagonalizable interwining operator. 
        Then each eigenspace $V_\lambda$ of $A$ is an invariant subspace and we have the decomposition $V = \oplus V_\lambda$ into subrepresentations.
        \item If $Z \in Z(G)$ is a central element such that $\rho(Z)$ is diagonalizable, then as a representation of $G$, $V$ decomposes into a direct sum of eigenspaces of $\rho(Z)$: $V = \oplus V_\lambda$. 
    \end{enumerate} 
\end{lemma}

Of course, there is no guarantee that $V_\lambda$ will be an irreducible representation;
moreover, in many cases the Lie groups we consider have no central elements
at all.

\section{Intertwining operators and Schur’s lemma}
In this section we consider the following questions:
\begin{question}\label{q:invariance_eigenvalues}
How can $G$-invariance of an operator $A \in \mathfrak{gl}(V)$ helps computing eigenvalues and eigenspaces?
\end{question}

\begin{question}\label{q:centre_classical_lie_groups}
    What is an easy way to compute the centre of classical Lie groups?
\end{question} 

The following Lemma can be seen as a way to answer these questions:
\begin{lemma}[Schur's lemma]
    \begin{enumerate}[label = (\roman*)]
        \item Let $V$ be an irreducible representation of a Lie group $G$ (similar result holds for representations of a Lie algebra $\mathfrak g$.).
        Then the space of intertwining operators is one-dimensional:
        \[
        \hom_G(V, V) = \mathbb C \id.
        \]
        \item Let $V, W$ be irreducible representations of a Lie group $G$ which are not isomorphic.
        Then $\hom_G(V, W) = 0$.
    \end{enumerate}
\end{lemma}

\begin{example}[Answer to \cref{q:centre_classical_lie_groups}]
Consider the group \( \mathrm{GL}(n, \mathbb{C}) \). Since \( \mathbb{C}^n \) is irreducible as a representation of \( \mathrm{GL}(n, \mathbb{C}) \), every operator commuting with \( \mathrm{GL}(n, \mathbb{C}) \) must be scalar. Thus, the center \( Z(\mathrm{GL}(n, \mathbb{C})) = \{ \lambda \cdot \mathrm{id}, \lambda \in \mathbb{C}^\times \} \); similarly, the center of the Lie algebra is \( Z(\mathfrak{gl}(n, \mathbb{C})) = \{ \lambda \cdot \mathrm{id}, \lambda \in \mathbb{C} \} \).

Since \( \mathbb{C}^n \) is also irreducible as a representation of \( \mathrm{SL}(n, \mathbb{C}) \), \( \mathrm{U}(n) \), \( \mathrm{SU}(n) \), \( \mathrm{SO}(n, \mathbb{C}) \), a similar argument can be used to compute the center of each of these groups. The answer is
\[
\begin{array}{ll}
Z(\mathrm{SL}(n, \mathbb{C})) = Z(\mathrm{SU}(n)) = \{\lambda \cdot \mathrm{id}, \, \lambda^n = 1 \} & \mathfrak{z}(\mathrm{U}(n)) = \{ \lambda \cdot \mathrm{id}, \, |\lambda| = 1 \} \\
Z(\mathfrak{sl}(n, \mathbb{C})) = Z(\mathfrak{su}(n)) = 0 & \mathfrak{z}(\mathfrak{u}(n)) = \{ \lambda \cdot \mathrm{id}, \, \lambda \in i\mathbb{R} \} \\
Z(\mathrm{SO}(n, \mathbb{C})) = Z(\mathrm{SO}(n, \mathbb{R})) = \{\pm 1\} & \mathfrak{z}(\mathfrak{so}(n, \mathbb{C})) = \mathfrak{z}(\mathfrak{so}(n, \mathbb{R})) = 0.
\end{array}
\]
\end{example}

To answer \cref{q:invariance_eigenvalues}, we can use the following corollary of Schur's lemma:
\begin{corollary}
Let \( V \) be a completely reducible representation of Lie group \( G \) (respectively, Lie algebra \( \mathfrak{g} \)). Then
\begin{enumerate}
    \item If \( V = \bigoplus V_i \), \( V_i \) irreducible, pairwise non-isomorphic, then any intertwining operator \( \Phi : V \to V \) is of the form \( \Phi = \bigoplus \lambda_i \mathrm{id}_{V_i} \).
    \item If \( V = \bigoplus n_i V_i = \bigoplus n_i \mathbb{C}^{n_i} \otimes V_i \), \( V_i \) irreducible, pairwise non-isomorphic, then any intertwining operator \( \Phi : V \to V \) is of the form \( \Phi = \bigoplus (A_i \otimes \mathrm{id}_{V_i}) \), where \( A_i \in \mathrm{End}(\mathbb{C}^{n_i}) \).
\end{enumerate}
\end{corollary}

\begin{proof}
For part (1), notice that any operator \( V \to V \) can be written in a block form: \( \Phi = \bigoplus \Phi_{ij}, \Phi_{ij} : V_i \to V_j \). By Schur's lemma, \( \Phi_{ij} = 0 \) for \( i \neq j \) and \( \Phi_{ii} = \lambda_i \mathrm{id}_{V_i} \). Part (2) is proved similarly.
\end{proof}

This result shows that indeed, if we can decompose a representation \( V \) into irreducible ones, this will give us a very effective tool for analyzing intertwining operators. For example, if \( V = \bigoplus V_i, V_i \neq V_j \), then \( \Phi = \bigoplus \lambda_i \mathrm{id}_{V_i} \), so one can find \( \lambda_i \) by computing \( \Phi(v) \) for just one vector in \( V_i \). It also shows that each eigenvalue \( \lambda_i \) will appear with multiplicity equal to \( \dim V_i \). This is exactly what we did in the baby example above, only now \( V_i \) was one-dimensional, where we had \( G = \mathbb{Z}_n \).

We also have the following corrolary on commutative groups and algebras.
\begin{proposition}
    If $G$ is a commutative group, then any irreducible complex
representation of $G$ is one-dimensional. Similarly, if $\mathfrak g$ is a commutative Lie algebra, then any irreducible complex representation of $\mathfrak g$ is one-dimensional.
\end{proposition}
Now we are ready to give a classification of irreducible representations of $\mathbb R$ and $\mathbb S^1$
\begin{example}\label{ex:irreducible_representations_of_R}
    Let $G = \mathbb R$.
    Then the only irreducible representations of $G$ are one-dimensional (which follows either from the proposition above, or from \cref{ex:irreducible_representations_of_R}), and are given by $t \mapsto e^{\lambda t} $ for some $\lambda \in \mathbb C$.
    Similarly, the only irreducible representations of $\mathbb S^1 = \mathbb R/ \mathbb Z$ are one-dimensional complex spaces $V_k$ for $k \in \mathbb Z$, and are given by $t \mapsto e^{2\pi i k t}$.
\end{example}

\section{Complete irreducibility of unitary representations, representations on finite groups}

\begin{definition}
    A complex representation $V$ of a real group $G$ is unitary if there exists a $G$-invariant inner-product (i.e.\ positive Hermitian form) on $V$: $\rho(G) \subseteq U(V)$.
    Similarly, a representation $V$ of a Lie algebra $\mathfrak g$ is unitary if there exists a $\mathfrak g$-invariant inner-product on $V$: $\rho(\mathfrak g) \subseteq \mathfrak u(V)$, or equivalently $(\rho(x)v, w) + (v, \rho(x)w) = 0$ for all $x \in \mathfrak g, v, w \in V$.
\end{definition}

\begin{theorem}\label{thm:unitary_implies_completeley_reducible}
    Unitary representations are completely reducible.
\end{theorem}
\begin{proof}
    We proceed by induction on the dimension of $V$.
    To perform the inductive step, we use the fact that if $W$ is a subrepresentation, then $W^\perp$ is a subrepresentation as well.
\end{proof}
\begin{example}[Unitary representations]
    \begin{enumerate}[label = (\roman*)]
        \item If $G$ acts by permutations on a finite set $S$, then the representation $\rho(g)f \equaldef f \circ g^{-1}$ on the space $F(S)$ of complex functions over $S$ is unitary.
        \item Representations of finite groups.
        \item Representations of compact groups (see \cref{thm:compact_implies_unitary}), e.g.\ finite groups, $\SU(n)$, $\SO(n)$, $\mathrm{U}(n)$, $\mathrm{SO}(n)$.
    \end{enumerate}
\end{example}

\section{Haar measure on compact Lie groups}
Since the proof that finite group representations are unitary is based on the existence of a right-invariant integral on the group, it is natural to ask whether a similar result holds for a broader class of Lie groups.

\begin{definition}
    A right Haar measure on a Lie group $G$ is a measure $\mu$ on $G$ that is invariant under the right action of $G$ on itself: $R_g^* \mu = \mu$.
    Equivalently $\int_G f(g) d\mu(g) = \int_G f(gh) d\mu(g)$ for all $h \in G$ and integrable functions $f$ on $G$.
\end{definition}

\begin{theorem}
    Let $G$ be a real Lie group.
    \begin{enumerate}
        \item Then $G$ is orientable and admits an orientation that is invariant under the right action of $G$ on itself.
        \item If $G$ is compact, then for a fixed choice of right-invariant orientation on $G$, there exists a unique right-invariant top degree non-vanishing differential form $\omega$ such that $\int_G \omega = 1$.
        This form is left-invariant and left-invariant  and invariant up to a sign under $i: g 
    \mapsto g^{-1}, i^*\omega = (-1)^{\dim G} \omega$.
    \end{enumerate}
\end{theorem}

\begin{theorem}
    Let $G$ be a compact Lie group.
    Then $G$ admits a right Haar measure, which is a Borel measure, and invariant under the right and left actions of $G$ on itself, and under inversion $i: g \mapsto g^{-1}$.
\end{theorem}

\begin{example}
    The Haar measure for $G= S^1 = \mathbb R/ \mathbb Z$ is the Lebesgue measure $\d x$ on $[0, 1]$.
\end{example}

\begin{theorem}\label{thm:compact_implies_unitary}
    Any finite-dimensional representation of a compact Lie group is unitary and thus completely reducible.
\end{theorem}

\section{Orthogonality of characters and Peter--Weyl theorem}
Throughout this section, we will assume that $G$ is a compact real Lie group with Haar measure $\d g$.
In the previous section, we saw that it is completely reducible, but we have no way of knowing what the factors of $G$ are, or what the multiplicities are.
This is what we will try to discuss in this section.

When $V$ is a representation of $G$ and $v_i$ a basis for $V$, we can consider the matrix corresponding to $\rho(g) \in \GL(V)$ in this basis, and denote with $\rho_{ij}$ its coefficients.
In this way we obtain scalar-valued functions $\rho_{ij}(g)$ on $G$.

We will make use of the following inner product on functions over $G$:
\begin{equation}\label{eq:inner_product}
    (f, g) = \int_G f(g) \overline{g(g)} \d g.
\end{equation}

The next theorem tells us that irreducible representations of a compact Lie group $G$ can give us an orthogonal set of functions on $G$.
\begin{theorem}
    Let $G$ be a compact real Lie group with Haar measure $\d g$.
    \begin{enumerate}[label = (\roman*)]
        \item Let $V, W$ be non-isomorphic irreducible representations of $G$ with basis $v_i, w_a$.
        Then the corresponding matrix coefficients are orthogonal:
        \[
        \left( \rho^V_{ij}, \rho^W_{ab} \right) = 0
        \]
        for all $i,j,a,b$, where the inner-product is defined as in \cref{eq:inner_product}.
        \item Let $V$ be an irreducible representation of $G$, and $v_i$ be an orthonormal basis with respect to a $G$-invariant inner product.
        Then the matrix coefficients $\rho_{ij}(g)$ are orthogonal:
        \[
        \left(\rho_{ij}, \rho_{kl} \right) = \frac{ \delta_{ik} \delta_{jl} }{\dim V}.
        \]
    \end{enumerate}
\end{theorem}

It is evident though that the set of matrix coefficients does depend on the choice of basis for the vector spaces.
For this reason, one considers a linear combination of coefficients that is basis-independent.

\begin{definition}
    A character of a representation $V$ of a real Lie group $G$ is the scalar function on the group defined by
    \[
    \chi_V(g) = \tr_V \rho(g) = \sum_i \rho_{ii}(g),
    \]
    where $\rho_{ij}(g)$ are the matrix coefficients of $\rho(g)$ in some basis of $V$.
\end{definition}
We have the following properties:
\begin{proposition}
    Let $V$ be a representation of a real Lie group $G$.
    \begin{enumerate}[label = (\roman*)]
        \item If $V = \mathbb C$ is the trivial representation, then $\chi_V(g) = 1$.
        \item $\chi_{V \oplus W} = \chi_V + \chi_W$.
        \item $\chi_{V \otimes W} = \chi_V \chi_W$.
        \item Characters are conjugation invariant: $\chi_V(hgh^{-1}) = \chi_V(g)$.
        \item If $G$ is compact, then $\chi_{V^*} = \overline{\chi_V}$.
    \end{enumerate}
\end{proposition}

The following theorem which follows from the orthogonality of matrix coefficients, tells us that characters of irreducible representations of a compact Lie group form an orthogonal family of functions on $G$.
\begin{theorem}
    Let $G$ be a compact real Lie group with Haar measure $\d g$.
    \begin{enumerate}[label = (\roman*)]
        \item Characters of non-isomorphic irreducible representations of $G$ are orthogonal:
        \[
        \left( \chi_V, \chi_W \right) = 0
        \]
        for all non-isomorphic irreducible representations $V, W$.
        \item For any representation $V$ of $G$ we have 
        \[
            \left( \chi_V, \chi_V \right) = 1 \text{ if and only if } V \text{ is irreducible}.
        \]
        \item The set $\left\{ \chi_V : V \in \hat G \right\}$ is an orthonormal family of functions of $G$, where $\hat G$ is the set of isomorphism classes of irreducible representations of $G$.
        \item For any representation $V$ of $G$ we can compute its multiplicities via:
        \[
        n_i = \left( \chi_V, \chi_{V_i} \right),
        \]
        where $V = \oplus_i n_i V_i$, and $V_i$ are pair-wise non-isomorphic irreducible representations.
    \end{enumerate}
\end{theorem}
Note that the last part of the theorem implies in particular that the factors and the multiplicities of a completely reducible representation of a compact Lie group can be computed from its character and are unique.

    
Returning to the matrix coefficients of representations, one might ask whether it is possible to give a formulation of their orthogonality in a way that does not require a choice of basis. The answer is “yes”. Indeed, let \( v \in V \), \( v^* \in V^* \). Then we can define a function on the group \( \rho_{v^*, v}(g) \) by
\[
\rho_{v^*, v}(g) = \langle v^*, \rho(g)v \rangle,
\]
and this correspondence extends to a mapping $ V^* \otimes V \rightarrow C^\infty(G, \mathbb{C})$
This is a generalization of a matrix coefficient: if \( v = v_j, v^* = v_j^* \), we recover matrix coefficient \( \rho_{ij}(g) \).

The space \( V^* \otimes V \) has additional structure. First, we have two commuting actions of \( G \) on it, given by action on the first factor and on the second one; in other words, \( V^* \otimes V \) is a \( G \)-bimodule. In addition, if \( V \) is unitary, then the inner product defines an inner product on \( V^* \) (the simplest way to define it is to say that if \( v_i \) is an orthonormal basis in \( V \), then the dual basis \( v_i^* \) is an orthonormal basis in \( V^* \)). Define an inner product on \( V^* \otimes V \) by
\begin{equation}\label{eq:inner_product_tensors}
    \langle v_1^* \otimes w_1, v_2^* \otimes w_2 \rangle = \frac{1}{\dim V} \langle v_1^*, v_2^* \rangle \langle w_1, w_2 \rangle.
\end{equation}

\begin{theorem}[Peter-Weyl theorem]
Let \( \hat{G} \) be the set of isomorphism classes of irreducible representations of \( G \). Define the map
\begin{align*}
    m : \bigoplus_{V_i \in \hat{G}} V_i^* \otimes V_i &\rightarrow C^\infty(G, \mathbb{C}), 
\text{ by}\\
    v^* \otimes v &\mapsto \langle v^*, \rho(\cdot)v \rangle.  
\end{align*}
Then
\begin{enumerate}[label = (\roman*)]
    \item $m$ is an isomorphism of \( G \)-bimodules, i.e.\ it is a linear isomorphism that commutes with the actions of \( G \) on the domain and the codomain:
    \begin{align*}
        m((gv^*) \otimes v) &= L_g(m(v^* \otimes v)),\\
        m(v^* \otimes (gv)) &= R_g(m(v^* \otimes v)),
    \end{align*}
    where $R_g, L_g$ are the right and left actions of $G$ on $C^\infty(G, \mathbb{C})$ given by $(R_g f)(x) = f(xg)$ and $(L_g f)(x) = f(g^{-1}x)$.
    \item The map $m$ extends to a Hilber spaces isomorphism:
    \[
    \hat{\bigoplus}_{V_i \in \hat{G}} V_i^* \otimes V_i \overset{\simeq}{\rightarrow} L^2(G, \d g),
    \] 
    where $\hat \oplus$ denotes the Hilbert space direct sum i.e.\ the completion of the algebraic direct sum with respect to the metric given by inner product defined by \cref{eq:inner_product_tensors}, and $L^2(G, \d g)$ is the Hilbert space of complex-valued square-integrable functions on G with respect to the Haar measure, with the inner product defined by \cref{eq:inner_product}.
    \item The set of characters 
    $$\left\{ \chi_V : V \in \hat G \right\}$$
    is an orthonormal basis of the space $\left(L^2(G, \d g)\right)^G$ of conjugation invariant functions on $G$, where $\hat G$ denotes the isomorphism classes of irreducible representations of $G$.
\end{enumerate}
\end{theorem}

The following example suggests that the study of $L^2(G)$ can be considered as a generalisation of harmonic analysis:
\begin{example}\label{ex:peter_wheyl_fourier}
    Let \( G = \mathbb S^1 = \mathbb{R}/\mathbb{Z} \). 
    As we have already discussed, the Haar measure on \( G \) is given by \( dx \) and the irreducible representations are parametrized by \( \mathbb{Z} \): for any \( k \in \mathbb{Z} \), we have one-dimensional representation \( V_k \) with the action of \( \mathbb S^1 \) given by \( \rho(a) = e^{2\pi i k a} \). The corresponding matrix coefficient is the same as character and is given by \( \chi_k(a) = e^{2\pi i k a} \).

Then the orthogonality relation of matrix coefficients gives
\[
\int_0^1 e^{2\pi i k x} e^{2\pi i l x} \, dx = \delta_{k l},
\]
which is the usual orthogonality relation for exponents. The Peter–Weyl theorem in this case just says that the exponents \( e^{2\pi i k x}, k \in \mathbb{Z} \), form an orthonormal basis of \( L^2(\mathbb S^1, dx) \) which is one of the main statements of the theory of Fourier series: every \( L^2 \) function on \( \mathbb S^1 \) can be written as a series \( f(x) = \sum_{k \in \mathbb{Z}} c_k e^{2\pi i k x} \) which converges in \( L^2 \) metric. For this reason, the study of the structure of \( L^2(G) \) can be considered as a generalization of harmonic analysis.
\end{example}

\section{Representation theory of $\mathfrak{sl}(2, \mathbb C)$}\label{sec:representation_theory_sl2}
Throughout this section, when we consider a representation $V$ of $\mathfrak{sl}(2, \mathbb C)$, we will denote the action of $\mathfrak{sl}(2, \mathbb C)$ on $V$ by $x v$ instead of $\rho_*(x)v$.
Note also that the following results are valid for the algebras $\mathfrak{su}(2)$ and $\mathfrak{so}(3)$, since they are isomorphic to $\mathfrak{sl}(2, \mathbb C)$.
\begin{theorem}
    Any finite-dimensional representation of $\mathfrak{sl}(2, \mathbb C)$ is completely reducible.
\end{theorem}
\begin{proof}
    This is because $\mathfrak{sl}(2, \mathbb C)$ is the complexification of $\su(2)$, which is compact, and thus unitary.
\end{proof}

Recall that $\mathfrak{sl}(2, \mathbb C)$ has a basis $e, f, h$ given by
\[
h = \begin{pmatrix}
    1 & 0 \\
    0 & -1
\end{pmatrix},    
e = \begin{pmatrix}
    0 & 1 \\
    0 & 0
\end{pmatrix},
f = \begin{pmatrix}
    0 & 0 \\
    1 & 0
\end{pmatrix},
\]
that satisfy the commutation relations
\[
[h, e] = 2e, [h, f] = -2f, [e, f] = h.
\]

\begin{definition}
    Let $V$ be a representation of $\mathfrak{sl}(2, \mathbb C)$.
    A vector $v \in V$ is called a vector of weight $\lambda \in \mathbb C$ if it is an eigenvector of $h$ with eigenvalue $\lambda$:
    \[
    h v = \lambda v.
    \]
    The subspaces of vectors of weight $\lambda$ are denoted by $V[\lambda]$.
    A "highest weight" of $V$ is a weight $\lambda$ (meaning that $V[\lambda] \neq 0$) with maximal real part, i.e.\ $\Re \lambda \geq \Re \mu$ for all weights $\mu$ of $V$.
\end{definition}
\begin{lemma}
    Let $V$ be a representation of $\mathfrak{sl}(2, \mathbb C)$.
    Then for every $\lambda \in \mathbb C$:
    \[
    e V[\lambda] \subseteq V[\lambda + 2], \text{ and } f V[\lambda] \subseteq V[\lambda - 2].
    \]
\end{lemma}

Given the above relations, one can show that every finite-dimensional representation of $\mathfrak{sl}(2, \mathbb C)$ is of the form:
\[
V = \bigoplus_{\lambda \in \mathbb C} V[\lambda],
\]
called the weight decomposition of $V$.
In fact, we will show below that the weights are integers.

\begin{theorem}[Representations of $\mathfrak{sl}(2, \mathbb C)$]\label{thm:irreducible_representations_of_sl_2_C}
    Let $V$ be a finite-dimensional representation of $\mathfrak{sl}(2, \mathbb C)$, and $v \in V[\lambda]$ be a highest weight vector. 
    Then
    \begin{enumerate}[label = (\roman*)]
        \item e v = 0
        \item For 
        \[
        v^k = \frac{f^k}{k!} v, k \geq 0,
        \]
        we have
        \begin{align}\label{eq:sl2_highest_weight_relations}
            h v^k &= (\lambda - 2k) v^k,\\
            f v^k &= (k+1) v^{k+1},\\
            e v^k &= (\lambda - k + 1) v^{k-1}, k > 0.           
        \end{align}
    \end{enumerate}
    If moreover $V$ is an irreducible representation, then $V = \spa\{v^0, \cdots, v^{n-1}\}$ and $\lambda = n-1$.
    In particular, there exists a unique irreducible representation of $\mathfrak{sl}(2,\mathbb C)$ of any given dimension.
\end{theorem}
\begin{corollary}
    Let $\rho: \mathfrak{sl}(2,\mathbb C) \to \gl(2,\mathbb C)$ be a finite-dimensional representation of $\mathfrak{sl}(2, \mathbb C)$.
    Then:
    \begin{enumerate}
        \item $\rho(h)$ is diagonalisable and all its eigenvalues are integers, i.e.
        V admits a weight decomposition with integer weights:
        \[
        V = \bigoplus_{n \in \mathbb Z} V[n].
        \]
        \item $\dim V[n] = \dim V[-n]$, and for $n \geq 0$, the maps
        \begin{align*}
            \rho(e)^n: V[n] &\rightarrow V[-n],\\
            \rho(f)^n: V[-n] &\rightarrow V[n],
        \end{align*}
        are isomorphisms.
    \end{enumerate}
\end{corollary}
Although only finitely many of $v^k$ are nonzero, it is convenient to consider $V$ as a quotient of an infinite-dimensional vector space with basis $v^k$:
\begin{lemma}
    Let $\lambda \in \mathbb C$ and define $M_\lambda$ as the infinite-dimensional vector space with basis $v_0, v_1, \cdots$. Then:
    \begin{enumerate}[label = (\roman*)]
        \item The relations in \cref{eq:sl2_highest_weight_relations} make $M_\lambda$ into an infinite-dimensional representation of $\mathfrak{sl}(2, \mathbb C)$.
        \item If $V$ is a finite-dimensional reducible representation of $\mathfrak{sl}(2, \mathbb C)$, then $V$ is isomorphic to a quotient $M_\lambda / W$ for some $\lambda \in \mathbb C$ and some subrepresentation $W$ of $M_\lambda$.
    \end{enumerate}
\end{lemma}

\begin{theorem}
    Let $V_n$ be be the finite-dimensional vector space with basis $v_0, v_1, \cdots, v_n$ and the action of $\mathfrak{sl}(2, \mathbb C)$ given by
    \begin{align*}
        h v^k &= (n - 2k) v^k,\\
        f v^k &= (k+1) v^{k+1}, k < n; fv^n = 0\\
        e v^k &= (n - k + 1) v^{k-1}, k > 0; ev^0 = 0.        
    \end{align*}
    Then $V_n$ is an irreducible representation of $\mathfrak{sl}(2, \mathbb C)$ of dimension $n+1$, which we call the irreducible representation with highest weight $n$.
    \begin{enumerate}[label = (\roman*)]
        \item For $n \neq m$, $V_n$ and $V_m$ are non-isomorphic.
        \item Every finite-dimensional irreducible representation $V$ of $\mathfrak{sl}(2, \mathbb C)$ is isomorphic to $V_n$, for $n = \dim V - 1$.
    \end{enumerate}
\end{theorem}


\chapter{Structure theory of Lie algebras}
In this chapter we will discuss the structure theory of Lie algebras, with the eventual goal of the full classification of semi-siple Lie algebras and their representations.
In this chapter $\mathfrak g$ will be a finite-dimensional Lie algebra over $\mathbb K \in \{\mathbb R, \mathbb C\}$.
\begin{definition}
    Let $\mathfrak g$ be a complex semisimple Lie algebra and $\mathfrak h$ be a subalgebra of $\mathfrak g$.
    Then $\mathfrak h$ is called a Cartan subalgebra of $\mathfrak g$ if one of the following equiavalent conditions holds:
    \begin{enumerate}[label = (\roman*)]
        \item $\mathfrak h$ is nilpotent and self-normalizing:
        \[
        \mathfrak h = C(\mathfrak h) = \{x \in \mathfrak g : [x, \mathfrak h] = 0\}.
        \]
    \item $\mathfrak h$ is toral (i.e.\ abelian and consisting of semisimple elements) and self-normalizing.
    \item $\mathfrak h$ is maximal among abelian subalgebras $\mathfrak h'$ such that $\ad_{\mathfrak{g}}\mathfrak h'$ is simultaneously diagonalizable.
    \end{enumerate}
\end{definition}
\section{Universal enveloping algebra}
Note that while there exists no product in a Lie algebra $V$, for every representation $\rho: \mathfrak g \to \mathfrak{gl}{V}$ the product $\rho(x) \rho(y)$ is well-defined.
While this may indeed depend on the representation $\rho$, the commutation relations of $\mathfrak g$ are preserved by the representation, and thus the product $\rho(x) \rho(y) - \rho(y) \rho(x)$ is a well-defined (in the sense of being independent to the representation $\rho$) element of $\mathfrak{gl}{V}$.
Motivated by this, we should think of the universal enveloping algebra as the "universal" associative algebra generated by products of operators of the form $\rho(x), x \in \mathfrak g$, and subject to relations imposed by the Lie algebra structure of $\mathfrak g$.

To be more rigorous, we recall the definition of a tensor algebra generated by a vector space $V$:
\begin{definition}
    Let $V$ be a vector space over a field $\mathbb K$.
    \begin{enumerate}[label = (\roman*)]
        \item 
        The tensor algebra $T(V)$ of $V$ as the vector space
    \[
    T(V) = \bigoplus_{n=0}^\infty V^{\otimes n},
    \]
    endowed with the product
    \begin{align*}
        V^{\otimes n} \times V^{\otimes m} &\to V^{\otimes n+m},\\
        (v_1 \otimes \cdots \otimes v_n, w_1 \otimes \cdots \otimes w_m) &\mapsto v_1 \otimes \cdots \otimes v_n \otimes w_1 \otimes \cdots \otimes w_m,
    \end{align*}
    where $V^{\otimes 0} = \mathbb K$ by convention.
    It is the free algebra on the vector space $V$ in the sense that it satisfies the following universal property:
    Any linear map $f: V \to A$ from $V$ to an associative algebra $A$ extends uniquely to an algebra homomorphism $T(V) \to A$.
    \[
    \begin{tikzcd}
        V \arrow[r, "f"] \arrow[d, hook] & A\\
        T(V) \arrow[ur, dashed]
    \end{tikzcd}
    \]
    \item
    The symmetric algebra $S(V)$ is the quotient of $T(V)$ by the ideal generated by elements of the form $v \otimes w - w \otimes v$.
    Fixing a basis $x_1, \cdots, x_n$ of $V$, we have that it is isomorphic to the polynomial ring $\mathbb K[x_1, \cdots, x_n]$.
    \end{enumerate}
\end{definition}
Note that the map $i: \mathfrak g \to U \mathfrak g$ can be shown to be injective.

We can now give a rigorous definiction of the universal enveloping algebra of a Lie algebra $\mathfrak g$.
\begin{definition}
    The universal enveloping algebra $U \mathfrak g$ of a Lie algebra $\mathfrak g$ is the associative algebra with unit over $\mathbb K$ with generators $i(x), x \in \mathfrak g$, subject to relations $i(x+y) = i(x)+i(y), i(c x) = ci(x), c \in \mathbb K $ and  $i(x) i(y) - i(y) i(x) = i([x,y])$.

    Alternatively, it is the quotient of the tensor algebra $T(\mathfrak g)$ by the ideal generated by elements of the form $x \otimes y - y \otimes x - [x,y]$:
    \[
    U \mathfrak g = T(\mathfrak g) / \langle x \otimes y - y \otimes x - [x,y] : x, y \in \mathfrak g \rangle.
    \]
\end{definition}

\begin{example}
    When $\mathfrak g$ is a commutative Lie algebra, the universal enveloping algebra is the symmetric algebra $S(\mathfrak g)$.
\end{example}
Note that in this example, the universal enveloping algebra is infinite-dimensional, which is in fact the case for any non-trivial Lie algebra.

\begin{remark}
    Even when $\mathfrak g \subseteq \mathfrak{gl}(n, \mathbb K)$ is a matrix algebra, multiplication in $U \mathfrak g$ is different from multiplication of matrices. For example, let $e$ be the standard generator of $\mathfrak{sl}(2, \mathbb C)$. 
    Then $e^22 = 0$ as a $2 \times 2$ matrix, but not in $U \mathfrak g$- and for a good reason: there are many representations of $\mathfrak sl(2, C)$ in which $\rho(e)^2 \neq 0$.
\end{remark}

The universal enveloping algebra has a universal property of its own:
\begin{theorem}[Universal property of universal enveloping algebra]
    Let $A$ be an associative algebra with unit over $\mathbb K$ and a linear map $f: \mathfrak g \to A$ such that $f([x,y]) = f(x)f(y) - f(y)f(x)$.
    Then there exists a unique algebra homomorphism $\tilde f: U \mathfrak g \to A$ such that $\tilde f \circ i = f$.
\end{theorem}
Given a representation $V$, we may let $A = \mathfrak{gl}(V)$ and $f = \rho$:
\[
\begin{tikzcd}
    \mathfrak g \arrow[r, "\rho"] \arrow[d, hook] & \mathfrak{gl}(V)\\
    U \mathfrak g \arrow[ur, dashed, "\tilde \rho"]
\end{tikzcd}
\]
Noting that every ring homomorphism $\phi: R \to \mathfrak{gl}(V)$ induces an $R$-module structure on $V$ by $r \cdot v = \phi(r) v$, we can obtain:
\begin{corollary}
    The categories of representations of a (not necessarily finite-dimensional) Lie algebra $\mathfrak g$ and of $U \mathfrak g$ modules are equivalent. 
    More concretely, any representation of $\mathfrak g$ has a canonical structure of a $U \mathfrak g$ module, and conversely, every $U \mathfrak g$ module has a canonical structure of a representation of $\mathfrak g$.
\end{corollary}

The following example shows us that we can use the universal enveloping algebra to construct interwining operators between representations of a Lie algebra.
The element $C$ defined below is known as the Casmir element in $\mathfrak{sl}(2, \mathbb C)$.
\begin{example}
    Let $C = e f + f e + \frac{1}{2}h^2 \in U \mathfrak{sl}(2, \mathbb C)$.
    Then $C$ is central in $U \mathfrak{sl}(2, \mathbb C)$, i.e.\ $C a = a C $ for all $a \in U \mathfrak{sl}(2, \mathbb C)$.
    This calculation is merely based in using the commutation relations of $\mathfrak{sl}(2, \mathbb C)$ to move $a$ from one side of $C$ to the other.

    In particular, for every representation $V$ of $\mathfrak{sl}(2, \mathbb C)$, $\rho(C): V \to V$ is an interwining operator.
    In particularm, it acts as a scalar on every irreducible representation $V[n]$, and if $V$ is not irreducible, then it can be decomposed into a direct sum of irreducible representations, and $\rho(C)$ acts as a scalar on each of them.
\end{example}

\section{Poincaré--Birkhoff--Witt theorem}
We will assume throughout this section that $\mathfrak g$ is a finite-dimensional Lie algebra over a field $\mathbb K$ and $U \mathfrak g$ its universal enveloping algebra.
We have already seen that when $\mathfrak g$ is commutative, $U \mathfrak g$ is the symmetric algebra $S(\mathfrak g)$ and its infinite dimensional.
The Poincaré--Birkhoff--Witt theorem gives us a more precise answer to the question of the dimension of $U \mathfrak g$.

Unlike a polynomial algebra, the universal enveloping algebra is not graded.
Indeed, if we were to define a degree by $\deg(x_1 \cdots x_k) = k$, then the commuting relation would imply that the degree of $[x,y]$ is 1, hence homogenous sums of degree-2 elements would not be closed under addition.
Instead, $ U g$ is a filtered algebra, where the filtration is given by:
\[
U_k \mathfrak g = \spa\left\{ x_1 \cdots x_p: p \leq k, x_i \in \mathfrak g \right\}.
\]
Then $\mathfrak U g $ becomes a filtered algebra:
\[
U g = \bigcup_{k=0}^\infty U_k \mathfrak g, \quad U_k \mathfrak g \cdot U_l \mathfrak g \subseteq U_{k+l} \mathfrak g.
\]
\begin{proposition}
    \begin{enumerate}[label = (\roman*)]
        \item If $x \in U_p \mathfrak g, y \in U_q \mathfrak g$, then $[x,y] \in U_{p+q-1} \mathfrak g$.
        \item The associated graded algebra
        \[
        \mathrm{Gr}U \mathfrak g = \bigoplus_{k=0}^\infty U_k \mathfrak g / U_{k-1} \mathfrak g
        \]
        is commutative.
        \item Let $x_1, \cdots, x_n$ be an ordered basis for $\mathfrak g$.
        Then the monomials
        \[
        x_1^{k_1} \cdots x_n^{k_n}, \quad \sum k_i \leq p,
        \]
        span $U_p \mathfrak g$. 
        In particular, each $U_p \mathfrak g$ is finite-dimensional.
    \end{enumerate}
\end{proposition}

\begin{theorem}[Poincaré -- Birkhoff -- Witt theorem ]
    Let $\mathfrak g$ be a finite-dimensional Lie algebra over $\mathbb K$ and denote with $U \mathfrak g$ its universal enveloping algebra.
    \begin{enumerate}[label = (\roman*)]
        \item Let $x_1, \cdots x_n$ be an ordered basis for $\mathfrak g$.
        Then the monomials
        \[
        x_1^{k_1} \cdots x_n^{k_n}, \quad \sum k_i \leq p,
        \]
        form a basis for $U_p \mathfrak g$.
        \item The graded algebra $\mathrm{Gr}U \mathfrak g$ is isomorphic to the symmetric algebra $S(\mathfrak g)$, with the isomoprism given by
        \begin{align*}
            S^p(\mathfrak g) &\to \mathrm{Gr}^p U \mathfrak g,\\
            a_1 \cdots a_p &\mapsto a_1 \cdots a_p \mod U_{p-1} \mathfrak g.
        \end{align*}
        and the inverse isomorphism given by
        \begin{align*}
            \mathrm{Gr}^p U \mathfrak g &\to S^p(\mathfrak g),\\
            a_1 \cdots a_p  &\mapsto a_1 \cdots a_p,\\
            a_1 \cdots a_l &\mapsto 0, l < p.
        \end{align*}
    \end{enumerate} 
\end{theorem}

\begin{corollary}
    Let $\mathfrak g$ be a finite-dimensional Lie algebra over $\mathbb K$ and denote with $U \mathfrak g$ its universal enveloping algebra.
    \begin{enumerate}[label = (\roman*)]
        \item The natural map $i: \mathfrak g \to U \mathfrak g$ is injective.
        \item Let $\mathfrak g_1, g_2$ be subalgebras of $\mathfrak g$.
        If $\mathfrak g = \mathfrak g_1 \oplus \mathfrak g_2$ as a vector space, then the multiplication map
        \[
        U \mathfrak g_1 \otimes U \mathfrak g_2 \overset{\simeq}{\to} U \mathfrak g
        \]
        is a vector space isomorphism.
        \item The unversal enveloping algebra $U \mathfrak g$ is a domain (it has no zero-divisors).
    \end{enumerate}
\end{corollary}
\begin{proof}
    For the last point, follows by contradiction after we assume that $f, g \in U \mathfrak g - \left\{ 0 \right\}$ with unit leading coefficient and $f g = 0$.
\end{proof}

Clearly the isomorphism between $\mathrm{Gr}U\mathfrak g$ and $S(\mathfrak g)$ can't be extended to an isomorphism between $U \mathfrak g$ and $S(\mathfrak g)$, unless of course $\mathfrak g$ is commutative.
However the following result is true for general Lie algebras:
\begin{proposition}
    The map $S \mathfrak g \to U \mathfrak g$ given by
    \begin{align*}
        S_p \mathfrak g &\to U_p \mathfrak g,\\
        \mathrm{sym}(x_1 \cdots x_p) &\mapsto \frac{1}{p!} \sum_{s \in S_p} x_{s(1)} \cdots x_{s(p)},
    \end{align*}
    is a $\mathfrak g$-module isomorphism.
\end{proposition}
\begin{proof}
    It is clearly a $\mathfrak g$-module morphism.
    It is an isomorphism, since it is a sum of isomorphisms $S_p \mathfrak g \to U_p \mathfrak g$.
\end{proof}

\section{Lie's and Engel's theorems}
The following theorem can be thought of as the reason that solvable Lie algebras are nice enough to study.
\begin{theorem}[Lie's theorem on solvable algebras]
    Let $\mathfrak g$ be a solvable Lie algebra over $\mathbb K$. Then:
    \begin{enumerate}
        \item There exists a basis in $V$ with respect to which all operators $\rho(x)$ are upper triangular.
        \item There exists a full flag of $V$ consisting of subspaces that are stable under $\rho(\mathfrak g)$.
    \end{enumerate}
    
\end{theorem}
\begin{proof}
    The proof is inductive on $\dim \mathfrak g$ and is based on showing the following result:
\begin{center}
        Every representation $\rho: \mathfrak g \to \mathfrak{gl}(V)$ of a solvable algebra admits a common eigenvector in $V$, i.e.\  $\exists v \in V: \rho(x) v = \lambda(x) v, \forall x \in \mathfrak g$.
\end{center}
To show this, we proceed again by induction on $\dim \mathfrak g$.
It is clear for the case $\dim \mathfrak g = 1$.
In the general case, we proceed as follows:
\begin{enumerate}
    \item Let $\mathfrak h$ be a subalgebra of $\mathfrak g$ such that:
    \[
    \dim \mathfrak h = \dim \mathfrak g - 1, \quad [\mathfrak g, \mathfrak g] \subseteq \mathfrak h
    \]
    Then $\mathfrak h$ is solvable, and by the induction hypothesis, there exists some $e \in V, \lambda \in \mathfrak h^*$ such that $\rho(H)e = \lambda(H)e$ for all $H \in \mathfrak h$.
    \item Letting $X \in \mathfrak g$ such that $\mathfrak g = \mathbb K X \oplus \mathfrak h$, we consider the subspace $E$ of $V$ spanned by the vectors $e, \rho(X)e, \rho(X)^2 e, \cdots$.
    Then it suffices to show that every vector of $E$ is a common eigenvector of $\rho(\mathfrak h)$.
    To do this, we proceed in two steps:
    \begin{enumerate}
        \item For all $H \in \mathfrak h$, we show that $\pi(H)$ is upper-triangular on $E$:
        \[
        \pi(H) = \begin{pmatrix}
            \lambda(H) & * & \cdots & *\\
            0 & \lambda(H) & \cdots & *\\
            \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & \cdots & \lambda(H)
        \end{pmatrix} \text{ in } E,
        \]
        with respect to the basis $e, \rho(X)e, \cdots$.
        \item We show that $\pi(H)$ is actually diagonal on $E$.
    \end{enumerate}
\end{enumerate}
\end{proof}

We have the following corollaries of Lie's theorem:
\begin{corollary}
    \begin{enumerate}[label = (\roman*)]
        \item Any irreducible complex representation of a solvable Lie algebra is one-dimensional.
        \item If a complex Lie algebra $\mathfrak g$ is solvable, then there exists a sequence of ideals in $\mathfrak g$: $0 \subseteq I_1 \subseteq I_2 \subseteq \cdots \subseteq I_n = \mathfrak g$ such that each $I_{k+1}/I_k$ is one-dimensional.
        \item $\mathfrak g$ is solvable if and only if $[\mathfrak g, \mathfrak g]$ is nilpotent.
    \end{enumerate}
\end{corollary}

One might ask if there is an analog of Lie's theorem for nilpotent Lie algebras.
Since every nilpotent algebra is solvable, the result of Lie's theorem holds, but one could wonder whether a stronger result is true; for example whether operators can be made strictly upper-triangular.
This is however false, as we can consider a commutative Lie algebra which acts diagonally in $\mathfrak C^n$.
However, the following result - which also explains the name of nilpotent algebras - and serves as an analog to Lie's theorem. 
\begin{theorem}[Engel's theorem]\label{thm:engel}
    \begin{enumerate}[label = (\roman*)]
        \item Let $V$ be a finite-dimensional (real or complex) vector space and $\mathfrak g \subseteq \gl(V)$ be a Lie subalgebra consisting of nilpotent operators.
        Then there exists a basis in $V$ with respect to which all operators in $\mathfrak g$ are strictly upper-triangular.
        \item A Lie algebra is nilpotent if and only if all its adjoint operators are nilpotent.
        In that case, there exists a basis in $\mathfrak g$ with respect to which all operators $ad_x$ for $x \in \mathfrak g$ are strictly upper-triangular.
    \end{enumerate}
\end{theorem}

\section{The radical. Semisimple and reductive Lie algebras}
So far we have defined the notion of solvable Lie algebras, which in a sense are close to being abelian.
Now we will introduce the notion of semisimple Lie algebras, which are as far as possible from being abelian, and show that any Lie algebra is built out of a solvable and a semisimple one.

\begin{definition}
    Let $\mathfrak g$ be a Lie algebra.
    \begin{enumerate}[label = (\roman*)]
        \item The radical $\rad \mathfrak g$ is the maximal solvable ideal of $\mathfrak g$.
        In particular, it exists and it is unique.
        \item $\mathfrak g$ is called semisimple if any of the following equivalent statements holds:
        \begin{enumerate}[label=\alph*]
            \item It contains no non-zero solvable ideals
            \item  It has trivial radical: $\rad \mathfrak g = 0$
            \item The Killing form is non-degenerate.
            \item It is a direct sum of simple Lie algebras.
        \end{enumerate} 
        \item $\mathfrak g$ is called simple if any of the following equivalent statements holds:
        \begin{enumerate}[label = (\alph*)]
            \item $\mathfrak g$ is not abelian and contains no ideals other than $0$ and $\mathfrak g$
            \item The commutator $[\mathfrak g, \mathfrak g]$ is orthogonal to the whole algebra $\mathfrak g$ with respect to the Killing form: $K([\mathfrak g, \mathfrak g], \mathfrak g) = 0$.
        \end{enumerate}
        \item $\mathfrak g$ is called reductive if $\rad \mathfrak g = \mathfrak z(\mathfrak g)$.
    \end{enumerate}
\end{definition}
Note that we have the following inclusion relations:
\[
\left\{ \text{simple algebras} \right\} \subseteq \left\{ \text{semisimple algebras} \right\} \subseteq \left\{ \text{reductive algebras} \right\}
\]

The reason that we exclude abelian algebras from simple algebras, is because we do not want to include one-dimensional algebras in the class of simple algebras.
One of the reasons for this is that the following result would not hold:
\begin{proposition}
    Any simple Lie algebra is semisimple.
\end{proposition}
\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item $\mathfrak{sl}(2, \mathbb C)$ is simple:
        Indeed, recall that $\ad h$ is diagonalizable with distinct eigenvalues, so its invariant subspaces are exactly the ones spanned by some of its eigenvectors.
        If $I\subseteq \mathfrak{sl}(2, \mathbb C)$ is a non-zero ideal, then it must be invariant under $\ad h$, and thus it must be spanned by a subset of $\{e, f, h\}$.
        However, $\ad_e(\ssl(2, \mathbb C)) = \ad_f(\ssl(2, \mathbb C)) = \ad_h(\ssl(2, \mathbb C)) = \ssl(2, \mathbb C)$, so the only ideals are $0$ and $\ssl(2, \mathbb C)$.
        \item $\mathfrak{sl}(n \mathbb C), \su(n), \mathfrak{sp}(n, \mathbb C), \so(n, \mathbb C)$ are semisimple (see next section).
        \item $\gl(n, \mathbb C), \mathfrak u(n)$ are reductive. 
    \end{enumerate}
\end{example}

\begin{proposition}\label{prop:semisimple_quotient}
    For any Lie algebra $\mathfrak g$ and solvable ideal $\mathfrak b \subseteq \mathfrak g$, the quotient $\mathfrak g/ \mathfrak b$ is semisimple if and only if $\mathfrak b = \rad (\mathfrak g)$.
\end{proposition}

\begin{theorem}[Levi decomposition]
    Any Lie algebra can be written as a direct sum of a semisimple and a solvable Lie algebra:
    \[
    \mathfrak g = \rad(\mathfrak g) \oplus \mathfrak g_{ss},
    \]
    where $\mathfrak g_{ss}$ is a semisimple algebra (not necessarily an ideal) in $\mathfrak g$.
    On the other hand, if $\mathfrak g = I \oplus \mathfrak{g}_ss$ can be written as a direct sum of a semisimple subalgebra $\mathfrak g_{ss}$ and a solvable ideal $I$, then $I= \rad(\mathfrak g)$.
\end{theorem}
\begin{proof}
    The second statement follows from \cref{prop:semisimple_quotient}.
\end{proof}
\begin{example}
    The Levi decomposition for the Poincaré group $G = \SO(3, \mathbb R) \ltimes \mathbb R^3$ is given by
    \[
    \mathfrak g = \mathfrak{so}(3, \mathbb R) \oplus \mathbb R^3.
    \]
    Indeed, the commutator is given by $[(A_1, b_1), (A_2, b_2)] = ([A_1, A_2], A_1 b_2 - A_2 b_1)$, so $\mathbb R^3$ is an abelian ideal.
    Also $\so(3, \mathbb R)$ is semisimple, because $\so(3, \mathbb R)_{\mathbb C} = \mathfrak{sl}(2, \mathbb C)$ which is simple.
\end{example}

Regarding reductive algebras, the reason to consider them is based on the following result:
\begin{proposition}
    Let $V$ be an irreducible complex representation of $\mathfrak g$.
    Then any $h \in \mathfrak g$ acts on $V$ by a scalar: $\rho(h) = \lambda(h) \id_V$, and any $h \in [\mathfrak g, \rad(\mathfrak g)]$ acts on $V$ by $0$.
\end{proposition}
Since from the point of view of representation theory, having non-zero elements which act by zero in an irreducible representation complicates the theory, we are led to consider algebras that satisfy $[\mathfrak g, \rad(\mathfrak g)] = 0$, or equivalently $\rad(\mathfrak g) \subseteq \mathfrak z(\mathfrak g)$.
But since $\mathfrak z(\mathfrak g) \subseteq \rad(\mathfrak g)$, this is equivalent to $\rad(\mathfrak g) = \mathfrak z(\mathfrak g)$, which is the definition of a reductive algebra. 
Of course any semisimple algebra is reductive, but the converse is not true, as the following remark shows:
\begin{remark}
    A Lie algebra is reductive, if and only if it is the direct sum of commuting semisimple and abelian algebras:
\[
\mathfrak g = \mathfrak z \oplus \mathfrak g_{ss}, \quad [\mathfrak z, \mathfrak g_{ss}] = 0.
\]
\end{remark}

\section{Invariant bilinear forms and semisimplicity of classical Lie algebras}
Recall that a bilinear form $B: \mathfrak g \times \mathfrak g \to \mathbb K$ is called invariant if $B(\ad_x y,z) + B(y,\ad_x z) = 0$ for all $x,y,z \in \mathfrak g$.
Using it, we can obtain ideals from other ideals:
\begin{lemma}
    Let $B$ be an invariant bilinear form on $\mathfrak g$ and $I \subseteq \mathfrak g$ an ideal.
    Then $I^\perp$ is also an ideal and in particular $\ker B = \mathfrak g^\perp$ is an ideal.
\end{lemma}
\begin{proposition}
    Let $V$ be a representation of $\mathfrak g$ and define a bilinear form on $\mathfrak g$ By
    \[
    B_V(x,y) = \tr_V(\rho(x) \rho(y)).
    \] 
    Then $B_V$ is a symmetric invariant bilinear form on $\mathfrak g$.
    When $V = \mathfrak g$ is the adjoint representation, then $B_{\mathfrak g} = K$ is called the Killing form.
\end{proposition}

The forms $B_V$ that arise from the representations are useful because they provide a criterion for reductibility, as the following result shows.
Later we will see that the special case of the Killing form provides a characterisation of semisimplicity and solvability as well.
\begin{proposition}[Reductivity critierion]
    Let $\mathfrak g$ be a Lie algebra admitting a representation $V$ for which $B_V$ is non-degenerate.
    Then $\mathfrak g$ is reductive.
\end{proposition}
Using this, we can show that the classical Lie algebras are semisimple:
\begin{theorem}[Semisimplicity of classical Lie algebras]
    \begin{enumerate}[label = (\roman*)]
        \item All classical algebras are reductive.
        \item $\ssl(2, \mathbb C), \so(3,\mathbb R)$ are simple.
        \item $\mathfrak sl(2, \mathbb K), \so(n, \mathbb K)$ for $n \geq 3$ and $\su(n), \mathfrak{sp}(n, \mathbb K)$ are semisimple.
        \item $\mathfrak gl(n, \mathbb K) = \mathbb K \id \oplus \mathfrak{sl}(n, \mathbb K), \mathfrak{u}(n) = i \mathbb R \id \oplus \su(n)$ have one-dimensional center. 
    \end{enumerate}
\end{theorem}

\section{Killing form and Cartan's criterion}
\begin{theorem}[Cartan's criterion]
    Let $\mathfrak g$ be a Lie algebra over $\mathbb K$ with Killing form $K$.
    \begin{enumerate}[label = (\roman*)]
        \item $\mathfrak g$ is semisimple if and only if the Killing form $K$ is non-degenerate.
        \item $\mathfrak g$ is solvable if and only if the $K([\mathfrak g, \mathfrak g], \mathfrak g) = 0$.
        \item $\mathfrak g$ is reductive if it admits a representation $V$ such that the form $B_V(x,y) = \tr_V(\rho(x) \rho(y))$ is non-degenerate.
    \end{enumerate}    
\end{theorem}
\begin{lemma}
    Let $V$ be a complex vector space and $\mathfrak g \subseteq \gl(V)$ be a Lie subalgebra.
    If $\tr(xy) = 0$ for all $x \in [\mathfrak g, \mathfrak g], y \in \mathfrak g$, then $\mathfrak g$ is solvable.
\end{lemma}

\section{Jordan decomposition}
Throughout this section, $V$ is a finite-dimensional complex vector space.
\begin{definition}
    An operator $A \in \gl(V)$ is called semisimple if every $A$-invariant subspace $W$ of $V$ admits an $A$-invariant complement $W'$, i.e. $V = W \oplus W'$.
    An operator $A$ is called nilpotent if $A^k = 0$ for some $k \in \mathbb N$.
\end{definition}

\begin{lemma}
    \begin{enumerate}[label = (\roman*)]
        \item An operator $A \in \gl(V)$ is semisimple if and only if it is diagonalizable.
        \item If $A \in \gl(V)$ is semisimple and $W$ is an $A$-invariant subspace, then the restriction of $A$ to $W$ and to $V/W$ are semisimple.
        \item Sums of commuting semisimple/nilpotent operators are semisimple/nilpotent.
    \end{enumerate}
\end{lemma}

\begin{theorem}[Jordan decomposition]
    Any operator $A \in \gl(V)$ can be uniquely written as a sum of commuting semisimple and nilpotent operators:
    \[
    A = A_s + A_n, A_s A_n = A_n A_s.
    \]
    Moreover:
    \begin{enumerate}[label = (\roman*)]
        \item $A_s, A_n$ can be written as polynomials of $A$, that is there exists $p \in \mathbb C[t]$ that depends on $A$ such that $A_s = p(A), A_n = A - p(A)$.
        \item There exists a basis for which $A_s$ is diagonal and $A_n$ is strictly upper-triangular.
    \end{enumerate}
\end{theorem}
\begin{proof}
    The fact that $A_n$ can be strictly upper-triangular in some basis follows from the fact that $A_n$ is nilpotent and \cref{thm:engel}.
    The fact that $A_s$ will be diagonal in that basis is clear from the construction.
\end{proof}

\chapter{Complex semisimple Lie algebras}
Throughout this chapter, $\mathfrak g$ will denote a finite-dimensional semisimple Lie algebra, which when not stated otherwise, will be complex.
In the first section we show that an equivalent definition for a semisimple Lie algebra is that it is a direct sum of simple Lie algebras.
We also see that every ideal of a semisimple Lie algebra is the direct sum of simple algebras.


\section{Properties of semisimple Lie algebras}
First, we clear out that considering complex Lie algebras is not a restriction, as the following result shows:
\begin{proposition}
    Let $\mathfrak g$ be a real Lie algebra.
    Then $\mathfrak g$ is semisimple if and only if $\mathfrak g_{\mathbb C}$ is semisimple.
\end{proposition}
\begin{proof}
    Follows other straight from the definition, or from Cartan's criterion of semisimplicity.
\end{proof}
The following will be useful in showing that semisimple Lie algebras are merely direct sums of simple ones:
\begin{proposition}
    Let $I$ be an ideal in a semisimple Lie algebra $\mathfrak g$.
    Then there exists an ideal $J$ such that $\mathfrak g = I \overset{\perp}{\oplus} J$, where the orthogonality is with respect to the Killing form.
\end{proposition}
Using this, we can obtain the following characterisation:
\begin{corollary}
    A Lie algebra is semisimple if and only if it is a direct sum of simple Lie algebras.
\end{corollary}
\begin{proof}
    Let $\mathfrak g$ be semisimple.
    We prove by induction on $\dim \mathfrak g$ that it is a direct sum of simple Lie algebras.
    Indeed, it is true for $\dim \mathfrak g = 1$, since $\mathfrak g$ is trivially simple.
    Assume now that the statement is true for all Lie algebras of dimension less than $n$.
    If $\mathfrak g$ is not simple, then it has a non-trivial ideal $I$, so we can write $\mathfrak g = I \oplus I^\perp$, with the orthogonality with respect to the Killing form.
    Then $I^\perp, I^\perp$ are ideals of the semisimple algebra $\mathfrak g$, so they are semisimple as well by the Cartan criterion and the fact that the Killing form on each of them is the restriction of the Killing form of $\mathfrak g$.
    By the induction hypothesis, we can write both of them as a direct sum of simple Lie algebras.
    
    For the other direction, it suffices to show that the direct sum $\mathfrak g \oplus \mathfrak h$ of semisimple Lie algebras $\mathfrak{g}, \mathfrak h$ is semisimple.
    Since $[\mathfrak g, \mathfrak h] = 0$, we have that the matrix of the operator $\ad_{g + h} \in \gl(\mathfrak g \oplus \mathfrak h)$ is block-diagonal with respect to any basis of $\mathfrak g \oplus \mathfrak h$.
    \[
    \ad_{g+h} = \begin{pmatrix}
        \ad_g & 0\\
        0 & \ad_h
    \end{pmatrix}.
    \]
    Thus
    \[
    K^{(\mathfrak g \oplus \mathfrak h)}((g,h), (g',h')) = K^{(\mathfrak g)}(g,g') + K^{(\mathfrak h)}(h,h'),
    \]
    which is clearly non-degenerate.
\end{proof}
\begin{corollary}
    If $\mathfrak g $ is a semisimple Lie algebra, then $[\mathfrak g, \mathfrak g] = \mathfrak g$.
\end{corollary}
We now turn our attention to ideals and quotients of semisimple algebras:
\begin{proposition}
    Let $\mathfrak g$ be a semisimple Lie algebra.
    \begin{enumerate}[label = (\roman*)]
        \item Considering the decomposition $\mathfrak g = \mathfrak g_1 \oplus \cdots \oplus \mathfrak g_n$ into simple algebras, any ideal $I$ of $\mathfrak g$ is of the form
        \[
        I = \bigoplus_{j \in J} \mathfrak g_j,
        \]
        for some $J \subseteq \left\{ 1, \cdots, n \right\}$.
        \item Any ideal $I$ of $\mathfrak g$ is semisimple, and any quotient $\mathfrak g/I$ is semisimple.
    \end{enumerate}
\end{proposition}

The next proposition tells us that For semisimple Lie algebras, all automorphisms are inner.
\begin{proposition}
    Let $\mathfrak g$ be a semisimple Lie algebra, and $G$ a connected Lie group with Lie algebra $\mathfrak g$.
    Then $\mathrm{Der}\mathfrak g = \ad(\mathfrak g) \simeq \mathfrak g$, and $\mathrm{Aut}(\mathfrak g)/\Ad(G)$ is discrete.
\end{proposition}

\begin{theorem}\label{thm:compact_real_form}
    Let $\mathfrak g$ be a complex semisimple Lie algebra.
    Then there exists a subalgebra $\mathfrak k$ that is a real form for $\mathfrak g$ and that $\mathfrak k$ is the algebra of a compact group $K$.
    The subalgebra $\mathfrak k$ is unique up to conjugation and is called the compact real form of $\mathfrak g$.
    
    If moreover $G$ is a connected Lie group with algebra $\mathfrak g$, then we can choose $K\subseteq G$, in which case we say that $K$ is the compact real form of $G$.
\end{theorem}
\section{Complete reducibility for semisimple algebras}
Throughout we assume that $\mathfrak g$ is a complex semisimple Lie algebra and $V$ is a finite-dimensional complex representation of $\mathfrak g$. 
\begin{theorem}
    Every complex finite-dimensinoal representation of a complex semisimple Lie algebra $\mathfrak g$ is completely irreducible.
\end{theorem}
\begin{proof}
    The following method is called Weyl's unitary trick.
    Let $\mathfrak k$ be the real compact form of $\mathfrak g$, defined in \cref{thm:compact_real_form}, and $K$ be the compact group with Lie($K$) = $\mathfrak k$.
    Then the representations of $\mathfrak k, K$ and $\mathfrak g$ are the same, and $K$ is compact, so we know that they are all completely reducible.
\end{proof}

\section{Semisimple elements and toral subalgebras}
\begin{definition}
    An element $x \in \mathfrak g$ is called semisimple if the operator $\ad_x : \mathfrak g \to \mathfrak g$ is semisimple (i.e.\ diagonalizable).
    It is called nilpotent if the operator $\ad_x$ is nilpotent.
\end{definition}
One can show (in \cref{ex:semisimple_elements}) that in the case of $\mathfrak g = \mathfrak{gl}(n ,\mathbb C)$, this definition coincides with the definition of semisimple operators.

The following theorem is a generalisation of the Jordan decomposition:
\begin{theorem}
    Let $\mathfrak g$ be a complex semisimple Lie algebra. 
    \[
    \mathfrak g = \left\{ \text{semisimple elements of} \right\} \oplus \left\{ \text{nilpotent elements of} \right\}
    \]
    In particular every nontrivial semisimple Lie algebra has nonzero semisimple elements.
    Moreover, writing the decomposition $x = x_s + x_n$, then $ad_x y = 0$ for some $y \in \mathfrak g$ implies that $ad_{x_s}(y) = 0$.
\end{theorem}
\begin{definition}
    A subalgebra $\mathfrak h \subseteq \mathfrak g$ is called toral if all its elements are semisimple and it is commutative.
    In that case, for every $\alpha \in \mathfrak h^*$ we define the root subspace
    \[
    \mathfrak g_\alpha = \left\{ x \in \mathfrak g : \ad_h x = \langle \alpha, x \rangle x \right\}
    \]
    and the set of roots 
    \[
    R = \left\{ \alpha \in \mathfrak h^* : \mathfrak g_\alpha \neq 0 \right\}.
    \]
\end{definition}
Note that the above definitions imply that $\mathfrak h \subseteq \mathfrak g_0$.
Nevertheless, the reason that we introduced the above definitions is that every toral algebra gives us a root decomposition.
\begin{theorem}
    Let $\mathfrak g$ be a complex semisimple algebra, $\mathfrak h \subseteq \mathfrak g$ a toral subalgebra, and $(\cdot, \cdot)$ be a non-degenerate invariant symmetric bilinear product (e.g.\ the Killing form) on $\mathfrak g$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item \[
        \mathfrak g = \bigoplus_{\alpha \in R} \mathfrak g_{\alpha}
        \]
        \item $[\mathfrak g_\alpha, \mathfrak g_\beta] \subseteq \mathfrak g_{\alpha+\beta}$.
        \item If $\beta \neq - \alpha$, then $\mathfrak g_\beta \perp \mathfrak g_\alpha$ with respect to the product $(\cdot, \cdot)$.
        \item For every $\alpha \in R$, the product $(\cdot, \cdot)$ induces a non-degenerate pairing $\mathfrak g_\alpha \otimes \mathfrak g_{-\alpha} \to \mathbb C$, i.e.\ for every $x \in \mathfrak g_\alpha$, there exists some $y \in \mathfrak g_{-\alpha}$ such that $(x,y) \neq 0$.
        \item When restricted to $\mathfrak g_0$, the form $(\cdot, \cdot)$ is non-degenerate.
        \item $\mathfrak g_0$ is a reductive subalgebra in $\mathfrak g$.
    \end{enumerate}
\end{theorem}

We also have the following lemma describing one property of the root decomposition:
\begin{lemma}
    Let $\mathfrak g $ be a complex semisimple algebra.
    Writing $x = x_x + x_n$ for the Jordan decomposition of some $x \in \mathfrak g_0$, we have $x_s, x_n \in \mathfrak g_0$.
\end{lemma}

\section{Root decomposition and root systems}

\begin{theorem}\label{thm:cartan_subalgebras_of_semisimple_algebras}
    Let $\mathfrak g_1, \cdots, \mathfrak g_n$ be simple Lie algebras and $\mathfrak g = \mathfrak g_1 \oplus \cdots \oplus \mathfrak g_n$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item Let $\mathfrak h_i \subseteq \mathfrak g_i$ be Cartan subalgebras of $\mathfrak g_i$ and $R_i \subseteq \mathfrak h_i^*$ be the corresponding root systems of $\mathfrak g_i$.
        Then $\mathfrak h = \mathfrak h_1 \oplus \cdots \oplus \mathfrak h_n$ is a Cartan subalgebra of $\mathfrak g$ and $R = R_1 \sqcup \cdots \sqcup R_n$ is the corresponding root system of $\mathfrak g$.
        \item Each Cartan subalgebra in $\mathfrak g$ has the form $\mathfrak h = \oplus \mathfrak h_i$ where $\mathfrak h_i \subseteq \mathfrak g_i$ is a Cartan subalgebra of $\mathfrak g_i$. 
    \end{enumerate}
\end{theorem}

\begin{lemma}
    Let $\mathfrak g$ be a complex semisimple Lie algebra and $(\cdot, \cdot)$ be a non-degenerate invariant symmetric bilinear form on $\mathfrak g$.
    Then:
    \begin{enumerate}[label = (\roman*)]
        \item For every root $\alpha \in R: (\alpha, \alpha) \neq 0$.
        \item Let $e \in \mathfrak g_\alpha, f \in \mathfrak g_{-\alpha}$ be such that $(e, f) = 2/(\alpha, \alpha)$, and let 
        \[
        h_\alpha = \frac{2 H_\alpha}{(\alpha, \alpha)}.
        \]
        Then $\langle h_\alpha, \alpha \rangle = 2$ and the elements $h_\alpha, e, f$ satisfy the commutation relations of the algebra $\sl(2, \mathbb C)$.
        We denote with $\mathfrak{sl}(2, \mathbb C) \subseteq \mathfrak g$ the subalgebra that they generate.
        \item So defined, $h_\alpha$ is independent of the choice of non-degenerate invariant bilinear form $(\cdot, \cdot)$.
    \end{enumerate}
\end{lemma}

\chapter{Root systems}
\section{Abstract root systems}
We fix a euclidean space $E$, i.e.\ a finite-dimensional real vector space with a positive definite inner product $(\cdot, \cdot)$.
\begin{definition}
    \begin{enumerate}
        \item An \emph{abstract root system} is a finite set $R \subseteq E$ such that:
        \begin{enumerate}[label = (\roman*)]
            \item $R$ generates $E$ as a vector space.
            \item For any two roots $\alpha, \beta \in R$, the length of the projection of $\alpha$ onto the line spanned by $\beta$ is a half-integer multiple of $\beta$, i.e.
            \[
            n_{\alpha \beta} = \frac{2(\alpha, \beta)}{(\beta, \beta)} \in \mathbb Z.
            \]
            \item The reflection along root gyperplanes $L_\alpha = \alpha^\perp$ sends roots to roots, i.e. for $\alpha \in R, s_\alpha \in \GL(E)$ defined by
            \[
            s_{\alpha}(\lambda) = \lambda - \frac{2(\alpha, \lambda)}{(\alpha, \alpha)} \alpha,
            \]
            we have $s_\alpha(R) = R$.
        \end{enumerate}
        \item The number $r = \dim E$ is called the rank of the root system.
        \item We say that $R$ is a \emph{reduced root system} if in addition it satisfies the following property:
        \[ \text{If } \alpha, c \alpha \in R, \text{ then } c = \pm 1.\]
        \item For each root $\alpha$ we define the coroot $\alpha^\vee \in E^*$ by 
        \[
        \langle \alpha^\vee, \lambda \rangle = \frac{2(\alpha, \lambda)}{(\alpha, \alpha)}.
        \]
        \item An element $t \in E$ is called regular, if for all $\alpha \in R$ we have $(t, \alpha) \neq 0$.
        \item Given a regular element, we define the corresponding polarisation of $R$ as a decomposition $R = R^+ \sqcup R^-$, where the positive roots $R_+$ and negative roots $R_-$ are defined as
        \[
        R^+ = \left\{ \alpha \in R : (\alpha, t) > 0 \right\}, \quad R^- = \left\{ \alpha \in R : (\alpha, t) < 0 \right\}.
        \]
        \item Given a polarisation, a positive root $\alpha \in R_+$ is called simple if it cannot be written as a sum of two positive roots.
        The set of simple roots is denoted by $\Pi \subseteq R_+$.
        \item Two root systems $R_1 \subseteq E_1, R_2 \subseteq E_2$ are called isomorphic if there exists an isomorphism $\varphi: E_1 \to E_2$ such that $\varphi(R_1) = R_2$, and $n_{\varphi(\alpha) \varphi(\beta)} = n_{\alpha \beta}$ for all $\alpha, \beta \in R_1$.
        \item The Weyl group is the group of automorphisms of $R$ generated by the reflections $s_\alpha, \alpha \in R$.
        \item The reflections $s_{\alpha_i}$ that correspond to simple roots $\alpha_i \in \Pi$ are called simple reflections.
        \item The weight lattice $P\subseteq E$ is defined as
        \[
        P = \left\{ \lambda \in E : \langle \lambda, \alpha^\vee \rangle \in \mathbb Z \text{ for all } \alpha \in R \right\}.
        \]
    \end{enumerate}
\end{definition}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item For an abstract root system, $\alpha, c \alpha \in R$ implies that $c \in \left\{ \pm 1, \pm 2, \pm \frac{1}{2} \right\}$.
        There exist however cases of abstract root systems that are not reduced.
        \item With the definitions above, we have the following relations for $\alpha, \beta \in R, \lambda \in E$:
        \begin{align*}
            \langle \alpha^\vee,  \alpha \rangle &= 2,\quad n_{\alpha \beta} = \langle\alpha, \beta^\vee \rangle, \quad s_\alpha(\lambda) = \lambda - \langle \lambda, \alpha^\vee \rangle \alpha.
        \end{align*}
    \end{enumerate}
\end{remark}

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item (Semisimple algebras) Let $\mathfrak g$ be a semisimple Lie algebra and $\mathfrak h \subseteq \mathfrak g$ a Cartan subalgebra.
        Then the set of roots $R = \left\{ \alpha \in \mathfrak h^* - \{0\} : \mathfrak g_\alpha \neq 0 \right\}$ is a reduced root system for $\mathfrak h_{R}^*$.
        \item ($A_{n - 1}$, the root system of $\mathfrak{sl}(n, \mathbb C)$) Let
        \begin{align*}
            E &= \left\{\lambda \in \mathbb R^n: \sum_{i} \lambda_i = 0\right\},\\
            R &= \left\{e_i - e_j: i\neq j\right\},\\
            s_{e_i - e_j} &= \text{ swapping of coordinates } i, j,\\
            R_+ &= \left\{e_i - e_j: i < j\right\},\\
            \Pi &= \left\{\alpha_i = e_i - e_{i+1}: i = 1, \cdots, n-1\right\},\\
            W &= S_n = \text{ transpositions of coordinates of } \mathbb R^n,\\
            P &= \left\{\left( \frac{m}{n}, \frac{m}{n} - k_2, \frac{m}{n} - k_n \right) \in \mathbb R^n :k_2, \cdots, k_n, m \in \mathbb Z, k_2 + \cdots + k_n = m\right\}.
        \end{align*}
        \item (rank $2$ root systems) All rank 2 systems of $\mathbb R^2$ are shown in \cref{fig:rank_two_systems}.
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{rank two root systems.png}
            \caption{Rank 2 root systems}
            \label{fig:rank_two_systems}
        \end{figure}
        
    \end{enumerate}
\end{example}

\section{Weyl group}
\begin{lemma}
    Let $R$ be a root system in $E$ and denote with $W$ the Weyl group of $R$. Then:
    \begin{enumerate}[label = (\roman*)]
        \item $W$ is a finite subgroup of $\mathrm{O}(E)$ and the root system $R$ is invariant under $W$.
        \item For any $w\in W, \alpha \in R: s_{w(\alpha)}=w s_\alpha w^{-1}$.
    \end{enumerate}
\end{lemma}

\section{Pairs of roots and rank two root systems}
For this section, we assume that $R$ is a reduced root system.
\begin{proposition}
    Let $\alpha, \beta \in R$ be roots that are not multiples of each other, with $|\alpha| \geq |\beta|$, and let $\phi$ be the angle between them.
    Then we must have one of the following possivilities:
\end{proposition}

\section{Positive roots and simple roots}
\begin{lemma}
    If $\alpha, \beta \in \Pi$ are simple, then $(\alpha, \beta) \leq 0$.
\end{lemma}
\begin{proposition}
    Let $R = R_+ \sqcup R_-$ be a polarised root system.
    Then the simple roots form a basis of the vector space $E$. 
\end{proposition}
The following lemma is useful in showing linear dependence:
\begin{lemma}
    Let $v_1, \cdots, v_k$ be a collection of non-zero vectors of $E$ such that $(v_i, v_j) \leq 0$ for all $i,j$.
    Then the vectors are linearly dependent.
\end{lemma}


\section{Simple reflections}

\begin{example}
    Simple reflections have length one, beceause the Weyl chambers $\alpha_i(C_+), C_+$ are seperated by exactly one hyperplane, namely $L_{\alpha_i}$:
    \[
    \left\{ \alpha \in R_+ : s_i(\alpha) \in R_- \right\} = \{\alpha_i\}.
    \]
    Indeed, we begin by noting that $L_{\alpha}$ seperates $s_{\alpha_i}(C_+), C_+$ if and only if $(s_{\alpha_i}(\alpha), C_+) < 0$.
    Considering first $\alpha = \alpha_i$, we have $s_{\alpha_i}(\alpha_i) = -\alpha_i$, so for every $\lambda \in C_+$ we have $\left(s_{\alpha_i}(\alpha_i), \lambda\right) = -(\alpha_i, \lambda) < 0$, so $L_{\alpha_i}$ seperates $C_+$ and $s_{\alpha_i}(C_+)$.
    On the other hand, no other simple root $\alpha_j$ seperates $C_+$ and $s_{\alpha_i}(C_+)$, because $(\alpha_j, C_+) > 0$ for all $j$ and $s_{\alpha_i}$ permutes the elements of $R_+ - \{\alpha_i\}$:
    \[
    \left( s_{\alpha_i}(\alpha_j), \lambda \right) = (\alpha_j, \lambda) - \frac{2(\alpha_i, \alpha_j)}{(\alpha_i, \alpha_i)}(\alpha_i, \lambda) > 0,
    \]
    for $\lambda \in C_+$, by recalling that $(\alpha_i, \alpha_j) \leq 0$.
\end{example}


\section{Theorem of highest weight}
Let $\mathfrak g$ be a complex semisimple Lie algebra and fix a Cartan subalgebra $\mathfrak h \subseteq \mathfrak g$, and denote with $\mathfrak h_0$ the real form of $\mathfrak h$, which is given as the set of functionals in $\mathfrak h$ on which all roots are real valued.
We denote with $\Delta$ the set of roots of $\mathfrak g$ with respect to $\mathfrak h$ and consider a nondegenerate symmetric invariant bilinear form $B$ on $\mathfrak g$.
Using $B$ to identify $\mathfrak h$ with $\mathfrak h^*$, we consider the dual element $H_\alpha \in \mathfrak h$ of $\alpha \in \Delta$, in which case one has that $\mathfrak h_0 = \sum_{\alpha \in \Delta} \mathbb R H_\alpha$.
We denote with $(\cdot, \cdot)$ the inner product on $\mathfrak h^*$ induced by $B$. We fix an ordering on $\mathfrak h_0^*$ and denote with $\Delta^+$ the set of positive roots and $\Pi$ the set of simple roots.
\begin{definition}
    Let $\phi : \mathfrak g \to \mathfrak{gl}(V)$ be a complex finite-dimensional representation of a complex semisimple algebra $\mathfrak g$.
    \begin{enumerate}[label = (\roman*)]
        \item For $\lambda \in \mathfrak h^*$, we define the \emph{weight space} $V_\lambda$ as
        \[
        V_\lambda = \left\{ v \in V : \rho(h)v = \langle h, \lambda \rangle v, \text{ for all } h \in \mathfrak h \right\}.
        \]
        When $V_\lambda \neq 0$, we call $\lambda$ a \emph{weight} of $V$, and the elements of $V_\lambda$ are called \emph{weight vectors}.
        \item We say that a weight $\lambda \in \mathfrak h^*$ is a \emph{highest weight} if it is the largest with respect to the ordering on $\mathfrak h^*$.
        \item We say that a weight $\lambda \in \mathfrak h^*$ is \emph{dominant} if $\left( \lambda, \alpha \right) \geq 0 $ for all positive roots $\alpha \in \Delta^+$.
        \item We say that a weight $\lambda \in \mathfrak h^*$ is \emph{algebraically integral} if $\left( \lambda, \alpha \right)/|a|^2 \in \mathbb Z$ for all roots $\alpha \in \Delta$.
    \end{enumerate}
\end{definition}

\begin{lemma}[Weight decomposition of semisimple algebra representations]
    Let $\phi : \mathfrak g \to \mathfrak{gl}(V)$ be a complex finite-dimensional representation of a complex semisimple algebra $\mathfrak g$.
    Then:
    \begin{enumerate}[label = (\roman*)]
        \item The action of $\phi(\mathfrak h)$ is simultaneously diagonalisable, so $V$ admits a \emph{weight space decomposition}:
        \[
        V = \bigoplus_{\lambda \in \mathfrak h^*} V_\lambda.
        \]
        \item Every weight takes real values on $\mathfrak h_0$ and is algebraically integral.
        \item Roots $\alpha \in \Delta$ and weights $\lambda \in \mathfrak h^*$ are related by the formula
        \[
        \phi(\mathfrak g_\alpha) V_\lambda \subseteq V_{\lambda + \alpha}.
        \]
    \end{enumerate}
\end{lemma}
\begin{proof}
    We use the analogous facts for the representations of $\mathfrak{sl}(2, \mathbb C)$ by restricting $\phi$ to the subalgebra $\mathfrak{sl}_\alpha$ of $\mathfrak g$ generated by $H_\alpha' = 2H_\alpha/|\alpha|^2 \in \mathfrak h$ and $E_{\pm \alpha}' \in \mathfrak g_{\pm a}$.
\end{proof}
\begin{theorem}[Theorem of highest weight]
    For every semisimple Lie algebra $\mathfrak g$, we have a bijective correspondance between the set of equivalency classes of finite-dimensional irreducible representations of $\mathfrak g$ and the set of dominant algebraically integral weights of $\mathfrak g$.
    \begin{align*}
        \left\{ 
            \begin{array}{c}
                \text{Finite-dimensional irreducible} \\
                \text{representations of } \mathfrak g
            \end{array}
        \right\}/\tilde \leftrightarrow \left\{ 
            \begin{array}{c}
                \text{Dominant algebraically integral} \\
                \text{weights of } \mathfrak g
            \end{array}
        \right\}
    \end{align*}
\end{theorem}
\begin{example}\cite[Paragraph 5.1]{knapp1996lie}
\begin{enumerate}[label = (\roman*)]
    \item $G = \SU(n)$, $\mathfrak g = \mathfrak{su}(n)^\mathbb C = \mathfrak{sl}(n, \mathbb C)$:
    \item \begin{enumerate}[label = (\alph*)]
        \item Consider the representation $\phi$ of $\mathfrak g$ on the space $V$ of homogenous polynomials of degree $N$ in the variables $z_1, \bar z_1, \cdots, z_n, \bar z_n$.
        It turns out that the weight decomposition of $V$ is the decomposition into sums of monomials.
    
        Considering $H = \diag(i t_1, \cdots, it_n )$, we have
        \[
        \phi(H)P(z,\bar z) = \sum_{j=1}^n i t_j \left( z_j \frac{\partial}{\partial z_j} - \bar z_j \frac{\partial}{\partial \bar z_j} \right) P(z, \bar z).
        \]
        Using this, we can see that the monomial $P = z_1^{k_1} \cdots z_n^{k_n} \bar{z_1}^{l_1} \cdots \bar{z_n}^{l_n} $ is a weight vector corresponding to the weight
        \[
        \sum_{j=1}^n (k_j - l_j) e_j,
        \]
        where $e_j \in \mathfrak h^*$ is the projection of the $j$-th diagonal entry: $e_j(H) = ih_j$.
        In fact, the weights of the representation are exactly:
        \[
        \sum_{j=1}^n (k_j - l_j) e_j, \quad k_j, l_j \in \mathbb Z_{\geq 0}, \quad \sum_{j=1}^n (k_j + l_j) = N,
        \]
        since if $P = \sum_{(k,l)} a_{kl} P_{kl}$ is a decomposition of a weight vector into monomials, then
        \[
        \phi(H)P = \sum_{(k,l)} a_{kl} \left( \sum_{j=1}^n (k_j - l_j) e_j \right) P_{kl},
        \]
        which is equal to a scalar times $P$ if and only if the weights corresponding to nonzero coefficients $a_{kl}$ coincide, implying that at most one coefficient is nonzero.
        Moreover, the same arguements imply that the weight vectors for each tuple $(k,l)$ is the span of the corresponding monomial and hence the weight decomposition of the representation is the decomposition into sums of monomials.
    
        Considering the usual ordering on $\mathfrak h_0^*$, we see that the highest weight is $\lambda = N e_1$.
        The subspace of holomorphic polynomials (i.e.\ no monomials containing $\bar z_j$) has highest weight $-N e_n$.
        \item Consider the representation $\phi: g \to \mathfrak{gl}(\bigwedge^k \mathbb C^n)$, given by
        \[
        \phi(X) \epsilon_{i_1} \wedge \cdots \wedge \epsilon_{i_k} = \sum_{j=1}^n e_1 \wedge \cdots \wedge X e_{i_j} \wedge \cdots \wedge \epsilon_{i_k},
        \]
        where $\epsilon_1, \cdots, \epsilon_n$ is the standard basis of $\mathbb C^n$.
        Then it is easy to see that $\phi(H) \epsilon_{i_1} \wedge \cdots \wedge \epsilon_{i_k}$ is a weight vector with weight $e_{i_1} + \cdots + e_{i_k}$.
        As above, we can see that the weights are exactly the sums
        \[
        \lambda_i = \sum_{j=1}^n e_{i_j}, \quad i_1 < \cdots < i_k.
        \]
        and the weight spaces are spanned by the corresponding basis elements:
        \[
        V_{\lambda_i} = \mathbb C \epsilon_{i_1} \wedge \cdots \wedge \epsilon_{i_k}.
        \]
        The highest weight is $\lambda = e_1 + \cdots + e_k$.
    \end{enumerate}
    
\end{enumerate}
\end{example}


%========================

\appendix

\chapter{Covering theory reminder}
In this chapter we recall certain facts and definitions from basic covering theory.
A nice reference for these, is the Chapter 2 from \cite{hatcher2002topology}.
We begin by defining covering spaces.
\begin{definition}
    A covering map is a continuous surjective map $p: \tilde X \to X$ such that for every $x \in X$ there exists an open neighborhood $U$ of $x$ such that $p^{-1}(U)$ is a disjoint union of open sets in $\tilde X$, each of which is mapped homeomorphically onto $U$ by $p$.
\end{definition}
The prototypical example of a covering map is $p: \mathbb S^1 \to \mathbb S^1, p(z) = z^n$.

\section{Lifting properties}
One of the particular characteristics of covering spaces are their lifting properties, that we will recall below.
\begin{proposition}[Homotopy lifting property]
    Let $p: \tilde X \to X$ be a covering space and a homotopy $f_t: Y \to X$.
    Then every lift $\tilde f_0:Y \to \tilde X$ of $f_0$ extends to a unique homotopy $\tilde f_t$ lifting $f_t$.
\end{proposition}
\begin{proof}
    See \cite[Proposition 1.30]{hatcher2002topology}.
\end{proof}
This in particular implies the path lifting property of covering spaces:
\begin{corollary}
    Let $p: \tilde X \to X$ be a covering space.
    Then for every path $\gamma: I \to X$ and every lift $\tilde x_0$ of some point $x_0 \in X$ admits a unique lift $\tilde \gamma: I \to \tilde X$ of $\gamma$ starting at $\tilde x_0$.
\end{corollary}
Both of the results above imply that path-homotopies lift to pathhomotopies, where we require for a path homotopy to keep the endpoints of paths fixed.
We also have the following corollary that is useful in proving the lifting criterion below.
\begin{corollary}
    The image subgroup $p_*(\pi_1(\tilde X,\tilde x_0))$ consists of the homotopy classes of loops in $X$ based at $x_0$ whose lifts to $\tilde X$ starting at $\tilde x_0$ are loops.
\end{corollary}
\begin{proof}
    See \cite[Corollary 1.31]{hatcher2002topology}.
\end{proof}
If we care about lifting maps and not homotopies, we have the following criterion that tells us when a lift exists.
Namely when $f$ sends loops to loops that lift to loops.
\begin{proposition}[Lifting criterion]
    Let $p: (\tilde X, \tilde x_0) \to (X,x_0)$ be a covering space and $f: (Y,y_0) \to (X,x_0)$ with $Y$ being path-connected and locally path-connected.
    Then a lift $\tilde f: (Y, y_0) \to (\tilde X, \tilde x_0)$ exists if and only if $f_*(\pi_1(Y,y_0)) \subset p_*(\pi_1(\tilde X,x_0))$.
\end{proposition}
\begin{proof}
    See \cite[Proposition 1.33]{hatcher2002topology}.
\end{proof}
And regarding uniqueness of lifts:
\begin{proposition}
    Let $p: \tilde X \to X$ be a covering space and $f: Y \to X$ be a map.
    If $Y$ is connected, then any two lifts $\tilde f_1, \tilde f_2: Y \to \tilde X$ of $f$ that coincide at one point will coincide everywhere on $Y$.  
\end{proposition}
\begin{proof}
    See \cite[Proposition 1.34]{hatcher2002topology}.
\end{proof}



\section{Universal covering}
In this section we will be concerned with proving that under mild conditions, a space has a universal covering, i.e.\ a simply connected covering space.
\begin{assumption}
We consider a topological space $X$ that will be path-connected, locally connected and semi-locally simply connected.
\end{assumption}
While the first two assummptions may seem rather natural, we will now explain the third one.
\begin{definition}
    A space $X$ is semi-locally simply connected if for every $x \in X$ there exists an open neighborhood $U$ of $x$ such that every loop in $U$ based at $x$ is homotopic in $X$ to a constant loop in $U$.
    In other words, the homomorphism
    \[
    \pi_1(U, x) \to \pi_1(X, x)
    \]
    induced by the inclusion is trivial.
\end{definition}
To motivate this assumption, we note that it is necessary for the existence of a universal covering.
Indeed, if $p: \tilde X \to X$ is a universal covering, $x_0 \in X$ and $U$ an evenly covered neighborhood of $x_0$, then any loop in $U$ based at $x$ lifts to a loop in $\tilde X$ based at some $\tilde x_0$.
Since $\tilde X$ is simply connected, this loop is homotopic to a constant loop in $\tilde X$ and the homotopy projects down to a homotopy in $X$.

Before constructing the universal cover, we remark that every universal cover $\tilde X$ can be thought of as homotopy classes of paths in $X$ starting at some fixed point $x_0$.
\begin{remark}
    Let $p: \tilde X \to X$ be a universal covering and $x_0 \in X$.
    Then the set of homotopy classes of paths in $X$ starting at $x_0$ is in bijection with $\tilde X$.
    Indeed, given a path $\gamma: I \to X$ starting at $x_0$, we associate it to its endpoint, which defines a map $\tilde \gamma: I \to \tilde X$.
    This map is well-defined since homotopic paths have the same endpoint.
    It is surjective, beccause $X$ is path-connected, while injectivity follows from the fact that $\tilde X$ is simply connected.
\end{remark}

We now proceed with the construction of the universal cover by fixing some $x_0 \in X$ and letting
\[
\tilde X \equaldef \left\{ [\gamma] \mid \gamma \text{ is a path in } X \text{ starting at } x_0 \right\},
\]
and defining the projection map $p: \tilde X \to X$ by $p\left([\gamma]\right) = \gamma(1)$.

To define a topology on $\tilde X$, we first consider a convenient basis of open sets for $X$:
\[
\mathcal U \equaldef \left\{ U \subseteq X \text{ open and path-connected} \mid \pi_1(U, x) \to \pi_1(X,x) \text{ is trivial for some } x \in U \right\}.
\]
Note that the above set is well-defined since if there exists some $x\in U$ such that $\pi_1(U,x) \to \pi_1(X,x)$ is trivial, then it is trivial for all $x' \in U$ because $U$ is path-connected.
To see that it is a basis for $X$,  we note that $V \in \mathcal U$ for every a path-connected open subset $V\subseteq U$.

To each homotopy class $[\gamma] \in \tilde X$ and $U \in \mathcal U$ we associate the set
\[
U_{[\gamma]} \equaldef \left\{ [\gamma \cdot \eta] \in \tilde X \mid \eta \text{ is a path in } U \text{ such that } \eta(0) = \gamma(1) \right\}.
\]
Then, given $x \in X$ and a neighborhood $U$ of $x$, the collection
\[
U_{[\gamma]} \text{ for paths } \gamma \text{ going from } x_0 \text{ to } x  
\]
will be the sheets of $U$.
The following property is crucial into showing that the topology defined by the above basis is well-defined:
\[
U[\gamma] = U_[\gamma'] \text{ for all } [\gamma'] \in U_{[\gamma]}.
\]
The proofs that $p:U_[\gamma] \to U$ is a homeomorphism for all $U \in \mathcal U$ and $[\gamma] \in \tilde X$ and that $\tilde X$ is simply connected can be found in \cite{hatcher2002topology}.

An elementary example is the torus:
\begin{example}
    For $X = \mathbb T^2$ being the torus, the universal covering $p: \mathbb R^2 \to \mathbb T^2$ and given by $p(t,s) = e^{2\pi i t, 2\pi i s }$.
    Under the identification $\pi_1(\mathbb T^2) \simeq \mathbb N^2$, which acts on $\mathbb R^2$ by translations, we have $\mathbb T^2 \simeq \mathbb R^2 / \mathbb N^2$.
\end{example}

Having now constructed the universal cover, we now move on to discussing deck transformations.
\begin{definition}
    A deck transformation of a covering space $p: \tilde X \to X$ is a homeomorphism $\phi: \tilde X \to \tilde X$ such that $p \circ \phi = p$.
    In other words, the following triangular diagram commutes:
    \[
    \begin{tikzcd}
        \tilde X \arrow[rr, "\phi"] \arrow[rd, "p"'] & & \tilde X \arrow[ld, "p"] \\
        & X &
    \end{tikzcd}
    \]
\end{definition}
For instance, the fundamental group $\pi_1(X, x_0)$ of $X$ acts by deck transformations on the universal cover $\tilde X$.
To each $[\alpha] \in \pi_1(X, x_0)$ we associate the deck transformation $\phi_{[\alpha]}$ defined by
\[
\phi_{[\alpha]}([\eta]) = [\alpha \cdot \eta].
\]
Considering the quotient space $\tilde X / \pi_1(X, x_0)$, we have the following result:
\begin{theorem}
    The quotient space $\tilde X / \pi_1(X, x_0)$ is homeomorphic to $X$ where the homeomorphism is induced by the projection map $p: \tilde X \to X$ as in the following diagram
    \[
    \begin{tikzcd}
        \tilde X \arrow[r, "p"] \arrow[d] & X \\
        \tilde X / \pi_1(X, x_0) \arrow[ur, "\simeq"', dashed]
    \end{tikzcd}
    \]
\end{theorem}


\chapter{Tensor products}
\section{Symmetric tensors}
The proof of the following is in \cref{thm:irreducible_representations_of_sl_2_C}.
\begin{fact}
    The irreducible representations of $\mathfrak{sl}(2, \mathbb C)$ are exactly the symmetric powers of the standard representation $\mathbb C^2$.
\end{fact}
Suppose $V$ is an $n$-dimensional vector space over a field $\mathbb{K}$.
\begin{definition}
    Suppose $V$ is an $n$-dimensional vector space over a field $\mathbb{K}$.
    We have the following equivalent definitions.
    \begin{enumerate}[label = (\roman*)]
        \item (As a quotient of the tensor product) 
        We define the \emph{symmetric tensor product} $\Sym^k(V)$ as the quotient
        \[
        \Sym^k(V) = V^{\otimes k}/I,
        \]
        where $I$ is the subspace spanned by all elements of the form $v_1 \otimes \cdots \otimes v_k - v_{\sigma(1)} \otimes \cdots \otimes v_{\sigma(k)}$ with $ v_1, \cdots, v_k \in V $ and $\sigma \in S_k$ being a permutation.
        We denote with $v_1 \cdots v_k$ the image of $v_1 \otimes \cdots \otimes v_k$ in $\Sym^k(V)$ through the canonical projection $\pi: V^k \to \Sym^k(V)$.
        \item (By universal property)
        We define the \emph{symmetric tensor product} $\Sym^k(V)$ as the vector space along with a symmetric multilinear map $\pi: V^k \to \Sym^k(V)$ such that for any symmetric multilinear map $f: V^k \to W$ there exists a unique linear map $\tilde f: \Sym^k(V) \to W$ such that the following diagram commutes:
        \[
        \begin{tikzcd}
            V^k \arrow[r, "f"] \arrow[d, "\pi"] & W \\
            \Sym^k(V) \arrow[ur, "\tilde f"']
        \end{tikzcd}
        \]
        \item (As a subspace of the tensor product)
        We define the \emph{symmetric tensor product} $\Sym^k(V)$ as the invariant subspace of $V^{\otimes k}$ under the action of the symmetric group $S_k$.
        \item (As a space of polynomials)
        We define the \emph{symmetric tensor product} $\Sym^k(V)$ as the space of homogeneous polynomials of degree $k$ on $V$.
    \end{enumerate}
\end{definition}
\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item Looking at the first definition, we see that the symmetric tensor product has a basis given by
        \[
        \left\{ e_{i_1} \cdots e_{i_k} \mid 1 \leq i_1 \leq \cdots \leq i_k \leq n \right\},
        \]
        with $e_1, \cdots, e_n$ being a basis of $V$.
        \item Assuming that we define $\Sym^k(V)$ as the quotient of a tensor product, the universal mapping is simply the quotient map $V \to \Sym^k(V) = V^{\otimes k}/I$.
        \item We could have also defined the symmetric tensor product with a universal property of having a symmetric multilinear map $\pi: V^{\otimes k} \to \Sym^k(V)$ such that for any symmetric map $f: V^{\otimes k} \to W$ there exists a unique linear map $\tilde f: \Sym^k(V) \to W$ such that the following diagram commutes:
        \[
        \begin{tikzcd}
            V^{\otimes k} \arrow[r, "f"] \arrow[d, "\pi"] & W \\
            \Sym^k(V) \arrow[ur, "\tilde f"']
        \end{tikzcd}
        \]
        \item The isomorphism between the quotient space and the space of polynomials is given by the map that sends $e_{i_1} \cdots e_{i_k}$ to $x_{i_1} \cdots x_{i_k}$.
    \end{enumerate}
    
    Recall that a representation $\rho: \mathfrak g \to \mathfrak{gl}(V)$ gives rise to a representation $\rho^{\otimes k}: \mathfrak g \to \mathfrak{gl}(V^{\otimes k})$, defined by
    \[
    \rho^{\otimes k}(X)(v_1 \otimes \cdots \otimes v_k) = \rho(X)v_1 \otimes v_2 \otimes \cdots \otimes v_k + \cdots + v_1 \otimes v_2 \otimes \cdots \otimes \rho(X)v_k.
    \]
    The analogue is true for the symmetric tensor product, where we have a representation $\Sym^k(\rho): \mathfrak g \to \mathfrak{gl}(\Sym^k(V))$ defined by
    \[
    \Sym^k(\rho)(X)(v_1 \cdots v_k) = \left(\rho(X)v_1\right) v_2 \cdots v_k + \cdots + v_1 v_2 \cdots \left(\rho(X)v_k\right).
    \]
\end{remark}

\begin{example}
    Consider the standard representation $\rho: \mathfrak{sl}(2, \mathbb C) \to \mathfrak{gl}(\mathbb C^2)$.
    Then $\Sym^2(C^2) \simeq \mathbb C x^2 \mathbb C xy + \mathbb C y^2 $ and if we denote with $x,y$ the standard basis of $\mathbb C^2$, we have $\Sym^2(\rho)(H)(x y) = (\rho(H)x)y + x (\rho(H)y) = 0$.
    Proceeding similarly, we obtain that $\Sym^2(\rho)(H) = \diag(2, 0, -2)$ in the basis $x^2, xy, y^2$. 
\end{example}

\begin{lemma}
    Suppose $V$ admits a weight decomposition $V = \oplus V_\alpha$.
    Then tensor, symmetric and exterior powers of $V$ admit weight decompositions as well.
    For $V^{\otimes 2}$, the weights are the sums of two weights of $V$ (counting $\alpha, \beta$ and $\beta, \alpha$ as different), for $\Sym^2(V)$ the weights are the pairwise sums of weights of $V$ (meaning that we count the weights of $\alpha, \beta$ and $\beta, \alpha$ as the same ), for $\bigwedge^2 V$ the weights are the sums of distinct weights of $V$ and the non-simple weights of $V$ multiplied by $2$.
    The weight decompositions are given by:
    \[
    V^{\otimes 2} = \bigoplus_{\gamma}
    \bigoplus_{\substack{\alpha, \beta:\\ \alpha+\beta = \gamma}} V_\alpha \otimes V_\beta, \quad 
    \Sym^2(V) = \bigoplus_{\gamma} \bigoplus_{\substack{\{\alpha, \beta\}:\\ \alpha+\beta = \gamma}} V_\alpha \cdot V_\beta, \quad
    \bigwedge ^2 V = \bigoplus_{\gamma} \bigoplus_{\substack{\{\alpha, \beta\}:\\ \alpha+\beta = \gamma}} V_\alpha \wedge V_\beta,
    \]
    with the convention that the symmetric and the wedge products on the right hand sides are interior and that certain wedge products $V_\alpha \wedge V_\alpha$ can be zero, and that the last sum is over unordered pairs (meaning that $\{\alpha, \alpha\}$ is an admissible pair).
\end{lemma}
\begin{proof}
    It is clear that $V_\alpha \otimes V_\beta \subseteq V^{\otimes 2}_{\alpha + \beta}$.
    Moreover, one can see that $V^{\otimes 2}$ and $\bigwedge^2 V$ decompose as above by explicitly constructing bases.
    It suffices to show that 
    \[
        V^{\otimes 2}_\gamma =
        \bigoplus_{\substack{\alpha, \beta:\\ \alpha+\beta = \gamma}} V_\alpha \otimes V_\beta, \quad 
        \bigwedge ^2 V_\gamma = \bigoplus_{\substack{\{\alpha, \beta\}:\\ \alpha+\beta = \gamma}} V_\alpha \wedge V_\beta.
    \]
    More generally, consider a decomposition $V = \oplus_\alpha V_{\alpha'}$ such that $V_\alpha \subseteq V_\alpha$.
    Then for $v \sum_{\gamma} \in V_\alpha$, we can find for each $\gamma_0$ some $H \in \mathfrak h$ such that $\langle H, \gamma_0 \rangle \neq \langle H, \alpha \rangle$.
    Then $\sum_\gamma \langle H, \alpha \rangle v_\gamma = \langle H, \alpha \rangle v = H v = H \sum_\gamma v_\gamma = \sum_\gamma \langle H, \gamma \rangle v_\gamma $.
    In particular, $v_\gamma'=0$ and thus $v_\alpha \in V_\alpha'$.
\end{proof}
\chapter{Exercises}
\begin{exercise}
    Let $G$ be a simply connected, abelian Lie group.
    Then the exponential map is an isomorphism.
\end{exercise}
\begin{proof}[Solution]
    The exponential map is a homomorphism, since $G$ is abelian.
    Moreover, it is a local diffeomorphism, so it is a covering map by \cref{prop:covering_map_criterion}.
    Since it is a covering map and $G$ is simply connected, it is a homeomorphism.
\end{proof}

\section{Kirillov's book}
In this section, we have solutions for certain exercises of \cite{kirillov2008introduction}.

\begin{exercise}[Exercise 2.3]
    Let $f:G_1 \to G_2$ be a Lie group morphism such that $f_*: \mathfrak g_1 \to \mathfrak g_2$ is an isomorphism.
    Then $f$ is a covering map and $\ker f$ is a discrete central group.
\end{exercise}
\begin{proof}[Solution]
    See \cref{prop:covering_map_criterion}.
\end{proof}
\begin{exercise}[Exercise 2.7]
Define a bilinear form on $\mathfrak{su}(2)$ by $(a, b) = \tr(ab^*)/2$. 
Show that this form is symmetric, positive deﬁnite, and invariant under the adjoint action of
$\SU(2)$.
\end{exercise}
\begin{proof}[Solution]
    It is symmetric since in $\su(2)$ we have $b^* = -b$, so $(a,b) = \tr(ab^*)/2 = -\tr(ab)/2 = 
    -\tr(ba)/2 = (b,a)$.
    It is positive definite since $(a,a) = \frac{1}{2} \sum_{i,j} |a_{ij}|^2$.
    It is also invariant under the adjoint action of $\SU(2)$ since the trace is invariant under conjugation, and the adjoint representation is given by conjugation.
\end{proof}

\begin{exercise}[Exercise 2.8]
    Consider the usual basis for $\mathfrak{su}(2)$:
    \[
    i \sigma_1 = \begin{pmatrix} 0 & i \\ i & 0 \end{pmatrix}, \quad 
    i\sigma_2 = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}, \quad 
    i \sigma_3 = \begin{pmatrix} i & 0 \\ 0 & -i \end{pmatrix}.
    \]
    Show that the adjoint representation
    \begin{align*}
        \Ad: \SU(2) &\to \GL(3, \mathbb R)\\
        g &\mapsto \text{ matrix of } \Ad_g \text{ in the basis } i\sigma_1, i\sigma_2, i\sigma_3
    \end{align*}
    gives a morphism of Lie groups $\SU(2) \to \SO(3)$.
\end{exercise}
\begin{proof}[Solution]
    Note that the above exercise gave us an isometry between $\mathfrak{su}(2)$ and $\mathbb R^3$, where the first is equipped with the inner product $(a,b) = \tr(ab^*)/2$ and the second with the standard euclidean inner product.
    Denoting with $[\Ad_g]$ the matrix of $\Ad_g$ in the basis $i \sigma_1, i \sigma_2, i \sigma_3$, the above isometry tells us $\Ad_g$ preserves the given inner product if and only if the matrix $[\Ad_g]$ preserves the standard euclidean inner product, i.e.\ $[Ad_g] \in \SO(3, \mathbb R)$.
    Indeed, denoting the isometry with $\Phi: \SU(2) \to \mathbb R^2$, it gives us an isomorphism 
    
    \begin{align*}
        \GL(\Phi): \GL(\SU(2)) &\overset{\simeq}{\to} \GL(3, \mathbb R)\\
        T &\mapsto \Phi T \Phi^{-1}
    \end{align*}
    and $T \in \U(2)$ if and only if $\GL(\Phi) \in \mathrm{O}(3)$.
    But we saw in the above exercise that the inner product is preserved, so the adjoint representation lies in $\mathrm{O}(3)$.

    To show that it lies actually in $\SO(3)$, note that the determinant of every matrix in $\mathrm{O}(3)$ is either $1$ or $-1$.
    However, $\SU(2)$ is connected, so the same is true for $\phi(\SU(2))$.
    Since it is connected and contains $I_3$, so the determinant of the adjoint representation must be $1$.
\end{proof}

\begin{exercise}[Exercise 2.9]
    Let $\phi: \SU(2) \to \SO(3, \mathbb R)$ be the morphism of Lie groups defined in the previous exercise.
    Compute explicitly the map map of the tangent spaces $\phi_*: \mathfrak{su}(2) \to \mathfrak{so}(3, \mathbb R)$ and show that it is an isomorphism.
    Deduce from this that $\ker \phi$ is a discrete normal subgroup in $\SU(2)$ and that $\Im \phi$ is an open subgroup in $\SO(3, \mathbb R)$.
\end{exercise}
\begin{proof}[Solution]
    Let $\Phi: \SU(2) \to \mathbb R^3$ be the isometry that maps $i\sigma_1, i \sigma_2, i\sigma_3$ to the standard basis of $\mathbb R^3$ and the induced isomohpsism $\GL(\Phi): \GL(\SU(2)) \to \GL(3, \mathbb R), \GL(\Phi)(T) = \Phi T \Phi^{-1}$.
    Then $Phi$ is given as the composition:
    \[\begin{array}{ccccc}
        \SU(2) &\to \quad \GL(\SU(2)) &\overset{\GL(\Phi)}{\to} &\GL(3, \mathbb R) \\
        g &\mapsto \quad \Ad_g &\mapsto &\Phi \Ad_g \Phi^{-1}
    \end{array}\]

    Diferentiating the above, we see that $\phi_* = \mathfrak{gl}(\Phi) \circ \ad$ which is simply the matrix of $\ad$ in the base $i\sigma_1, i\sigma_2, i\sigma_3$.
    Hence we calculate
    \[
    ad_{i\sigma_1} = - 2J_x, \quad
    ad_{i\sigma_2} = - 2J_y, \quad
    ad_{i\sigma_3} = - 2J_z.
    \]
    and thus the matrix of $\phi_* = -2 \diag(1,1,1)$ which is clearly an isomorphism.
    Hence $\phi$ is a covering map and its kernel is a discrete central normal subgroup of $\SU(2)$, and its image is an open subgroup of $\SO(3, \mathbb R)$.
\end{proof}

\begin{exercise}
    Prove that the map $\phi$ used in the two previous exercises establishes an isomorphism $\SU(2)/\mathbb Z^2 \to \SO(3, \mathbb R)$, and thus, since $\SU(2)\simeq \mathbb S^3$, we have $\mathbb{RP}^3 \simeq \SO(3, \mathbb R)$.
\end{exercise}
\begin{exercise}[Exercise 3.12]
        Let $x, y \in \mathfrak g$.
        Then $e^x e^y = e^y e^x$ if and only if $[x,y]=0$.
        In that case $e^x e^y = e^y e^x = e^{x+y}$.    
\end{exercise}
\begin{proof}[Solution]
    The if direction follows from the definition of the commutator via Taylor series.
    For the other direction we follow the method suggested in Exercise 3.11 of \cite{kirillov2008introduction}:
    \[
    \Ad(e^x)y = e^{\ad x} y = \sum_{n\geq 0} \frac{(\ad x)^n}{n!} y = y
    \]
    But $C_g e^z = e^{\Ad_g z}$, so the above implies that $e^x e^y e^{-x} = C_{e^x}e^y = e^{\Ad(e^x)y} = e^y$.
\end{proof}

\begin{exercise}[Exercise 3.15]
    Let $G$ be a connected simply-connected complex Lie group with Lie algebra $\mathfrak g$.
    Then every real form $\mathfrak k$ of $\mathfrak g$ is realised as the Lie algebra of a real form $K$ of $G$.
\end{exercise}
\begin{proof}[Solution]
    Considering $\mathfrak g = \mathfrak k \oplus i \mathfrak k$, we define the real Lie algebra automorphism
    \[
    \theta: \mathfrak g \to \mathfrak g, \quad x + iy \mapsto x - iy.
    \]
    Since $G$ is connected and simply connected, the second fundamental theorem of Lie groups tells us that $\theta$ can be lifted to a Lie group automorphism $\Theta: G \to G$.
    Then $K = G^\Theta$ is a real form of $G$ with Lie algebra $\ker \theta = \mathfrak k$.
\end{proof}

\begin{exercise}[Exercise 5.1]
    Let $\mathfrak g $ be a Lie algebra
    \begin{enumerate}[label = (\roman*)]
        \item If $0 \to V_1 \to W \to V_2 \to 0$ is a short exact sequence of representations of $\mathfrak g$.
        Then
        \[
        B_W = B_{V_1} + B_{V_2}
        \]
        where $B$ is the form defined by $B_W(x,y) = \tr\left(\rho_W(x)\rho_W(y)\right)$ for $x,y \in \mathfrak g$.
        \item If $I$ is an ideal of $\mathfrak g$, then the restriction of the Killing form to $I$ is the Killing form of $I$.
    \end{enumerate} 
\end{exercise}
\begin{proof}[Solution]
    \begin{enumerate}[label = (\roman*)]
        \item 
        Recall that $0 \to V_1 \to W \to V_2 \to 0$ being a short exact sequence of representations the following diagram commutes for all $x \in \mathfrak g$:
    \[
    \begin{tikzcd}
        0 \arrow[r] & V_1 \arrow[r, "\alpha"] \arrow[d, "\rho_{V_1}(x)"] & W \arrow[r, "\beta"] \arrow[d, "\rho_W(x)"] & V_2 \arrow[r] \arrow[d, "\rho_{V_2}(x)"] & 0\\
        0 \arrow[r] & V_1 \arrow[r, "\alpha"] & W \arrow[r, "\beta"]  & V_2 \arrow[r] & 0\\
    \end{tikzcd}
    \]
    where both rows are short exact.
    
    In particular, using exactness, we see that $\alpha(V_1)$ is a subrepresentation of $W$, so we obtain a quotient representation $W/\alpha(V_1)$.
    Moreover, $\beta$ passes to the quotient $W/\alpha(V_1)$, giving us a representation isomorphism, shown in the following diagram:
    \[
    \begin{tikzcd}
        W/\alpha(V_1) \arrow[r, "\beta"] \arrow[d, "\rho_{W/V_1}(x)"] & V_2 \arrow[d, "\rho_{V_2}(x)"]\\
        W/\alpha(V_1) \arrow[r, "\beta'"] & V_2
    \end{tikzcd}
    \]
    
    Let $v_1, \cdots, v_k$ be a basis for $V_1$.
    Then by exactness, $\alpha(v_1), \cdots, \alpha(v_k)$ is a basis of $\alpha(V_1)$, which we extend to a basis of $W = \spa{\alpha(v_1), \cdots, \alpha(v_k), w_1, \cdots, w_l}$.
    With respect to this basis, the matrix of some $\rho_W(x) \in \mathfrak{gl}(W)$ is given by
    \[
    \rho_W(x) = 
    \begin{pmatrix}
        \rho_{V_1}(x) & *\\
        0 & \rho_{W/V_1}(x)
    \end{pmatrix},
    \]
    where $\rho_{V_1}(x) \in \mathbb C^{k \times k}$ is the matrix of $\rho_{V_1}(x)$ in the basis $v_1, \cdots, v_k$ and $\rho_{W/V_1}(x) \in \mathbb C^{l \times l}$ is the matrix of $\rho_{W/V_1}(x)$ in the basis $\beta(w_1), \cdots, \beta(w_l)$ of $V_2$.
    Thus
    \[
    \rho_W(x)\rho = 
    \begin{pmatrix}
        \rho_{V_1}(x) \rho_{V_1}(y) & *\\
        0 & \rho_{W/V_1}(x)\rho_{W/V_1}(y)
    \end{pmatrix},
    \]
    so the $B_W(x, y) = \tr(\rho_W(x)\rho_W(y)) = \tr(\rho_{V_1}(x)\rho_{V_1}(y)) + \tr(\rho_{W/V_1}(x)\rho_{W/V_1}(y)) = B_{V_1}(x,y) + B_{V_2}(x,y)$.
    \item 
    Let $W$ be a subspace of $\mathfrak g$ that is complementary to $I$, i.e.\ $\mathfrak g = I \oplus W$.
    Then for any $x,y \in I$, in any basis obtained by adjoining a basis of $I$ with a basis of $W$, the matrix of $\ad_x \ad_y$ will be given by
    \[
    \begin{pmatrix}
        {\ad_x}_{|I} {\ad_y}_{|I} & *\\
        0 & 0
    \end{pmatrix},
    \]
    since $\ad_x(I), \ad_y(I) \subseteq I$.
    Thus $B_V(x,y) = \tr({\ad_{\rho(x)}}_{|I} {\ad_{\rho(y)}}_{|I})$ which is the Killing form of $I$.
    \end{enumerate}
    
\end{proof}

\begin{exercise}[Exercise 5.5]
    The only real Lie algebra that admits a positive definite Killing form is the trivial $\mathfrak g = 0$.
\end{exercise}
\begin{proof}[Solution]
    Let $\mathfrak g$ be a real Lie algebra admitting a positive definite Killing form $K$.
    Since $K$ is invariant under algebra automorphisms, we have that $Ad(G) \subseteq \SO(\mathfrak g)$, thus $\ad(\mathfrak g) \subseteq \so(\mathfrak g)$.
    Thus, for $x \in \mathfrak g$, we have $(\ad_x)^t = -\ad_x$, so $0 \leq K(x,x) = \tr(\ad_x \ad_x) = - \tr(\ad_x (\ad_x)^t) \leq 0 $.
    Thus $K(x,x) = 0 $ and definiteness implies that $x = 0$.
\end{proof}

\begin{exercise}[Exercise 6.2]\label{ex:semisimple_elements}
    Recall that an operator $A:V \to V$ is semisimple if every $A$-invariant subspace has a complementary $A$-invariant subspace, while an element $x \in \mathfrak g$ is semisimple if the operator $\ad_x$ is semisimple.
    Show that in the case of $\mathfrak g = V = \gl(n, \mathbb C)$, the two definitions agree.
\end{exercise}
\begin{proof}[Solution]
    Recall that an operator $A:V \to V$ is semisimple if and only if it is diagonalisable, so it suffices to show that $x \in \gl(n, \mathbb C)$ is diagonalisable if and only if $\ad_x$ is.
    
    We begin by showing that if $x$ is diagonalisable, then $\ad_x$ is.
    Let $x = \diag(\lambda_1, \cdots, \lambda_n)$.
    Then the $(i,j)$-th entry of $\ad_x y$ is given by $(\lambda_i - \lambda_j)y_{ij}$, so the basis $E_{ij}$ of $\gl(n, \mathbb C)$ is a basis of eigenvectors of $\ad_x$ corresponding to the eigenvalues $\lambda_i - \lambda_j$.
    Similarly, if $x = P D P^{-1}$, then $\ad_x (P E_{ij}P^{-1}) = (d_i - d_j) E_{ij}$, so the basis $P E_{ij}P^{-1}$ is a basis of eigenvectors of $\ad_x$ corresponding to the eigenvalues $d_i - d_j$, and thus $ad_x$ is also diagonalisable.

    Similarly, one can show that if $x$ is nilpotent (i.e. $x^n = 0$ for some $n$), then $\ad_x$ is nilpotent.
    So if $ad_x$ is semisimple, considering the Jordan decomposition $x = x_{ss} + x_n$, we see that $\ad_x - \ad_{x_{ss}} = \ad_{x_n}$ is semisimple and nilpotent, so $x_n = 0$ and $x = x_{ss}$ is semisimple.
\end{proof}

\begin{exercise}[Exercise 6.7]
    Let $\mathfrak g$ be a complex semisimple Lie algebra and $(\cdot, \cdot)$ be a non-degenerate invariant symmetric bilinear form on $\mathfrak g$.
    For $\alpha \in R$ we define
    \[
    h_\alpha = \frac{2H_\alpha}{(\alpha, \alpha)}.
    \]
    Then the definition of $h_\alpha$ is independent of the choice of the non-degenerate invariant symmetric bilinear form.
\end{exercise}
\begin{proof}[Solution]
    We begin by considering the case of $\mathfrak g$ being simple.
    Then, for any other form $(\cdot, \cdot)'$ we have $(\cdot, \cdot)' = c (\cdot, \cdot)$ for some $c \in \mathbb C^\times$.
    Letting $H_\alpha'$ be the element in $\mathfrak h$ satisfying $(H_\alpha', \cdot )' = \alpha$, we see that $H_\alpha' = c H_\alpha$, hence $h_\alpha' = \frac{2H_\alpha'}{(\alpha, \alpha)'} = \frac{2c H_\alpha}{c (\alpha, \alpha)} = h_\alpha$.

    For the case of $\mathfrak g$ being semisimple, we use \cref{thm:cartan_subalgebras_of_semisimple_algebras}, to see that there exist some $i$ such that $\alpha \in R_i \subseteq \mathfrak h_i^*$.
    A priori, we only know that $H_\alpha \in \mathfrak h$, but in fact it lies in $\mathfrak h_i$.
    To see this, it suffices to show that the isomorphism $\mathfrak h \to \mathfrak h^*$ sends $\mathfrak h_i$ to $\mathfrak h_i^*$.
    Indeed, let $e \in \left(\mathfrak h_i\right)_\alpha, f \in \left( \mathfrak h_i \right)_{-\alpha}$ such that $(e,f) = 1$.
    Then we know that $[e,f] = H_\alpha$, so for any $h_j \in \mathfrak h_j$ we have $(H_\alpha, h_j) = ([e,f], h_j) = (e, [f,h_j]) = 0$, which implies that $H_\alpha$
    Let $x_i \in \mathfrak h_i, x_j \in \mathfrak h_j$.
    Then 
\end{proof}

\begin{exercise}[Exercise 7.2]
    Let $R \subseteq E$ be a root system and $\Pi = \{\alpha_1, \cdots, \alpha_r\} \subseteq R_+$ be a set of simple roots, determined by the element $t \in E$.
    \begin{enumerate}[label = (\roman*)]
        \item The set
        \[
        R^\vee = \left\{ \alpha^\vee : \alpha \in R \right\}
        \]
        is a root system in $E^*$.
        \item The set 
        \[
        \Pi^\vee = \left\{ \alpha_1^\vee, \cdots, \alpha_r^\vee \right\}
        \]
        is a set of simple roots of $R^\vee$.
    \end{enumerate}
\end{exercise}
\begin{proof}[Solution]
    \begin{enumerate}[label = (\roman*)]
        \item Noting that $E \mapsto E^*, e \mapsto (e, \cdot)$ is an isomorphism, and that $e^\vee$ is a multiple of $(e, \cdot)$, it is clear that $\Pi^\vee$ is a basis for $E^*$.
        Using this isomorphism, we define an inner product on $E^*$ by $\left((e_1, \cdot), (e_2, \cdot)\right) = (e_1, e_2)$.
        Then
        \[
        \frac{2(\alpha^\vee, \beta^\vee)}{(\beta^\vee, \beta^\vee)} = n_{\beta \alpha} \in \mathbb Z
        \]  
        Finally, we compute
        \begin{align*}
            s_{\alpha^\vee}(\beta^\vee) &= \beta^\vee - \frac{2(\alpha^\vee, \beta^\vee)}{(\beta^\vee, \beta^\vee)} \alpha^\vee =
            \frac{2\beta^*}{|\beta|^2} - \frac{2(\alpha, \beta)}{|\alpha|^2} \frac{2\alpha^*}{|\alpha|^2}\\
            &= \frac{2}{|\beta|^2} \left(s_\alpha(\beta)\right)^* = \left(s_\alpha(\beta)\right)^\vee \in R^\vee
        \end{align*}
        \item We use the element $t^*$ to see that $(R_+)^\vee = (R^\vee)_+$.
        On the other hand, we know that $\Pi^\vee$ a basis for $E^*$.
        In particular, every root in $\Pi^\vee$ can't be written as a sum of positive roots, so $\Pi^\vee$ consists of simple roots.
        On the other hand, the simple roots of $R_+$ are a basis of $E^*$, so they are at most $r$, hence $\Pi^\vee$ contains all simple roots of $R^\vee$ as well.
    \end{enumerate}
\end{proof}

\begin{exercise}[Exercise 7.3]
    Let $v_1, \cdots, v_k$ be a collection of non-zero vectors in a Euclidean space $E$ such that for $i \neq j$, $(v_i, v_j) \leq 0$.
    Then $\{v_1, \cdots, v_k\}$ are linearly independent.
\end{exercise}
\begin{proof}[Solution]
    Let $u = \sum_{i=1}^k a_i v_i = 0$.
    Writing $I = \left\{ i: a_i > 0 \right\}, J = \left\{ j: a_j < 0 \right\}$.
    Then $I \cap J = \emptyset$ and
    \[
    v \equaldef \sum_{i \in I} c_i v_i = \sum_{j \in J} c_j v_j
    \]
    where $c_i = a_i, c_j = -a_j$ for $i \in I, j \in J$.
    Then
    \[
    (v,v) = \sum_{i \in I, j \in J} c_i c_j (v_i, v_j) \leq 0
    \]
    since $c_i, c_j \geq 0, (v_i, v_j) \leq 0$, so $v = 0$ and thus $u = 0$.
\end{proof}

\section{Knapp's book}
\begin{exercise}[Exercise I.1]
    \begin{enumerate}[label = (\roman*)]
        \item The Heisenberg algebra
        \[
        \mathfrak g = \left\{ \begin{pmatrix}
            0 & x & z\\
            0 & 0 & y\\
            0 & 0 & 0
        \end{pmatrix}: x,y,z \in \mathbb K \right\}
        \]
        is nilpotent.
        \item Consider the group of translations and dialations of the plane:
        \[
        G = \mathbb K^2 \rtimes \mathbb K^\times \simeq \left\{ \begin{pmatrix}
            t & 0 & x\\
            0 & t & y\\
            0 & 0 & 1
        \end{pmatrix} : t, x, y, \in \mathbb K \right\}.
        \]
        Show that its Lie algebra
        \[
        \mathfrak g = \left\{ \begin{pmatrix}
            t & 0 & x\\
            0 & t & y\\
            0 & 0 & 0
        \end{pmatrix}: t,x,y \in \mathbb K \right\}
        \]
        is split-solvable, i.e.\ it admits a full-flag of ideals (in $\mathfrak g$).
    \end{enumerate}
\end{exercise}
\begin{proof}[Solution]
    \begin{enumerate}[label = (\roman*)]
        \item We define as a $k$-th upper triangular matrix a matrix for which non-zero elements are on the $r$-th diagonal, i.e. $a_{ij} \neq 0$ implies $i-j \leq k$.
        Then it is easy to see that the product of a $k$-th and an $l$-th upper triangular matrix is a $k+l$-upper triangular matrix.
        \item The ideals are:
        \begin{align*}
            \mathfrak a_0 = 0 \subseteq
            \mathfrak a_1 = \left\{
            \begin{pmatrix}
                0 & 0 & x\\
                0 & 0 & 0\\
                0 & 0 & 0
            \end{pmatrix} : x \in \mathbb K
            \right\} \subseteq
            \mathfrak a_2 = \left\{
            \begin{pmatrix}
                0 & 0 & x\\
                0 & 0 & y\\
                0 & 0 & 0
            \end{pmatrix} : x, y \in \mathbb K
            \right\} \subseteq \mathfrak a_3 = \mathfrak g
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{exercise}[Exercise I.2]\label{ex:solvable_not_nilpotent}
    For 
\[
A = \begin{pmatrix}
\alpha & \beta \\
\gamma & \delta
\end{pmatrix}
\]
any nonsingular matrix over $k$, let $\mathfrak{g}_A$ be the 3-dimensional algebra over $k$ with basis $X, Y, Z$ satisfying
\begin{align*}
    [X, Y] &= 0,\\
    [X, Z] &= \alpha X + \beta Y,\\
    [Y, Z] &= \gamma X + \delta Y.    
\end{align*}


\begin{itemize}
    \item[(a)] Show that $\mathfrak{g}_A$ is a Lie algebra by showing that
    \[
    X \leftrightarrow \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}, \quad Y \leftrightarrow \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}, \quad Z \leftrightarrow \begin{pmatrix} \alpha & \beta & 0 \\ \gamma & \delta & 0 \\ 0 & 0 & 0 \end{pmatrix}
    \]
    gives an isomorphism with a Lie algebra of matrices.
    
    \item[(b)] Show that $\mathfrak{g}_A$ is solvable but not nilpotent.
    
    \item[(c)] Let $k = \mathbb{R}$. Take $\delta = 1$ and $\beta = \gamma = 0$. Show that the various Lie algebras $\mathfrak{g}_A$ for $\alpha > 0$ are mutually nonisomorphic. (Therefore for $k = \mathbb{R}$ that there are uncountably many nonisomorphic solvable real Lie algebras of dimension 3.)
\end{itemize}
\end{exercise}
\begin{proof}[Solution]
    \begin{enumerate}
        \item[(b)] Note that $[\mathfrak g_A, \mathfrak g_A] = \spa{X,Y}$ is an abelian ideal of $\mathfrak{g}_A$.
        Hence, the lower central series is $\mathfrak g_A \supseteq \spa{X,Y} \supseteq 0$, so $\mathfrak g_A$ is solvable.
        On the other hand, the lower central series does not terminate: $D^i\mathfrak g_A = D^1 \mathfrak g_A = \spa\{X, Y\}$, so it is not nilpotent.
        \item[(a)] Note that $\ad Z$ is diagonalisable with eigenvalues $-\alpha, -1, 0$, coming from $X, Y, Z$ respectively.
        In fact, the eigenvalues remain the same for $\ad(Z + sX + tY)$, and hence the ratio of the largest eigenvalue to the second largest is $\alpha$ for any constant multiple of this transformation.
        Consider $\alpha, \alpha' > 1$ and the corresponding algebras $g_\alpha, g_{\alpha'}$ generated by $X,Y,Z$ and $X',Y',Z'$ respectively, that satisfy the relations above.
        If $T: \mathfrak g_\alpha \to \mathfrak g_{\alpha'}$ was a Lie algebra isomorphism, then it would satisfy $T([\mathfrak g_\alpha, \mathfrak g_\alpha]^c) = [\mathfrak g_{\alpha'}, \mathfrak g_{\alpha'}]^c$, so it would send $Z$ to some multiple of $Z' + sX' + tY'$ for some $s,t \in \mathbb K$.
        Thus $ T(\ad Z)T^{-1} = \ad(Z + sX + tY)$, up to a constant multiple, and hence the ratio of the smallest to the second smallest eigenvalue would be $\alpha$. 
    \end{enumerate}
\end{proof}

\begin{exercise}[Exercise I.4]
    \begin{enumerate}
        \item Let $\mathfrak g$ be a real Lie algebra of complex matrices with the property
        \[
        iX \not \in \mathfrak g \text{ for all } X \in \mathfrak g - \{0 \}
        \]
        Then
        \[
        \mathfrak g^\mathbb C \simeq \mathfrak g \oplus i\mathfrak g
        \]
        \item
        \[
        \mathfrak{sl}(2,\mathbb R)^\mathbb C \simeq \mathfrak{su}(2)^\mathbb C
        \]
    \end{enumerate}
\end{exercise}
\begin{proof}[Solution]
    \begin{enumerate}
        \item
        We recall the construction of $\mathfrak g^\mathbb C$: it is the $\mathbb C$-vector space $\mathfrak g \otimes_\mathbb R \mathbb C$ that is spanned by $x \otimes 1_\mathbb C$ and scalar multiplication defined by $z \left(x \otimes 1_\mathbb C\right) = x \otimes z$.
        We consider the map $\mathfrak g \otimes_\mathbb R \mathbb C \to \mathfrak g \oplus i\mathfrak g$ that extends $x \otimes 1_\mathbb z \mapsto (\mathrm{Re}z x, i\mathrm{Im}z x)$.
        It is clearly surjective, and for injectivity we consider $x_1, \cdots, x_n$ be a basis for $\mathfrak g$.
        If the image $z_1 x_1 + \cdots z_n x_n$ of $x_1 \otimes z_1 + \cdots + x_n \otimes z_n$ is zero, then $\mathrm{Re}(z_1)x_1 + \cdots + \mathrm{Re}(z_n)x_n = 0$, so $\mathrm{Re}(z_i) = 0$ for all $i$.
        Similarly we show that $\mathrm{Im}(z_i) = 0$ for all $i$.
        \item Both $\mathfrak{sl}(2, \mathbb R)$ and $\mathfrak{su}(2)$ satisfy the property above, so the result applies.
        Looking at $\mathfrak{sl}(2,\mathbb C)$, every traceless complex matrix can be written as $x + iy$ for some traceless real matrices $x, y$, and every such sum is a tracless complex matrix, so 
        \[
        \mathfrak{sl}(2, \mathbb C) = \mathfrak{sl}(2, \mathbb R) \oplus i\mathfrak{sl}(2, \mathbb R) \simeq \mathfrak{sl}(2,\mathbb R)^\mathbb C
        \]

        On the other hand
        \[
        \mathfrak{su}(2,\mathbb C) = \left\{\begin{pmatrix}
            ia & b\\
            -\overline{b} & -ia
        \end{pmatrix}: a \in \mathbb R, b \in \mathbb C\right\},
        \]
        so any traceless complex matrix uniquely decomposes into a sum $x + iy$ with $x, y \in \mathfrak{su}(2)$.
        Thus
        \[
        \mathfrak{sl}(2,\mathbb C) = \mathfrak{su}(2) \oplus i \mathfrak{su}(2) \simeq \mathfrak{su}(2)^\mathbb C.
        \]
    \end{enumerate}
\end{proof}

\begin{exercise}[Exercise I.5]\label{ex:so_3_killing_form}
    The Killing form of $\mathfrak{so}(3)$ is a constant multiple of the dot product in $\mathbb R^3$.    
\end{exercise}
\begin{proof}[Solution]
    Let
    \[
    i = \begin{pmatrix}
        0 & 0 & 0\\
        0 & 0 & 1\\
        0 & -1 & 0
    \end{pmatrix}, \quad j = \begin{pmatrix}
        0 & 0 & 1\\
        0 & 0 & 0\\
        -1 & 0 & 0
    \end{pmatrix}, \quad k = \begin{pmatrix}
        0 & 1 & 0\\
        -1 & 0 & 0\\
        0 & 0 & 0
    \end{pmatrix}
    \]
    be the basis for $\mathfrak{so}(3)$.
    Then the adjoint representation is given by
    \[
    \ad_i = \begin{pmatrix}
        0 & 0 & 0\\
        0 & 0 & -1\\
        0 & 1 & 0
    \end{pmatrix},
    \ad_j = \begin{pmatrix}
        0 & 0 & 1\\
        0 & 0 & 0\\
        -1 & 0 & 0
    \end{pmatrix},
    \ad_k = \begin{pmatrix}
        0 & -1 & 0\\
        1 & 0 & 0\\
        0 & 0 & 0
    \end{pmatrix}.
    \]
    Hence, the Killing form is given by
    \[
    K = -2I_3.
    \]
\end{proof}

\begin{exercise}[Exercise I.12]
    If a finite-dimensional Lie algebra $\mathfrak g$ is nilpotent, then the Killing form is trivial.
\end{exercise}
\begin{proof}[Solution]
    Since $\mathfrak g$ is nilpotent, we know that $\ad(\mathfrak g)$ is nilpotent as well, in the sense that all operators $\ad_x$ are nilpotent, for $x \in \mathfrak g$.
    By Engel's theorem, we know that there exists a basis of $\mathfrak g$ for which all operators $\ad_x$ are strictly upper triangular, in particular the trace of $\ad_x \ad_y$ is zero for all $x,y\in \mathfrak g$.
\end{proof}
\begin{exercise}[Exercise I.13]\label{ex:simple_killing_form}
    Suppose $\mathfrak g$ is a simple complex Lie subalgebra of complex matrices.
    Then the Killing form is a multiple of the trace form: $B(x,y) = c \tr(xy)$.
\end{exercise}
\begin{proof}[Solution]
    Letting $C(x,y) = tr(xy)$, it suffices to show that for simple Lie algebras, the Killing form is the only invariant bilinear form up to scalar multiplication.

    Note first that since $\mathfrak g$ is simple, the adjoint representation of $\mathfrak g$ is irreducible, because being an invariant subspace under the adjoint representation is equivalent to being an ideal.

    Denote with $b,c: \mathfrak g \to \mathfrak g^*$ the maps $x \mapsto B(x, \cdot), x \mapsto C(x,y)$, where $C(x,y) = \tr(x \cdot)$.
    Then by Cartan's criterion, we know that $b$ is an isomorphism, so we can consider $b^{-1}c: \mathfrak g \to \mathfrak g$.
    Since the adjoint representation is irreducible, by Schur's lemma, it suffices to show that $b^{-1}c$ is an interwining operator, because then $b^{-1}c$ is a multiple of the identity.
    Indeed, 
    \begin{align*}
        B\left(\ad_x b^{-1}c(y), z\right) &= B([x, b^{-1}c(y)], z)\\
        &= -B(b^{-1}c(y), [x,z]) = -C(y, [x,z]) = C([x, y], z) = c(\ad_x y)(z),
    \end{align*}
    hence $b^{-1}c(\ad_x) = \ad_x b^{-1}c(y)$, i.e.\ $b^{-1}c$ is an interwining operator, so it is a multiple of the identity.
\end{proof}

\begin{exercise}[Exercise II.1]
We know from \cref{ex:simple_killing_form} that in each of the following cases, the Killing form is a constant multiple of the trace form.
Finding this constant, show: 
\begin{enumerate}[label = (\roman*)]
    \item On $\mathfrak{sl}(n, \mathbb C), n\geq 1$, we have $B = 2n \tr$.
    \item $\mathfrak{so}(2n+1, \mathbb C), n\geq 2$
    \item $\mathfrak{sp}(n, \mathbb C), n \geq 3$
    \item $\mathfrak{so}(2n, \mathbb C), n \geq 4$
\end{enumerate} 
\end{exercise}
\begin{proof}[Solution]
    We use 
    \begin{lemma}[Corollary 2.24 of \cite{knapp1996lie}]
        Let $\mathfrak g$ be a semisimple Lie algebra with Cartan subalgebra $\mathfrak h$.
        Then on $\mathfrak h \times \mathfrak h$, the Killing form $B$ is given by
        \[
        B(H,H') = \sum_{\alpha \in \Delta} \alpha(H) \alpha(H')
        \]
        where $H, H' \in \mathfrak h$ and $\Delta$ is the set of roots of $\mathfrak g$.
    \end{lemma}
    \begin{enumerate}[label = (\roman*)]
        \item We let $x = E_{11} - E_{22}$, so for $\alpha \in \Delta_+$ being a positive root:
        \[
        \alpha(x) = \begin{cases}
            2 & \alpha = \varepsilon_1 - \varepsilon_2\\
            1 & \alpha = \varepsilon_1 - \varepsilon_j \text{ or } \varepsilon_2 - \varepsilon_j \text{ for } j \geq 3\\
            0 & \text{otherwise}
        \end{cases}.
        \] 
        Thus, we calculate
        \[
        B(x.x) = 2 \sum_{\alpha \in \Delta_+} \alpha(x)^2 = 2(4 + (n-2) + (n-2)) = 4n = 2\tr(x^2)
        \]
    \end{enumerate}
\end{proof}
\begin{remark}
    Ways to show that a representation is irreducible:
    \begin{enumerate}[label = (\roman*)]
        \item If it is a representation of $\ssl(2, \mathbb C)$, and there is a vector with eigenvalue $\dim V - 1$, then the representation is irreducible (see \cref{ex:sl_2_C_irreducible}).
        \item The weight diagram does not contain 
    \end{enumerate}
\end{remark}

\begin{exercise}\label{ex:sl_2_C_irreducible}
[Exercise 1.17 - A quick way to show that a representation of $\mathfrak{sl}(2,\mathbb C)$ is irreducible]
Let $V_n$ be the vector space of homogenous complex polynomials of degree $n$ in two variables.
We define the representation
\begin{align*}
    \Phi: \SL(2,\mathbb C) &\to \GL(V_n)\\
    \Phi(g)p(z_1, z_2) &= p \left( g^{-1}(x,y)^t \right)
\end{align*}    
Denoting with $\phi = \Phi_*: \mathfrak{sl}(2,\mathbb C) \to \gl(V_n)$ the induced representation, show that $\phi$ is irreducible.
\end{exercise}
\begin{proof}[Solution]
    \underline{Idea:} If the there is a vector in a representation of $\SL(2, C)$ with eigenvalue $\dim V - 1$, then the representation is irreducible.

    Since $\phi$ is a representation of $\mathfrak{sl}(2,\mathbb C)$, we know from \cref{thm:irreducible_representations_of_sl_2_C} that it splits into irreducible representations, each of which is the direct sum of eigenvectors of $\phi(h)$, and whose dimension is equal to the highest weight (i.e.\ the highest eigenvalue) of $\phi(h)$.

    Let $p(z_1,z_2) = z_2^n$, and note that $\phi(h)p = n p$.
    Considering the decomposition $\phi = \phi_1 \oplus \cdots \oplus \phi_k$ into irreducible representations and the corresponding decompositions $V_n = V_n^{(1)} \oplus \cdots \oplus V_n^{(k)}$, and $p = p_1 + \cdots + p_k$, we assume without loss of generality that $p_1 \neq 0$.
    Then $p_1$ is an eigenvector of $\phi_1$ with eigenvalue $n$, so the highest weight of $\phi_1$ is of the form $n + 2i$ for some $i \geq 0$.
    Due to dimension restrictions, we have that $i = 0$ and $V_n^{(1)} = V_n$, i.e. $V_n$ is irreducible.
\end{proof}

\section{Fulton, Harris}
\begin{exercise}[Exercise 11.1]
    Find the decomposition into irreducible representations of the representation of $\mathfrak{sl}(2, \mathbb C)$ on $V = \Sym^5(\mathbb C^2)\otimes \Sym^2(\mathbb C^2)$.
\end{exercise}
\begin{proof}[Solution]
    The eigenvalues with multiplicities can be seen in \cref{fig:eigenvalue_diagram}.
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\textwidth]{eigenvalues_of_tensor_product.jpg}
        \caption{Eigenvalue diagram}
        \label{fig:eigenvalue_diagram}
    \end{figure}
    The highest weight vector $x^5 \otimes x^2$ with weight $7$ generates a subrepresentation isomorphic to $\Sym^7(\mathbb C^2)$:
    \[
    \bigoplus_{j = 0}^7 \rho(F)^j \mathbb C (x^5 \otimes x^2)
    \]
    Note that this is not literally the sum of products of weight spaces of $\Sym^5(\mathbb C^2)$ and $\Sym^2(\mathbb C^2)$, since for example
    \[
    \rho(F) (x^5 \otimes x^2) = 5x^4 y \otimes x^2 \oplus x^5 \otimes 2xy.
    \]
    This representation accounts for the eigenvalues $\pm 7, \pm 5, \pm 3, \pm 1$.
    Considering a nonzero vector of weight $5$ in the complement of this subrepresentation, by applying $\rho(F)$ it generates a complementary subrepresentation isomorphic to $\Sym^5(\mathbb C^2)$.
    Similarly we obtain a subrepresentation isomorphic to $\Sym^3(\mathbb C^2)$.
\end{proof}
\begin{exercise}[Exercise 11.3]
    Show that $\bigwedge^2\left(\Sym^2(\mathbb C^2)\right)$ is isomorphic to $\Sym^2(\mathbb C^2)$.
\end{exercise}
\begin{proof}
    The eigenvalues are $0, \pm 2$ with multiplicity $1$.
    This follows from the fact that for $v_\alpha, v_\beta \in \Sym^2(\mathbb C^2)$ of eigenvalues $\alpha, \beta$ respectively, we have that $v_\alpha \wedge v_\beta$ has eigenvalues $\alpha + \beta$, provided that $v_\alpha \wedge v_\beta \neq 0$. 
\end{proof}

\begin{exercise}[Exercise 14.14]\label{ex:irreducible_subrepresentation}
    Let $V$ be a representation of a semisimple Lie algebra $\mathfrak g$.
    We fix a decomposition of the roots $R = R^+ \cup R^-$ and denote with $\Pi$ the set of simple positive roots.
    If $v$ is a highest weight vector, then the subspace $W$ generated under succesive applications of root spaces $\mathfrak g_\beta$ corresponding to primitive negative roots $\beta \in -\Pi$ is an irreducible subrepresentation with highest weight $v$.
\end{exercise}
\begin{proof}
    Let
    \[
    W_n = \spa\{ w_n v : w_n \text{ word of length at most } n \text{ with letters in } \mathfrak g_\beta \text{ with } \beta \in -\Pi \},
    \]
    and denote with $\Pi = \{\alpha_1, \cdots, \alpha_r\}$ the set of simple positive roots.
    Then $W$ is the sum of all $W_n$, so it suffices to show that $X_\beta (Y_{-\alpha_i}) \in W$ for $\beta \in R$ and $Y$ a word with letters in root spaces of primitive negative roots.
    For the case where $\beta > 0$, we have $X_\beta (Y_{-\alpha_i}) = [X_\beta, Y_{-\alpha_i}]v + Y_{-\alpha_i} X_\beta v = 0$.
    Indeed, we have $X v = 0$, because $v$ is a highest weight vector.
    To show $[X,Y]v = 0$, note that  $[X, Y] \in \mathfrak g_{\beta - \alpha_i}$.
    Letting $\beta = n_1 \alpha_1 + \cdots + n_r \alpha_r$ for nonnegative integers $n_i$, we have $\beta - \alpha_i = n_1 \alpha_1 + \cdots + (n_i - 1) \alpha_i + \cdots + n_r \alpha_r$.
    If $n_1 > 0$, then $\beta - \alpha_i \geq 0$, so $[X,Y]v = 0$ or is a multiple of $v$.
    If $n_1 = 0$, then $\beta - \alpha_i$ can't be a positive root (since the coefficient in $\alpha_i$ is negative), and can't be negative (since the coefficients in the other simple roots are nonnegative), so it can't be a root, and $[X,Y] = 0$.

    For the case where $\beta < 0$, recalling that $[\mathfrak g_{\gamma}, \mathfrak g_{\delta} ] = \mathfrak g_{\gamma + \delta}$, we have that $X_\beta$ is a bracket (of brackets) of elements in root space $\mathfrak g_{-\alpha_j}$ corresponding to primitive negative roots, $X Y$ will be in the span of words with letters in root spaces of primitive negative roots.

    For the case of $X_\beta = H \in \mathfrak h$, we proceed inductively and have that $H Y v = [H,Y]v + YHv \in W_{n-1}$ because $H v = \lambda v$ for some $\lambda$, and $[H, Y] $ is a word of length $n-1$.
\end{proof}


\section{Linear algebra exercises}
\begin{exercise}
    Let $A$ be a diagonalizable operator with distinct eigenvalues.
    Then the invariant subspaces of $A$ are exactly the ones that are spanned by some family of its eigenvectors.
\end{exercise}
\begin{proof}[First Solution]
Let $n$ be the ambient dimension and $W$ be an invariant subspace of $A$ and consider some complement $W^{\perp}$ of $W$.
Considering some basis $e_1, \ldots, e_k$ of $W$ and $e_{k+1}, \ldots, e_n$ of $W^{\perp}$, we have that in this basis $A$ is block-diagonal:
\[
A = \begin{pmatrix} A_1 & 0 \\ 0 & A_2 \end{pmatrix}
\]
where $A_1$ is the restriction of $A$ to $W$ and $A_2$ is the restriction to $W^{\perp}$.
In particular, the characteristic polynomial of $A$ is the product of the characteristic polynomials of $A_1$ and $A_2$.
In particular, $A_1$ and $A_2$ will be diagonalisable with distinct eigenvalues (that are also eigenvalues of $A$).
Letting $w_1, \ldots, w_k$ be a basis of eigenvectors of $A_1$, we see that these must be 
eigenvectors of $A$ as well, so $W$ is indeed generated by eigenvectors of $A$.
\end{proof}
\begin{proof}[Second Solution]
    Let $e_1, \cdots, e_d$ be a basis of eigenvectors of $A$.
    If the result were not true, then there would exist an invariant subspace $W$, some element $w = a_1 e_{i_1} + \cdots a_k e_{i_k} \in W$ with $a_1, \cdots a_k \neq 0$ and some $j_0 \in \llbracket 1, k \rrbracket $ such that $e_{i_{j_0}}$ is contained in $W$.
    Without loss of generality assume that $j_0 = k$.
    Then 
    \begin{align*}
        (A = \lambda_1 I)w &= a_2 (\lambda_1 - \lambda_2) e_{i_2} + \cdots + a_k (\lambda_1 - \lambda_k) e_{i_k} = \\
        a_2' e_{i_2} + \cdots + a_{k-1}' e_{i_{k-1}}
    \end{align*}
    which is contained in $W$ and $a_2', \cdots, a_{k-1}' \neq 0$.
    Keeping on like that, we arrive at $e_k \in W$, which is a contradiction.
\end{proof}

\chapter{Geomtric realisation}

\begin{proposition}
    $\SU(2)$ is homeomorphic to $\mathbb S^4$.
    In particular, it is simply connected.
\end{proposition}

\chapter{$SO(d,d)$}
The goal of this chapter is to show that $SO(d,d)$ has 2 connected components.
First we show that a Lie group $G$ has the same number of components as its maximal compact subgroup.
\begin{proposition}
    Let $G$ be a Lie group and $K$ its maximal compact subgroup.
    Then $G$ has the same number of connected components as $K$.
\end{proposition}
\begin{proof}
    The Cartan decomposition of $G$ tells us that 
    \begin{align*}
        K \times \mathfrak p &\overset{\simeq}{\to} G\\
        (k, X) &\mapsto k e^X
    \end{align*}
    is a diffeomorphism.
    Since $K \times \mathfrak p$ is contractible to $K$, they have the same number of components.
\end{proof}
Given this, we just need to calculate a maximal compact subgroup of $SO(d,d)$.
\begin{proposition}
    A maximal compact subgroup of $SO(d,d)$ is $S(O(d) \times O(d))$.
\end{proposition}
\begin{proof}
    Clearly it suffices to show that each $g \in \SO(d,d)$ has the form
    \[
    g = \begin{pmatrix} A & 0 \\ 0 & D \end{pmatrix}
    \]
    for certain square matrices $A, D$, since then they will both necessarily lie in $\mathrm{O}(d)$.
    Since the maximal compact subgroup of an algebraic subgroup $G$ of $\GL(d, \mathbb C)$ is given by the set of fixed points of the involution $A \mapsto (A^*)^{-1}$, we see that its Lie algebra $\mathfrak k$ is the space of antisymmetric matrices that commute with $J = \begin{pmatrix} I & 0 \\ 0 & -I \end{pmatrix}$.
    These are then matrices of the form 
    \[
    x = \begin{pmatrix} A & 0 \\ 0 & D \end{pmatrix}
    \]
    with $A, D$ square and antisymmetric.
    Thus $e^{\mathfrak k}$ has matrices of the desired form and is a generating set for $K$.
\end{proof}

\chapter{Analyzing Lie algebras}
This is taken from \cite[§14]{fulton2013representation}.

{\bf \underline{Step 0}:} \emph{Make sure that the Lie algebra is semisimple.}

{\bf \underline{Step 1}:} \emph{Find a Cartan subalgebra $\mathfrak h \subseteq \mathfrak g$:}
This is a maximal $\mathfrak h \subseteq \mathfrak g$ among subalgebras that are abelian and act diagonally on some faithfull (and hence every) representation of $\mathfrak g$.
Recall that a subalgebra $\mathfrak h$ acting diagonally on some faithfull representation $\rho: \mathfrak g \to \gl(V)$ (i.e.\ $\rho(H)$ is diagonalizable in $V$ for all $H \in \mathfrak h$ ) is going to act diagonally on every representation of $\mathfrak g$ (this follows from the fact that the Jordan decomposition is preserved under representations of semisimple algebras).

{\bf \underline{Step 2}:} \emph{Perform the Cartan decomposition with respect to $\mathfrak h$:}
This is merely the decomposition of $\mathfrak g$ that we obtain by letting $\mathfrak h$ act diagonally by the adjoint representation:
\[
\mathfrak g = \mathfrak h \oplus \bigoplus_{\alpha \in \Delta} \mathfrak g_\alpha
\]
where $\Delta$ is the set of roots of $\mathfrak g$.

One can show that
\begin{enumerate}[label = (\roman*)]
    \item each root space $\mathfrak g_\alpha$ is one-dimensional.
    \item $\Delta$ generates a lattice $\Lambda_\Delta$ in $\mathfrak h^*$ of rank $\dim \mathfrak h$.
    \item $R$ is symmetric about the origin.
\end{enumerate}
Using these, it follows that any two roots are congruent modulo $\Lambda_\Delta$:
\[
\Delta \subseteq \alpha + \Lambda_\Delta \text{ for all } \alpha \in \Delta.
\]

{\bf \underline{Step 3}:} \emph{Find the distinguished subalgebras $\ssl_\alpha$:}
A crucial ingredient is the restriction of the arbitrary representation into representations of subalgebras that are isomorphic to $\ssl(2, \mathbb C)$.
Considering a root $\alpha \in \Delta$, we define the subalgebra
\[
\ssl_\alpha \mathfrak g_\alpha \oplus \mathfrak g_{-\alpha} \oplus [\mathfrak g_\alpha, \mathfrak g_{-\alpha}].
\]
One can show that $[g_\alpha, g_{-\alpha}] \neq 0$ and that $[[g_\alpha, g_{-\alpha}], g_\alpha] \neq 0$.
Then one can pick $X_\alpha \in \mathfrak g_\alpha, X_{-\alpha} \in \mathfrak g_{-\alpha}$ such that $H_\alpha = [X_\alpha, X_{-\alpha}]$ satisfies $\langle H_\alpha, \alpha \rangle = 2$.
Then the subalgebra $\ssl_\alpha$ is generated by $X_\alpha, X_{-\alpha}, H_\alpha$ and is isomorphic to $\ssl(2, \mathbb C)$.

{\bf \underline{Step 4}:} \emph{Use the integrality of eigenvalues of the  $H_\alpha$:}
Note that by the analysis of representations of $\ssl(2, \mathbb C)$, $H_\alpha$ will have integer eigenvalues for every $\alpha \in \Delta$.
Denoting the \emph{weight lattice} by
\[
\Lambda_W = \left\{ \lambda \in \mathfrak h^*: \langle \lambda, H_\alpha \rangle \in \mathbb Z \text{ for all } \alpha \in \Delta \right\},
\]
we have that all weights of all representations of $\mathfrak g$ lie in $\Lambda_W$.
In particular $\Lambda_\Delta \subseteq \Lambda_W$.
Also letting $\alpha \in \Delta$, we can break up the weights of $V$ into congruent classes modulo $\mathbb Z \alpha$.
For every weight $\beta \in \mathfrak h^*$,
\[
V_{[\beta]} = \bigoplus_{n \in \mathbb Z} V_{\beta + n\alpha}
\]
is a subrepresentation of $\ssl_\alpha$ in $V$.

{\bf \underline{Step 5}:} \emph{Use the symmetry of the eigenvalues of $H_\alpha$:}
Using what we know about the representations of $\ssl(2,\mathbb C)$, we can show that for $\beta$ being the first weight of $V$ appearing in its congruency class modulo $\mathbb Z \alpha$, we have 
\[
V_{[\beta]} = V_{\beta} \oplus V_{\beta + \alpha} \oplus \cdots \oplus V_{\beta - \langle H_\alpha, \beta \rangle \alpha}.
\]
Then one can show that the reflection $s_\alpha$ reflects the weight spaces:
\[
s_\alpha (\beta + k \alpha) = \beta + (-\langle H_\alpha, \beta \rangle - k) \alpha.
\]

{\bf \underline{Step 6}:} \emph{Draw a picture.}

{\bf \underline{Step 7}:} \emph{Choose a direction in $\mathfrak h^*$:}
By taking a linear functional $l:\Lambda_\Delta \to \mathbb C$ that is irrational with respect to $\Lambda_\Delta$, which gives us a decomposition of the set of roots, and allows us to find a highest weight vector (i.e.\ a weight vector that is anhiliated by all $\mathfrak g_\alpha$ for $\alpha \in \Delta^+$).

One can show (cf. \cref{ex:irreducible_subrepresentation}) that
\begin{proposition}
    For any semisimple complex Lie algebra
    \begin{enumerate}[label = (\roman*)]
        \item Every finite-dimensional representation $V$ of $\mathfrak g$ has a highest weight vector $v$.
        \item The subspace of $W$ generated in the following two equivalent ways is irreducible subrepresentation of $V$ with highest vector $v$:
        \begin{enumerate}[label = (\alph*)]
            \item By succesive applications of $\mathfrak g_\beta$ with $\beta \in \Delta^-$.
            \item By succesive applications of $\mathfrak g_\beta$ with $\beta$ being primitive negative roots. 
        \end{enumerate}
        \item An irreducible representation posseses a unique highest vector up to scalars.
        \item The set of weights of $V$ will be exactly the weights $\alpha + \Lambda_R \cap \mathrm{Conv}(W \cdot \alpha)$.
    \end{enumerate}
\end{proposition}




{\bf \underline{Step 8}:} \emph{Classify irreducible rpresentations of $\mathfrak g$:}
We define the Weyl chamber as the vectors in the real span of $\Delta$ that form an acute angle with all positive roots.
\[
\mathcal W = \left\{ \lambda \in \mathfrak h^*: \langle \lambda, \gamma \rangle > 0 \text{ for all } \gamma \in \Delta^+ \right\}.
\]
\begin{theorem}
    There exists a bijective correspondence
    \[
    \left\{ \text{irreducible finite dimensional representations of } \mathfrak g \right\} \leftrightarrow \mathcal W \cap \Lambda_W
    \]
    Denoting the representation corresponding to $\alpha \in \mathcal W \cap \Lambda_W$ by $\Gamma_\alpha$, the weights of $\Gamma_\alpha$ will be $$(\alpha + \Lambda_\Delta) \cap \Lambda_W \cap \mathrm{Conv}(W \cdot \alpha).$$
\end{theorem}

\section{Drawing root and weight diagrams}
We take the example of $\mathfrak{sl}(3, \mathbb C)$, and in particular reply to the following question:
\begin{question}
    Why do the weights of the standard representation of $\ssl(n, \mathbb C)$ lie on the vertices of a simplex?
\end{question}

The root diagram will be a drawing of $\mathbb spa_{\mathbb R} \Delta$, with angles and lengths determined by the Killing form.
Recall that the way we define the Killing form on $\mathfrak h^*$ is using the identification $\mathfrak h \simeq \mathfrak h^*, H \mapsto \langle H, \cdot \rangle$.
For each root $\alpha \in \Delta$, we know that we can find elements $E_\alpha, F_\alpha, H_\alpha$ such that they are the basis for a subalgebra $\ssl_\alpha \simeq \ssl(2, \mathbb C)$.
Note that then we have that $\langle H_\alpha, \alpha \rangle = 2$, and using the invariance of the Killing form under the adjoint representation, we have that
\[
T_\alpha = \frac{2}{(H_\alpha, H_\alpha)} H_\alpha.
\]
Hence,
\[
(\alpha, \beta) = \frac{2}{|H_\alpha|^2} \langle H_\alpha, \beta \rangle
\]

Going to the case of $\ssl(3, \mathbb C)$, we have that $(X, Y) = 2 \cdot 3 \tr(XY)$, so $|H_\alpha|^2 = 12$ and $|\alpha|^2 = 1/3$.
We can also calculate the angles between the roots:
\begin{align*}
    (L_1 - L_2, L_2 - L_3) = \frac{2}{12} \langle H_{1,2}, L_2 - L_3 \rangle = -1/6,
\end{align*}
so the angle between $L_1 - L_2$ and $L_2 - L_3$ is $\pi/2 + \pi/6$.
Similarly, we find that the angle between $L_1 - L_2$ and $L_1 - L_3$ is $\pi/2 - \pi/6$.
This suffices to draw the root diagram and the root lattice.
To draw the weights $L_1, L_2, L_3$ of the standard representation, we use the fact that $L_1 + L_2 + L_3 = 0$, so $(L_1 - L_2) + (L_2 - L_3) = 3 L_1$, and we proceed similarly for the other weights.
All in all, we obtain \cref{fig:roots_and_weights_for_standard_sl_3_C}, where we can clearly see that the weights of the standard representation fall on the vertices of a 2-simplex.
\begin{figure}[h]\label{fig:roots_and_weights_for_standard_sl_3_C}
    \centering
    \includegraphics[width=0.6\textwidth]{roots_and_weights_for_standard_sl_3_C.jpg}
    \caption{Roots (in black) of $\ssl(3,\mathbb C)$ and weights (in purple) of its adjoint representation.}
\end{figure}


\chapter{Case studies}
\section{$\mathbb R$}
The representations of $\mathfrak g = \mathbb R$ are given as $\rho_*: \mathbb R \to \mathfrak{gl}(V)$, $t \mapsto t A$, where $A = \rho_*(1)$, and the representations of $G = \mathbb R$ are given by $\rho: \mathbb R \to \GL(V)$, $t \mapsto e^{tA}$ for $A = \rho_*(1)$.
The irreducible representations of $\mathbb R$ are one-dimensional and are given by $\rho_*: \mathbb R \to \mathfrak{gl}(V) \simeq \mathbb C, t \mapsto t \lambda$ for some $\lambda \in \mathbb C$, and the corresponding representation of $G$ is given by $\rho: \mathbb R \to \GL(V) \simeq \mathbb C^{\times}, t \mapsto e^{t \lambda}$ (see \cref{ex:irreducible_representations_of_R}).
\section{$\mathbb S^1$}
The representations $\rho:\mathbb S^1 \to \GL(V)$ are in bijective correspondence with the ones $\tilde \rho: \mathbb R \to \GL(V)$ such that $\rho(\mathbb Z) = \id$, since $p: \mathbb R \to \mathbb S^1 = \mathbb R / \mathbb Z$ is a covering group.
\[
    \begin{tikzcd}
        \mathbb R \arrow[dr, "\tilde \rho"] \arrow[d, "p"']  \\
        \mathbb S^1 \arrow[r, "\rho"] & \GL(V)
    \end{tikzcd}
\]

The irreducible ones of $\mathbb S^1$ are in (the same) bijective correspondence with the irreducible ones of $\mathbb R$ such that $\tilde \rho(1) = 1$, where we recall that the irreducible representations of $\mathbb R$ are exactly the one-dimensional ones and they have the form $\rho(t) = e^{t \lambda} \in \GL(V) \simeq \mathbb C^{\times}$.
Hence the irreducible representations of $\mathbb S^1$ are exactly one-dimensional complex vector spaces $V_k$ and are given by $\mathbb S^1 = \mathbb R/ \mathbb Z \to \GL(V_k) \simeq C^{\times}, t + \mathbb Z \mapsto e^{2 \pi i k }$.

\section{$\mathfrak{sl}(2, \mathbb C)$}
The reference for this section is \cite[Lecture 11.1]{fulton2013representation}.
The goal is to classify the finite-dimensional irreducible representations of $\mathfrak{sl}(2, \mathbb C)$ and show
\begin{theorem}\label{thm:irreducible_representations_of_sl_2_C}
    The finite-dimensional irreducible representations of $\mathfrak{sl}(2, \mathbb C)$ are exactly the symmetric powers of the standard representation $\mathbb C^2$.
\end{theorem}
We fix the basis for $\mathfrak{sl}(2, \mathbb C)$ as
\[
H = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}, \quad E = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \quad F = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}.
\]
satisfying the relations
\[
[H, E] = 2E, \quad [H, F] = -2F, \quad [E, F] = H.
\]

We fix a finite-dimensional representation $\rho: \mathfrak{sl}(2, \mathbb C) \to \GL(V)$.
Then using the preservation of the Jordan decomposition for semisimple algebras, we have that the action of $H$ on $V$ is diagonalizable:
\[
V = \oplus_{\alpha \in \mathbb C} V_\alpha
\]
Using the commutativity relations and the irreduciblity hypothesis, we find for any eigenvalue $\alpha_0 \in \mathbb C$ that occurs in $V$, all other eigenvalues are in $\alpha_0 \in 2\mathbb Z$:
\[
V = \oplus_{k \in \mathbb Z} V_{\alpha_0 + 2k}.
\]
Now we fix $n$ to be the largest eigenvalue of $\rho(H)$ appearing in $V$ with eigenvector $v$, and again using the commutativity relations, we can depict the action of $\mathfrak{sl}(2, \mathbb C)$ on $V$ with \cref{fig:action_of_sl_2_C_on_V}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{sl_2_C_action.png}
    \caption{Action of $\mathfrak{sl}(2, \mathbb C)$ on $V$: $X=E, Y=F$}
    \label{fig:action_of_sl_2_C_on_V}
\end{figure}
Using the relation
\[
E(F^m v) = m(n-m+1)F^{m-1}v,
\]
we show that $\{v, Fv, F^2v, \cdots \}$ is a basis of $V$.
Counting dimensions, we obtain that
\[
\text{All eigenspaces } V_\alpha \text{ of } H \text{ are one-dimensional.}
\]
Letting $m$ be the first integer such that $F^m v = 0$, we have that $n = m + 1$, and in particular $n$ is a non-negative integer.
Since the action of $H, E, F$ on each eigenspace is dictated by the commutativity relations, we have that every finite-dimensional representation $V$ of $\mathfrak{sl}(2, \mathbb C)$ is determined by the largest eigenvalue $n$ of $H$.
Moreover, the eigenvalues form a string of integers differing by $2$ and symmetric about the origin in $\mathbb Z$.
All in all, we have shown that
\[
\begin{array}{c}
\text{There is a unique finite-dimensional irreducible representation } V^{(n)} \text{ of } \mathfrak{sl}(2, \mathbb C)
\end{array}
\]
Moreover, $\dim V^{(n)} = n+1$ and the eigenvalues of $H$ are $n, n-2, \cdots, -n$.
In particular, we have the following useful fact that allows us to compute the number of irreducible factors of a representation of $\mathfrak{sl}(2, \mathbb C)$:
\begin{remark}
    Let $V$ be a finite-dimensional representation of $\mathfrak{sl}(2, \mathbb C)$.
    \begin{enumerate}[label = (\roman*)]
        \item $V$ is irreducible if and only if the eigenvalues of $H$ are integers of the same parity and occur with multiplicity $1$.
        \item The number of irreducible factors of $V$ is the sum of the multiplicities of $0$ and $1$ as eigenvalues of $H$.
    \end{enumerate}
    For the second point, we have in particular that the number of irreducible factors isomorphic to some $V^{(2n)}$ is the multiplicity of $0$ as an eigenvalue of $H$, while the number of irreducible factors isomorphic to some $V^{(2n+1)}$ is the multiplicity of $1$ as an eigenvalue of $H$.
\end{remark}
\begin{example}
    Consider the adjoint representation $\ad: \mathfrak{sl}(2, \mathbb C) \to \mathfrak{gl}(\mathfrak{sl}(2, \mathbb C))$.
    We have seen that in the basis $H, E, F$
    \[
    \ad H = \begin{pmatrix} 0 & 0 & 0 \\ 0 & -2 & 0 \\ 0 & 0 & 2 \end{pmatrix}.
    \]
    This means that the weight decomposition of $H$ is $V_0 = \mathbb C H, V_2 = \mathbb C F, V_{-2} = \mathbb C E$.
    All eigenvalues are of the same parity and occur with multiplicity $1$, so the adjoint representation is irreducible, with highest weight $2$.
    Hence the adjoint representation is isomorphic to $\Sym^2(\mathbb C^2)$, and the isomorphisms are exactly the scalar multiples of $T: \mathfrak{sl}(2, \mathbb C) \to \Sym^2(\mathbb C^2)$ given by $T(H) = xy, T(E) = y^2, T(F) = x^2$.
\end{example}
Existence also holds, since we can construct $V^{(n)}$ as the span of $v, Fv, \cdots, F^nv$.
Alternatively, we can show that $V^{(n)} = \Sym^n(\mathbb C^2)$, the $n$-th symmetric power of the standard representation of $\mathfrak{sl}(2, \mathbb C)$.
Indeed, denoting with $\rho_2: \mathfrak{sl}(2, \mathbb C) \to \mathbb C^2 $ the standard representation, one can show that
\[
\Sym^n(\rho_2)(x^{n-k}y^k) = (n - 2k) x^{n-k}y^k,
\]
so $\Sym^n(\rho_2) = \diag(n, n-2, \cdots, -n)$ in the basis $x^{n-k}y^k, k = 0, \cdots, n$.
In particular, for the first two values of $n$ we have that
\begin{enumerate}[label = (\roman*)]
    \item $n = 0: V^{(0)} = \mathbb C$ is the trivial representation.
    \item $n = 1: V^{(1)} = \mathbb C^2$ is the standard representation.
    \item $n = 2: V^{(2)} = \Sym^2(\mathbb C^2) \simeq \ssl(2, \mathbb C)$ is the adjoint representation. 
\end{enumerate}

\section{$\mathfrak{sl}(3, \mathbb C)$}
We proceed analogously to the case of $\mathfrak{sl}(2, \mathbb C)$.

{\bf \underline{First step}:}
The first step was to find the element $H$ that was used to decompose the representation.
A single element does not suffice for $\mathfrak{sl}(3, \mathbb C)$, so we consider the Cartan subalgebra $\mathfrak h$ of diagonal matrices in $\mathfrak{sl}(3, \mathbb C)$.
Since diagnoalizable commuting matrices are simultaneously diagonalizable, we obtain that
\begin{center}
    Any finite dimensional representation $V$ of $\mathfrak{sl}(3, \mathbb C)$ admits a decomposition $$V = \oplus_{\alpha \in \mathbb C} V_\alpha$$ where $V_\alpha$ is the eigenspace for $\mathfrak h$ with eigenvalue $\alpha \in \mathfrak h^*$.
\end{center}

{\bf \underline{Second step}:} See how $\mathfrak{sl}(3, \mathbb C)$ acts on the decomposition of $V$.
In the case of $\mathfrak{sl}(2, \mathbb C)$, we used the vectors $E, F$, which were eigenvectors of the adjoint action of $H$ and move between eigenspaces $V_\alpha$.
Now, we will consider the decomposition of the adjoint representation of $\mathfrak{sl}(3, \mathbb C)$:
\[
\mathfrak{sl}(3, \mathbb C) = \mathfrak h \bigoplus_{\alpha \in \mathfrak h^*} \mathfrak g_\alpha
\]
where $\mathfrak g_\alpha = \{ x \in \mathfrak{sl}(3, \mathbb C) : [H, x] = \alpha(H) x \}$.
More concretely, we have that the eigenspaces are
\[
\mathfrak g_{L_i - L_j} = \mathbb C E_{ij}, \text{ where } L_i \diag(a_1, a_2, a_3) = a_i.
\]
Now the answer of how $\ssl(3,\mathbb C)$ acts on itself and on $V$ is given by considering the action of each root space $\mathfrak g_\alpha$ on other root spaces $\mathfrak g_\alpha'$ or on weight spaces $V_\beta$:
\[
    \ad(\mathfrak g_\alpha) \mathfrak g_{\alpha'} \subset \mathfrak g_{\alpha + \alpha'}, \quad
    \ad(\mathfrak g_\alpha) V_\beta \subset V_{\alpha + \beta}.
\]
for roots $\alpha = L_i - L_j, \alpha' = L_{i'} - L_{j'}$ and a weight $\beta \in \mathfrak h^*$.
In \cref{fig:root_diagram_sl_3_C} we see a pictorial representation of the roots of $\mathfrak{sl}(3, \mathbb C)$ and the adjoint action of $\mathfrak{sl}(3,\mathbb C)$.
There we can consider each dot as a root space $\mathfrak g_\alpha$ and visualize that $\mathfrak g_\alpha$ acts on $\ssl(3, \mathbb C)$ by "translation". 
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weight_diagram.png}
        \caption{Root diagram of $\ssl(3, \mathbb C)$}
        \label{fig:root_diagram_sl_3_C_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.525\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_action_weight_diagram.png}
        \caption{Adjoint action of $\mathfrak g_{L_1 - L_3}$}
        \label{fig:root_diagram_sl_3_C_action}
    \end{subfigure}
    \caption{Root diagram of $\ssl(3, \mathbb C)$ and adjoint action}
    \label{fig:root_diagram_sl_3_C}
\end{figure}

{\bf \underline{Third step}:} Decompose irreducible representations into weight spaces.
This insight on the action of $\mathfrak{sl}(3, \mathbb C)$ on the decomposition of $V$, along with an irreducibility condition tells us that 
\begin{center}
    The weights $\alpha$ of an irreducible representation of $\mathfrak{sl}(3, \mathbb C)$ differ from one another by an integral linear combination of the simple roots $L_i - L_j$, i.e.\ for any weight $\alpha_0$ of $V$:
    \[
    V = \bigoplus_{\alpha \in \spa_{\mathbb Z} \{L_i - L_j\}} V_{\alpha_0 + \alpha}.
    \]
\end{center}
Letting $\Lambda_R$ be the root lattice $\spa_{\mathbb Z} \{L_i - L_j\}$ generated by the roots of $\mathfrak{sl}(3, \mathbb C)$, we have that the weights lie on the translate $\alpha_0 + \Lambda_R$ of it.

{\bf \underline{Fourth step}:} Find the highest weight of the representation.
We now assume that $V$ is a finite dimensional irreducible representation of $\mathfrak{sl}(3, \mathbb C)$.
Before, we found an "extremal" eigenvector $v \in V_\alpha$ of $H$ that was annihilated by $E$, and then we used it to generate $V$ through the action of $E$.
Then, it was clear what an extremal weight was, since the roots of $\sl(2,\mathbb C)$ were scalars.
Now, we have to introduce an ordering on the set of weights.
To do this, we chose a hyperplane in $\mathfrak h^*$ and order the weights by how far they lie from it.
More concretely, we introduce the functional $\lambda: \mathfrak h^* \to \mathbb \mathbb C$ by extending linearly a functional $\lambda: \Lambda_\mathbb R \to \mathbb R$, which will measure the distance to the hyperplane $\ker \lambda$.
The extremal weight space will be the $V_\alpha$ with $\mathrm{Re}\lambda(\alpha)>0$ maximal.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{sl_3_C_roots_ordering.png}
    \caption{Ordering on $\mathfrak h^*$ induced by $\lambda$}
    \label{fig:roots_ordering}
\end{figure}
Note that to avoid ambiguity, we must choose $\ker \lambda$ to be a hyperplane that does not contain any root, i.e.\ $\ker \lambda \cap \Lambda_R = \{0\}$.
In the case of $\mathfrak{sl}(3, \mathbb C)$, we can choose 
$\lambda(a_1L_1 + a+2 L_2 + a_3 L_3) = a_1a + a_2b + a_3c$
for some $a > b > c$ satisfying $a + b + c = 0$.
Then we can find a highest weight vector $v \in V_\alpha$, i.e.\ an eigenvector of $\mathfrak h$ that is anhiliated by $E_{12}, E_{13}, E_{23}$.
This gives us a way to generate $V$ from $v$ by the succesive applications of $E_{21}, E_{31}, E_{32}$.
From this, we can draw the following conclusions:
\begin{enumerate}[label = (\roman*)]
    \item The weights are found in the $1/3$-plane of $\mathfrak h^*$ with corner $\alpha$, as in \cref{fig:weights_plane}.
    \item The highest weight weight space $V_\alpha$ is one-dimensional.
    \item The weight spaces $V_{\alpha + k(L_2 - L_1)}, k \in \mathbb Z_{\geq 0}$ are also one-dimensional (they lie on the wall of the $1/3$-plane).
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{plane_of_weights.png}
    \caption{Plane in which the weights of $V$ should lie}
    \label{fig:weights_plane}
\end{figure}

And in the case where $V$ is a a finite-dimensional representation of $\mathfrak{sl}(3, \mathbb C)$, and $v \in V$ is a highest weight vector, we have that
\begin{enumerate}[label = (\roman*)]
    \item The subrepresentation $W$ generated by succesive applications of $E_{21}, E_{31}, E_{32}$ on $v$ is an irreducible subrepresentation of $V$.
    \item The set of highest-weight vectors of $V$ forms a union of subspaces $\Psi_W$ of $V$, with $\dim \Psi_W$ being equal to the times that the subrepresentation $W$ appears in $V$.
\end{enumerate}
\begin{center}
    
{\bf \underline{Fifth step}:} Construction of the weight lattice.
In the case of $\ssl(3, \mathbb C)$ we found all possible weights by starting from a highest weight vector and applying repeatedly $F$.
To do the same, we consider a highest weight vector $v \in V_\alpha$ and let $m > 0$ be the first positive integer for which $E_{21}^m(v) = 0$.
This gives us a string of one-dimensional weight spaces $\mathfrak g_{\alpha + k (L_2 - L_1)}$ whose weights lie on the wall of the $1/3$-plane, as in \cref{fig:sl_3_C_string_of_weights}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{sl_3_C_string_of_weights.png}
    \caption{String of weights $\alpha + k(L_2 - L_1)$}
    \label{fig:sl_3_C_string_of_weights}
\end{figure}
The subalgebra $\ssl_{L_2 - L_1}$ of $\ssl(3, \mathbb C)$ generated by $H_{1,2}, E_{1,2}, E_{2,1}$ admits the irreducible subrepresentation of this string of weight spaces
\[
W = \bigoplus_{k \in \mathbb Z} V_{\alpha + k (L_2 - L_1)}.
\]
\end{center}
By the analysis we did on the representations of $\ssl(3, \mathbb C)$, we deduce that the string of weights is symmetric with respect to the reflection along the hyperplane $\langle L, H_{1,2} \rangle = 0$.
We note here without proof that this eigenline is perpendicular to the string of weights.
Similarly, looking at the subalgebra $\ssl_{L_2 - L_3}$, its weights will be symmetric with respect to the reflection along $\langle L, H_{2,3} \rangle = 0$.
Looking at the last eigenspace of the first string, corresponding to the weight $\beta = \alpha + (m-1)(L_2 - L_1)$, we notice that for eigenvector $v' \in V_\beta$ is anhiliated by $E_{1,2}, E_{2,3}, E_{1,3}$.
By choosing an another functional $l':\Lambda_R \to \mathbb R$, $\beta$ would be a highest weight, and repeating the same analysis we just did, we find that all eigenvalues of $V$ occur belor or to the right of the lines through $\beta$ in the directions of $L_1 - L_2, L_3 - L_1$ and that the strings of eigenvalues occuring on these two lines are symmetric about the lines $\langle L, H_{1,2} \rangle = 0$ and $\langle L, H_{1,3} \rangle = 0$, giving us \cref{fig:sl_3_C_string_of_weights_2}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{sl_3_C_string_of_weights_2.png}
    \caption{Second and third strings of weights of $V$}
    \label{fig:sl_3_C_string_of_weights_2}
\end{figure}
Repeating the same process for the extremal weights found at each step, we see that the set of eigenvalues will be bounded by a hexagon symmetric with respect to the eigenlines $\langle L, H_{i,j} \rangle = 0$ and with one vertex in $\alpha$, giving us \cref{fig:sl_3_C_weight_hexagon}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{sl_3_C_weight_hexagon.png}
    \caption{Weight hexagon of $V$}
    \label{fig:sl_3_C_weight_hexagon}
\end{figure}

Letting $\Lambda_W = \spa_{\mathbb Z} \{L_i : i\}$ be the lattice of interstices and using the fact that weights of representations of $\ssl(2, \mathbb C)$ are integers, we obtain
\begin{proposition}
    All the eigenvalues of any irreducible finite-dimensional representation of $\ssl(3, \mathbb C)$ must lie in the lattice $\Lambda_W = \spa_{\mathbb Z} \{L_i : i\} \subseteq \mathfrak h^*$ and be congruent modulo the lattice $\Lambda_R = \spa{\mathbb Z} \{L_i - L_j : i \neq j\}$.
\end{proposition}

Having chosen an ordering functional $l: \Lambda_R \to \mathbb R, l(a_1 L_1 + a_2 L_2 + a_3 L_3) = a_1 a + a_2 b + a_3 c$ with $a > b > c$, we remark that the highest vector of any representation will lie on the $1/6$-plane of the form $ a L_1 - b L_3$ where $a, b \geq 0$.
\begin{theorem}\label{thm:irreducible_representations_of_sl_3_C}
    For any pair of natural numbers $a, b$, there exists a unique irreducible finite-dimensional representation $\Gamma_{a,b}$ of $\ssl(3, \mathbb C)$ with highest weight $a L_1 - b L_3$.
    In particular
    \[
    \Gamma_{a,b} = \ker(\iota_{a,b}) \subseteq \Sym^a(V) \otimes \Sym^b(V^*),
    \]
    where $V$ is the standard representation of $\ssl(3, \mathbb C)$ and
    \begin{align*}
        \iota_{a,b}: \Sym^a(V) \otimes \Sym^b(V^*) &\to \mathbb C, \nonumber \\
        (v_1 \otimes \cdots \otimes v_a) \otimes (v_1^* \otimes \cdots \otimes v_b^*) &\mapsto \sum_{i,j} \langle v_i, v_j^* \rangle (v_1 \cdots \hat{v_i} \cdots v_a) \otimes (v_1^* \cdots \hat{v_j^*} \cdots v_b^*).
    \end{align*}
\end{theorem}

\begin{example}
    \begin{itemize}
        \item $\Gamma_{1,0}$ is the standard representation $V$.
        \item $\Gamma_{0,1}$ is the dual representation $V^*$.
        \item $\Gamma_{1,1}$ is the adjoint representation $\ad: \ssl(3, \mathbb C) \to \gl(\ssl(3, \mathbb C))$.
    \end{itemize}
\end{example}

\begin{fact}
    The weights of irreducible finite-dimensional representations of $\ssl(3, \mathbb C)$ are convex hulls of triangles or hexagons.
    The triangles correspond to the symmetric powers of the standard or the dual representation ($\Gamma_{n,0}, \Gamma_{0,n}$), while the hexagons correspond to the tensor product of the standard representation with its dual.
\end{fact}

\subsection{Examples}
In \cref{fig:sl_3_C_weights} we see the weights of different representations of $\ssl(3, \mathbb C)$.
We have the following remarks to make:

Regarding the dual of the standard representation (\cref{fig:sl_3_C_weights_dual}), we note that the weights of the dual of any representations are the negatives of the weights of the representation.

Regarding the symmetric powers pictured in \cref{fig:sl_3_C_weights_symmetric}, \cref{fig:sl_3_C_weights_symmetric_dual}, we can see that they are irreducible since they can't be decomposed into collections arising from representations of $\ssl(3, \mathbb C)$.
One way to see this is to note that they do not decompose into triangles or hexagons.
Alternatively, looking for instance at $\Sym^2(V)$, we see that symmetry with respect to reflections tells us that the extremal weights need to be included as weights of the representation, while the fact that the borders of the hexagon need to be uninterrupted strings of weights tells us that the representation needs to include all the weights shown in the figure.

Regarding the tensor product of the standard representation with its dual (\cref{fig:sl_3_C_weights_tensor}), its weights need to be the sums of the weights of the standard representation and the dual of the standard representation.
Moreover, the representation is isomorphic to the adjoint representation $\ad: \ssl(3, \mathbb C) \to \gl(\gl(3, \mathbb C))$ on the space $\gl(3, \mathbb C)$, via the isomorphism $T: V \otimes V^* \to \hom(V, V)$ given by $T(v \otimes \varphi)(w) = \varphi(w) v$.
By comparing with the picture of the adjoint representation (\cref{fig:sl_3_C_weights_adjoint}), we are hinted that the representation reducible, since it contains the adjoint representation in $\ssl(3, \mathbb C)$.
To see this in another way, note that the image of $\pi \otimes \pi^*$ is $T^{-1}(\ssl(3, \mathbb C))$, which is thus a subrepresentation.

Regarding $\Sym^3(V)$ and $\Sym^3(V^*)$ (\cref{fig:sl_3_C_weights_symmetric_cube}, \cref{fig:sl_3_C_weights_symmetric_cube_dual}), we see that they are irreducible.
In general it is clear that symmetric powers correspond will be exactly the representations whose weight diagrams are triangles (and not hexagons).
In terms of \cref{thm:irreducible_representations_of_sl_3_C}, we have
\[
\Gamma_{n,0} = \Sym^n(V), \quad \Gamma_{0,n} = \Sym^n(V^*).
\]

Regarding $\Sym^2(V) \otimes V^*$ (\cref{fig:sl_3_C_weights_sym_square_tensor_dual}), we see that it is reducible, since the map
\[
\iota: \Sym^2(V) \otimes V^* \to \mathbb C, v w \otimes u^* \mapsto \langle u^*, v \rangle w +  \langle u^*, w \rangle v
\]
is a morphism of representations, so its kernel will be a subrepresentationm, with weight diagram pictured in \cref{fig:sl_3_C_weights_G_2_1}.
Now, $\ker(\iota)$ will contain an irreducible subrepresentation $\Gamma$ with highest weight $2 L_1 - L_3$.
Looking at the diagram, we can tell that $\Gamma$ will contain all the weights of $\ker i$.
To know if $\ker i = \Gamma$, we need to check whether the weight spaces corresponding to $L_1, L_2, L_3$ are one or two-dimensional, which we can see from the fact that $\Gamma_{L_1}$ will be the span of the image of $V_{2L_1 - L_3}$ under $E_{2,1}E_{3,2}$ and $E_{3,2}E_{2,1}$.

We also observe that if $V, W$ are representations with highest weights $\alpha, \beta$, then the tensor product $V \otimes W$ will have highest weight $\alpha + \beta$ and $\Sym^n(V)$ will have highest weight $n \alpha$. 
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_standard.png}
        \caption{Standard representation $V$}
        \label{fig:sl_3_C_weights_standard}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_dual.png}
        \caption{Dual representation $V^*$}
        \label{fig:sl_3_C_weights_dual}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_symmetric.png}
        \caption{Symmetric power $\Sym^2(V)$}
        \label{fig:sl_3_C_weights_symmetric}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_symmetric_dual.png}
        \caption{Symmetric power of dual $\Sym^2(V^*)$}
        \label{fig:sl_3_C_weights_symmetric_dual}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_adjoint.jpg}
        \caption{$\Gamma_{1,1}$ or the adjoint representation $\ad : \ssl(3,\mathbb C) \to \gl(\ssl(3,\mathbb C))$}
        \label{fig:sl_3_C_weights_adjoint}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_tensor.png}
        \caption{Tensor product $V \otimes V^* \simeq \ad : \ssl(3,\mathbb C) \to \gl(\gl(3, \mathbb C))$}
        \label{fig:sl_3_C_weights_tensor}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_symmetric_cube.png}
        \caption{$\Sym^3(V)$}
        \label{fig:sl_3_C_weights_symmetric_cube}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_symmetric_cube_dual.png}
        \caption{$\Sym^3(V^*)$}
        \label{fig:sl_3_C_weights_symmetric_cube_dual}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_sym_square_tensor_dual.png}
        \caption{$\Sym^2(V) \otimes V^*$}
        \label{fig:sl_3_C_weights_sym_square_tensor_dual}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sl_3_C_weights_G_2_1.png}
        \caption{$\Gamma_{2,1}$}
        \label{fig:sl_3_C_weights_G_2_1}
    \end{subfigure}
    \caption{Weights of representations of $\ssl(3, \mathbb C)$}
    \label{fig:sl_3_C_weights}
\end{figure}

\section{$\mathrm{SL}(d, \mathbb K)$}
We have
\begin{align*}
    \mathfrak{sl}(d, \mathbb K) &= \{ x \in \gl(d, \mathbb K) : \tr x = 0\}\\
\end{align*}
The Cartan subalgebra is given by
\[
\mathfrak h = \left\{ a_1 H_1 + \cdots + a_d H_d : a_1 + \cdots + a_d = 0 \right\},
\]
where $H_i = E_{i,i}$.
We denote with $L_i \in \mathfrak h^*$ the dual basis of $H_i$.
The roots are 
\[
    \Delta = \{ L_i - L_j : i \neq j \},
\] 
and the root spaces are $$\mathfrak g_{L_i - L_j} = \mathbb C E_{i,j}.$$

To find the Killing form, by \cref{ex:simple_killing_form} we know that it is a constant multplle of the trace form.
Then by a concrete calculation over $\mathfrak h$, we find that the constant is $2d$:
\[
K(x, y) = 2d \tr(xy).
\]

The root lattice is given by
\[
\Lambda_R = \left\{ \sum c_i L_i : c_i \in \mathbb Z, \sum_i c_i = 0 \right\}/\left\{ \sum_i L_i = 0 \right\}.
\]

The distinguished subalgebra is 
$$\mathfrak s_{L_i - L_j} \simeq \mathbb C E_{i,j} \oplus \mathbb C E_{j,i} \oplus \mathbb C (H_i - H_j),$$
and the hyperplane $\Omega_{L_i - L_j} = (L_i - L_j)^\perp$ is given by
\[
\Omega_{L_i - L_j} = \left\{ \sum_r c_r L_r : c_i = c_j \right\}.
\]

The weight lattice is given by 
\[
\Lambda_W = \spa_{\mathbb Z} \{L_1, \cdots, L_d\} / \left\{ \sum L_i = 0  \right\}
\]

The Weyl group is the symmetric group $S_d$ and each reflection $s_{L_i - L_j}$ is exchanges $L_i$ and $L_j \in \mathfrak h^*$.
\subsection{Killing form}
We consider the case $d = 2$.
In the ordered basis $h,e,f$, the adjoint representation is given by
\[
\ad_h = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & -2 \end{pmatrix}, \quad
\ad_e = \begin{pmatrix} 0 & 0 & 1 \\ -2 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, \quad
\ad_f = \begin{pmatrix} 0 & -1 & 0 \\ 0 & 0 & 0 \\ 2 & 0 & 0 \end{pmatrix}.
\]
So the Killing form is given by
\[
B = \begin{pmatrix} 8 & 0 & 0 \\ 0 & 0 & 4 \\ 0 & 4 & 0 \end{pmatrix}.
\]

\subsection{Simplicity}
We will show that $\SL(d, \mathbb K)$ is simple.
First we note that if any ideal $I$ of $\mathfrak{sl}(d, \mathbb K)$ contains some $E_{ij}$ with $i \neq j$, then it $I = \mathfrak{sl}(d, \mathbb K)$, since
\begin{align*}
    [[E_{ij}, E_{ji}], E_{ji}] &= -2 E_{ji} \in I.
    [E_{ji}, E_{jk}] = \begin{cases}
        E_{ik} & \text{if } i \neq k\\
        E_{ii} - E_{jj} & \text{if } i = k
    \end{cases}
\end{align*}
Thus it suffices to show that any nonzero ideal $I$ of $\mathfrak{sl}(d, \mathbb K)$ contains some $E_{ij}$ with $i \neq j$.
Indeed, if $X \in I - \{0 \}$, then we consider two cases.
If there exists an off-diagonal nonzero entry $x_{ji}, j\neq i$ of $X$, then $[[X, E_{ij}], E_{ij}] = -2x_{ji}E_{ij} \in I$.
If on the other hand all nonzero entries of $X$ are concentrated on the diagonal, we let some $i \neq j$ such that $x_{ii} \neq x_{jj}$.
Then $[X, E_{ij}] = (x_{ii} - x_{jj})E_{ij} \in I$. 

\section{$\mathrm{Sp}(2n, \mathbb C)$}
We will consider the skew-symmetric bilinear form $Q$ with matrix given by
\[
J_{n,n} = \begin{pmatrix}
    0 & I_n\\
    -I_n & 0
\end{pmatrix}.
\]
Using block matrices, we find the algebra to be
\[
\mathfrak{sp}(2n, \mathbb C) = \left\{ \begin{pmatrix}
    A & B\\
    C & -A^t
\end{pmatrix}: A, B,C \in \mathfrak{gl}(n, \mathbb C), B^t = B, C^t = C
\right\}.
\]

\subsection{Cartan subalgebra}
We will consider the Cartan subalgebra $\mathfrak h$ of diagonal matrices in $\mathfrak{sp}(2n, \mathbb C)$ given by
\[
\mathfrak h = \left\{
    \begin{pmatrix}
        \diag(a_1,\cdots, a_n, -a_1,\cdots, -a_n)
    \end{pmatrix} : a_i \in \mathbb{C}    
\right\},
\]

It is a maximal abelian subalgebra of $\so(m, \mathbb C)$, because $H = \diag(1, 2, \cdots, n, -1, \cdots, -n)$ is diagonalisable with distinct eigenvalues.
Thus any matrix that commutes with it will have the same eigenvectors, i.e.\ it will be diagonalisable.

\subsection{Roots and root decomposition}
Denote with $L_i$ the dual basis of $H_i = E_{ii} - E_{n+i,n+i}$.
Then we have the following relations:
\begin{align*}
    \ad_{H}(E_{ij} - E_{n+j,n+i}) &= (L_i - L_j)(E_{ij} - E_{n+j,n+i}),\\
    \ad_{H}(E_{i,n+j} + E_{j,n+i}) &= (L_i + L_j)(E_{i,n+j} + E_{j,n+i}),\\
    \ad_{H}(E_{n+i,j} + E_{n+j,i}) &= (-L_i - L_j)(E_{n+i,j} + E_{n+j,i}),\\
    \ad_{H}(E_{i,n+i}) &= 2L_i(E_{i,n+i}),\\
    \ad_{H}(E_{n+i,i}) &= -2L_i(E_{n+i,i}).
\end{align*}
This diagonalizes the adjoint action of $\mathfrak h$, so the roots are
\[
\Delta = \{ (\pm L_i \pm L_j) : 1 \leq i \leq j \leq n \},
\]
and the root decomposition of $\mathfrak{sp}(2n, \mathbb C)$ is given by
\[
\mathfrak{sp}(2n,\mathbb C) = \mathfrak h \oplus \bigoplus_{1 \leq i \leq j \leq n} \mathfrak g_{\pm L_i \pm L_j},
\]
where
\begin{align*}
    \mathfrak g_{L_i - L_j} &= \mathbb C \left( E_{ij} - E_{n+j,n+i} \right),\\
    \mathfrak g_{L_i + L_j} &= \mathbb C \left( E_{i,n+j} + E_{j,n+i} \right),\\
    \mathfrak g_{2L_i} &= \mathbb C \left( E_{i,n+i} \right),\\
    \mathfrak g_{-2L_i} &= \mathbb C \left( E_{n+i,i} \right).
\end{align*}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{root_diagram_sp(4,C).jpg}
    \caption{Root diagram for $\mathfrak{sp}(4, \mathbb C)$}
    \label{fig:root_diagram_sp_4_C}
\end{figure}
\subsection{Killing form}
We know the Killing form is a multiple of the trace form, so by concrete calculation on $\mathfrak{h}$ we find it to be
\[
B(x,y) = 4(n+1)\tr(xy).
\]

\section{$\SO(n)$}
Here there is, as we will see very shortly, a very big difference in behavior
between the so-called "even" orthogonal Lie algebras $\SO(2n, \mathbb C)$ and the "odd"
orthogonal Lie algebras $\SO(2n+1, \mathbb C)$. 
Interestingly enough, the latter seem at first glance to be more complicated, especially in terms of notation; but when we analyze their representations we see that in fact they behave more regularly than the even ones.

We will consider a complex vector space $V$, endowed with a non-degenerate symmetric bilinear form $Q$, whose matrix with respect to a base $e_1, \cdots, e_m$ for $V$ will be denoted with $M$.
In the even dimensional case $m = 2n$, we will use:
\[
M = \begin{pmatrix}
    0 & I_n\\
    I_n & 0
\end{pmatrix}.
\]
Using block matrices, one can show that
\[
\mathfrak{so}(2n, \mathbb C) = \left\{ \begin{pmatrix}
    A & B\\
    C & -A^t
\end{pmatrix}: A, B,C \in \mathfrak{gl}(n, \mathbb C), B^t = -B, C^t = -C
\right\}.
\]

In the odd dimensional case $m = 2n+1$, we will use:
\[
M = \begin{pmatrix}
    0 & I_n & 0\\
    I_n & 0 & 0\\
    0 & 0 & 1
\end{pmatrix}
\]
and the Lie algebra is given by
\[
\mathfrak{so}(2n+1, \mathbb C) = \left\{
    \begin{pmatrix}
        A & B & -H^t\\
        C & -A^t & -G^t\\
        G & H & 0
    \end{pmatrix}: A, B, C \in \mathfrak{gl}(n, \mathbb C), G,H \in C^{1 \times n}
\right\}.
\]

\subsection{Cartan subalgebra}
We will consider the Cartan subalgebra $\mathfrak h$ of diagonal matrices in $\so(m, \mathbb C)$.
In the even dimensional case of $\mathfrak{so(2n, \mathbb C)}$, we have:
\[
\mathfrak h = \left\{
    \begin{pmatrix}
        \diag(a_1,\cdots, a_n, -a_1,\cdots, -a_n)
    \end{pmatrix} : a_i \in \mathbb{C}    
\right\},
\]
and in the odd dimensional case of $\mathfrak{so(2n + 1, \mathbb C)}$, we have:
\[
\mathfrak h = \left\{
    \begin{pmatrix}
        \diag(a_1,\cdots, a_n, -a_1,\cdots, -a_n, 0)
    \end{pmatrix} : a_i \in \mathbb{C}    
\right\}.
\]

It is a maximal abelian subalgebra of $\so(m, \mathbb C)$, because $H = \diag(1, 2, \cdots, n, -1, \cdots, -n)$ (or $ H = \diag(1, 2, \cdots, n, -1, \cdots, -n, 0)$ in the odd dimensional case) is diagonalisable with distinct eigenvalues.
Thus any matrix that commutes with it will have the same eigenvectors, i.e.\ it will be diagonalisable.

\subsection{Roots and root decomposition}
Denote with $L_i$ the dual basis of $H_i = E_{ii} - E_{n+i,n+i}$.
Then we have the following relations:
\begin{align*}
    \ad_{H}(E_{ij} - E_{ji}) &= (L_i - L_j)(E_{ij} - E_{ji}),\\
    \ad_{H}(E_{i,n+j} - E_{j,n+i}) &= (L_i + L_j)(E_{ij} - E_{ji}),\\
    \ad_{H}(E_{n+i,j} - E_{n+j,i}) &= (-L_i - L_j)(E_{ij} - E_{ji}),\\
    \ad_{H}(E_{n+i,n+j} - E_{n+j,n+i}) &= (-L_i + L_j)(E_{ij} - E_{ji}).
\end{align*}
In the even dimensional case of $\so(2n, \mathbb C)$, this diagonalizes the adjoint action of $\mathfrak h$, so the roots are
\[
\Delta = \{ (\pm L_i \pm L_j) : 1 \leq i < j \leq n \},
\]
and the root decomposition of $\mathfrak{so}(2n, \mathbb C)$ is given by
\[
\mathfrak{so}(2n,\mathbb C) = \mathfrak h \oplus \bigoplus_{1 \leq i < j \leq n} \mathfrak g_{\pm L_i \pm L_j},
\]
where
\begin{align*}
    \mathfrak g_{L_i - L_j} &= \mathbb C \left( E_{ij} - E_{ji} \right),\\
    \mathfrak g_{L_i + L_j} &= \mathbb C \left( E_{i,n+j} - E_{j,n+i} \right),\\
    \mathfrak g_{-L_i - L_j} &= \mathbb C \left( E_{n+i,j} - E_{n+j,i} \right),\\
    \mathfrak g_{-L_i + L_j} &= \mathbb C \left( E_{n+i,n+j} - E_{n+j,n+i} \right).
\end{align*}

For the odd-dimensional case of $\so(2n+1, \mathbb C)$, to diagonalise the adjoint action of $\mathfrak h$ we also need the relations:
\begin{align*}
    \ad_{H}(E_{i,2n+1} - E_{2n+1,n+i}) &= L_i(E_{i,2n+1} - E_{2n+1,i}),\\
    \ad_{H}(E_{n+i,2n+1} - E_{2n+1,i}) &= -L_i(E_{i,2n+1} - E_{2n+1,i}),\\
\end{align*}
so the root space decomposition is given by
\[
    \mathfrak{so}(2n+1,\mathbb C) = \mathfrak h \oplus \bigoplus_{1 \leq i < j \leq n} \mathfrak g_{\pm L_i \pm L_j} \oplus \bigoplus_{1 \leq i \leq n} \mathfrak g_{\pm L_i}.
\]
where the last root spaces are given by
\[
\mathfrak g_{L_i} = \mathbb C \left( E_{i,2n+1} - E_{2n+1,n+i} \right), \quad \mathfrak g_{-L_i} = \mathbb C \left( E_{n+i,2n+1} - E_{2n+1,i} \right).
\]
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{root_diagram_a1+a_1.jpg}
        \caption{$A_1 \oplus A_1$: Root diagram for $\mathfrak{so}(4, \mathbb C)$}
        \label{fig:root_diagram_so_4_C}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{root_diagram_so(5,C).jpg}
        \caption{Root diagram for $\mathfrak{so}(5, \mathbb C)$}
        \label{fig:root_diagram_so_5_C}
    \end{subfigure}
    \caption{Root diagrams for $\mathfrak{so}(4, \mathbb C)$ and $\mathfrak{so}(5, \mathbb C)$}
\end{figure}
\section{$\SO(3)$}
Let
\[
i = \begin{pmatrix}
    0 & 0 & 0\\
    0 & 0 & 1\\
    0 & -1 & 0
\end{pmatrix}, \quad j = \begin{pmatrix}
    0 & 0 & 1\\
    0 & 0 & 0\\
    -1 & 0 & 0
\end{pmatrix}, \quad k = \begin{pmatrix}
    0 & 1 & 0\\
    -1 & 0 & 0\\
    0 & 0 & 0
\end{pmatrix}
\]
be the basis for $\mathfrak{so}(3)$.
In this basis, the Killing form is given by 
\[
K = - 2I_3.
\]

\section{$\SO(p,q)$}
Assume that $p \leq q$.
We consider the matrices
\[
S_{p,q} = \begin{pmatrix} 
    0 & 0 & J_p \\ 
    0 & I_{q-p} & 0\\
    J_p & 0 & 0
\end{pmatrix},
\]
and $J_p$ is the $p \times p$ matrix with ones on the antidiagonal.
Then
\begin{align*}
    \SO(p,q) &= \left\{ g \in \SL(d, \mathbb R) : g^t S_{p,q} g = S_{p,q} \right\}\\
    \mathfrak{so}(p,q) &= \left\{ x \in \mathfrak{sl}(d, \mathbb R) : x^t S_{p,q} + S_{p,q}x = 0 \right\}
\end{align*}
The Cartan involution is given by 
\[
\sigma: \SO(p,q) \to \SO(p,q), \quad g \mapsto S_{p,q} g S_{p,q}.
\]
To see that it is indeed a Cartan involution, recall that the Killing form of subgroups of $\GL(d, \mathbb R)$ is given by $K(x,y) = \tr(xy)$, so using the relation $x^t S_{p,q} = - S_{p,q}x$ for $x \in \mathfrak{so}(p,q)$, we see that $K(x, \sigma(x)) = - \tr(x x^t)$, which is clearly negative definite.

Note that $S_{p,q}$ can be seen as the reflection $S_{p,q}(v_1 + v_{-1}) = v_1 - v_{-1}$, where $v_{\pm 1} \in V_{\pm 1}$ and $V_{\pm 1}$ are the eigenspaces of $S_{p,q}$ corresponding to eigenvalues $\pm 1$.
Then $K = \mathrm{Fix}(\sigma) = \left\{ g \in \SO(p,q) : g V_1 = V_1 \right\} = \mathrm{Stab}_{\SO(p,q)}(V_1 \oplus V_{-1}) = \mathrm{S}(\mathrm{O}(p) \times \mathrm{O}(q))$, and the Cartan decomposition of the algebra is
\begin{align*}
    \mathfrak{k} &= \left\{ x \in \so(p,q) : x^t = -x \right\}\\
    \mathfrak{p} &= \left\{ x \in \so(p,q) : x^t = x \right\}\\
\end{align*}
The Cartan subalgebra is given by the intersection of the Cartan subalgebra of $\mathfrak{sl}(p+q)$ and $\mathfrak{so}(p,q)$:
\[
\mathfrak{a} = \left\{ \diag(a_1, \cdots, a_p, 0, \cdots, 0, -a_p, \cdots, -a_1) : a_1, \cdots, a_p \in \mathbb R  \right\}
\]
and since the Cartan decomposition is compatible with the one of $\mathfrak{sl}(d, \mathbb R)$, the Cartan projection is the same as well.

\section{Lie algebras of dimension $2$}
There exist only two possible Lie algebras over a field $\mathbb K$ up to isomorphism:
\begin{enumerate}
    \item The abelian
    \item The algebra of the group of affine transformations of the $\mathbb K$-plane:
    \[
        \mathrm{Aff}(2,\mathbb K) = \left\{  
    \begin{pmatrix}
    x & y \\
    0 & 1
    \end{pmatrix}: x,y \in \mathbb K\right\},
    \]
    or equivalently any algebra admitting a basis $X, Y$ such that $[X,Y] = Y$.
\end{enumerate}
\subsection{Killing form}
The Killing form is clearly identically zero in the abelian case.
Asumme now that $\mathfrak g = \mathfrak{aff}(n, \mathbb K)$.
Then
\[
\ad_X = \begin{pmatrix}
    0 & 0 \\
    0 & 1
    \end{pmatrix}, 
    \ad_Y = \begin{pmatrix}
    0 & 0 \\
    -1 & 0
    \end{pmatrix},
    \text{ so }
    B = \begin{pmatrix}
    1 & 0 \\
    0 & 0
    \end{pmatrix},
\]
where the matrix of the Killing form is in the basis $X, Y$.

\chapter{Fun facts}

\begin{question}
    What do Lie groups have to do with harmonic analysis?
\end{question}
\begin{proof}[Answer]
    The study of the space $L^2(G, \d g)$ where $G$ is a compact real Lie group and $\d g$ denotes its Haar measure, can be considered as a generalisation of the classical Fourier analysis on the circle $\mathbb S^1$, as can be seen by setting $G = \mathbb S^1$ (see \cref{ex:peter_wheyl_fourier}).
\end{proof}


\printbibliography
\end{document}